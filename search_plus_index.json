{"./":{"url":"./","title":"关于博客说明","keywords":"","body":"博客简介： 博客是博主多年的测试和运维工作的知识总结，主要涉及测试、运维以及docker容器编排和部署等方面知识，也会涉及到cicd、Linux方面知识；甚至有多种工具和shell语言组合后的东西；博主希望通过对自己掌握的知识进行汇总，展示出来，给需要的人提供帮助或者思路。若能帮助到大家，也就达到博主的目的。 如果大家是用手机浏览 本网站，下拉滑动后点击左上角，即可看到目录树，来查看所需的文章。 本人csdn博客地址：https://blog.csdn.net/qq_39919755 会不定期进行同步，毕竟csdn被百度抓取可能性较大，能帮助到更多利用搜索器寻找答案的人； 博客主要分为Linux、shell、Jenkins、docker、kubernetes（rancher）、NGINX、git、jmeter、自动化测试、大数据处理等几大类，绝大部分都是原创，甚至很多是博主亲身踩过的坑，借博客形式展示出来，在运维和测试人员遇到类似问题、场景提供解决思路或者灵感 若要转载博客，请注明出处；若商用，请联系博主 博主目前处于待业期，如果北京朝阳区这边 有运维、自动化测试相关工作 ，可以给博主发邮件631782068@qq.com "},"linux/tar-deal.html":{"url":"linux/tar-deal.html","title":"\"-\"在tar命令中的巧用","keywords":"","body":"\"-\"在tar命令中的巧用 首先来看示例： tar -cvf - /home | tar -xvf - 前面把压缩结果存到-，后面通过管道 | 把存到-中的文件解压，如果纯粹看这个，觉得这不瞎折腾么，下面从实战来说明使用它的好处 实战案例1： 海量小文件传输方法 接收机：nc -l 8888 | tar xzf - -C /dest-dir 发送机：tar czf - /source-dir/ | nc 接收机ip 8888 在 GNU 指令中，如果单独使用 - 符号，不加任何该加的文件名称时，代表\"标准输入/输出\"的意思,上面命令把结果输入到-,然后再解压-; 接收机可以带上参数v,如 xzvf（便于可视化）；如果发送机压缩命令带有z接收机也必须带上参数z 另外8888或其他端口，一定要放开，或者关闭防火墙；通过这种方式传递文件，相等于把先把文件压缩、然后通过scp 传输、再解压这几步合并成两步，并且省去了等待压缩和解压的时间。 实战案例2： find /directory -type f -name \"mypattern\" | tar -cf archive.tar -T - 找到匹配的文件后，直接压缩 实战案例3 docker cp 命令 docker cp命令中用法 ]# docker cp --help Usage: docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- ​ docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH 官网解释： Use ‘-‘ as the source to read a tar archive from stdin and extract it to a directory destination in a container. Use ‘-‘ as the destination to stream a tar archive of a container source to stdout. 所以我们可以 docker cp cfcc20077ad1:/opt/aa.tgz - | tar xvf - -C ./xx tar czf - anaconda-ks.cfg initial-setup-ks.cfg | docker cp - cfcc20077ad1:/opt/mydir tar成对使用，前面一个是stdin 后面一个是stdout , 最终文件不会改变，-里面存的是压缩包，传输完成后还是压缩包，如果是文件传输完成后还是文件 ，只不过tar 传输比较快 \"-\"不光在tar命令中，可以这么使用；在yaml中，kubectl中 都有类似用法，kubectl中用法，可以参考我的kubectl命令文档。 "},"linux/limit_ip.html":{"url":"linux/limit_ip.html","title":"利用iptables或防火墙指定ip访问服务器某个端口","keywords":"","body":"如何利用iptables或防火墙指定ip访问服务器某个端口 有如下两种方式： 1 firewall-cmd --permanent --add-rich-rule 'rule family=ipv4 source address=10.219.82.83 port port=12181 protocol=tcp accept' systemctl restart firewalld 移除--add-rich-rule规则 用--remove-rich-rule= 帮助文档显示： --list-rich-rules List rich language rules added for a zone [P] [Z] --add-rich-rule= ​ Add rich language rule 'rule' for a zone [P] [Z] [T] --remove-rich-rule= ​ Remove rich language rule 'rule' from a zone [P] [Z] 2 iptables-save > iptables.rules /sbin/iptables -A INPUT -s 172.16.35.31 -p tcp --dport 12181 -j ACCEPT # 中控白名单 /sbin/iptables -A OUTPUT -d 172.16.35.31 -p tcp --sport 12181 -j ACCEPT /sbin/iptables -A INPUT -p tcp --dport 12181 -j DROP #其他服务器无法访问12181端口 /sbin/iptables -A OUTPUT -p tcp --sport 12181 -j DROP \\#iptables --list-rules #查看规则 ，根据情况删除下面规则 \\#iptables -D INPUT -j REJECT --reject-with icmp-host-prohibited # \\#iptables -D FORWARD -j REJECT --reject-with icmp-host-prohibited # service iptables save systemctl restart iptables "},"linux/set_ip.html":{"url":"linux/set_ip.html","title":"如何设置centos7.6和Ubuntu19.4的ip4","keywords":"","body":"centos7.6和Ubuntu19.04设置ip centos7任何版本都可以通过nmtui命令来设置ip， 但centos7.6ip的设置需要注意，在网卡配置文件/etc/sysconfig/network-scripts/ifcfg-eth0 ,需要配置如下两项 #网卡eth0名称因环境而已 ONBOOT=yes IPV6_PRIVACY=no 一定要增加 IPV6_PRIVACY=no ，centos7.6 默认采用IP6技术了 Ubuntu19.4 配置静态IP sudo vim /etc/netplan/50-cloud-init.yaml #文件名称因环境而异，但一定是*.yaml network: ​ ethernets: ​ ens33: ​ dhcp4: no ​ addresses: [192.16.1.101/24, ] ​ gateway4: 192.16.1.2 ​ nameservers: ​ addresses: [192.16.1.2,8.8.8.8] ​ version: 2 完成后执行 sudo netplan apply 即可生效,博主参考了：https://www.tecmint.com/configure-network-static-ip-address-in-ubuntu/ "},"linux/extend_disk.html":{"url":"linux/extend_disk.html","title":"Linux中磁盘扩展与缩减","keywords":"","body":"磁盘扩展和缩减知识汇总 新增分区，挂靠到新的目录方法 1,首先通过命令lsblk 查看增加分区的情况； [root@apptrace0011 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 501G 0 disk ├─sda1 8:1 0 1G 0 part /boot ├─sda2 8:2 0 199G 0 part │ ├─centos-root 253:0 0 50G 0 lvm / │ ├─centos-swap 253:1 0 3.9G 0 lvm [SWAP] │ └─centos-home 253:2 0 445.1G 0 lvm /home └─sda3 8:3 0 301G 0 part └─centos-home 253:2 0 445.1G 0 lvm /home sdb 8:16 0 10G 0 disk sr0 11:0 1 1024M 0 rom 关于sda/sdb说明，如果通过vmmare 虚拟机控制台等工具，直接在原有的1个硬盘扩充的存储空间；如原有硬盘是200G， 扩充到500G扩充后，扩充的存储还是在sda分区下；如果新增一个硬盘，是在sdb分区，依次类推sdc…… 2，通过命令fdisk -l [root@apptrace0011 ~]# fdisk -l Disk /dev/sda: 537.9 GB, 537944653824 bytes, 1050673152 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000e999c Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 1 FAT12 /dev/sda2 2099200 419430399 208665600 8e Linux LVM /dev/sda3 419430400 1050673151 315621376 8e Linux LVM Disk /dev/sdb: 10.7 GB, 10737418240 bytes, 20971520 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-root: 53.7 GB, 53687091200 bytes, 104857600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-swap: 4160 MB, 4160749568 bytes, 8126464 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-home: 477.9 GB, 477940940800 bytes, 933478400 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes 查看到sda 硬盘和sdb硬盘的情况，sda都已经做分区（但还有空间可以进行分区，下种类型讲） sdb还未有分区；如果新增了硬盘，没有看到可以执行命令： partprobe /dev/sdb ,没有这个命令，自行安装 yum -y install parted 3，如果新增硬盘在sdb下 可以按照如下方式直接挂载 fdisk /dev/sdb 输入m 查看用法 最常用几个用法 p 打印分区情况 n 新增分区； d删除分区；w保存 t改变格式 输入p 打印分区情况 输入 n新增分区 Partition number (1-4, default 1): 正常情况默认选中1；，如果上步p打印时已经有sdb1，输入2 然后输入t 改变分区格式 Command (m for help): t Selected partition 1 Partition type (type L to list all types): L 0 Empty 24 NEC DOS 81 Minix / old Lin bf Solaris 1 FAT12 27 Hidden NTFS Win 82 Linux swap / So c1 DRDOS/sec (FAT- 2 XENIX root 39 Plan 9 83 Linux c4 DRDOS/sec (FAT- 3 XENIX usr 3c PartitionMagic 84 OS/2 hidden or c6 DRDOS/sec (FAT- 4 FAT16 选择8e Linux LVM 这个格式，(有的是83、linux格式的) 最后输入w 保存退出（不能漏掉） Partition type (type L to list all types): 8e Changed type of partition 'Linux' to 'Linux LVM'. 如果保存出现错误,可以 partprobe /dev/sdb (没有数字） 然后再进入 fdisk /dev/sdb 继续上面的操作 甚至重启 4，接着格式化： centos7 可以用mkfs.xfs /dev/sdb1 ，Ubuntu或者centos6 用mkfs.ext4 /dev/sdb1 来格式 输入mkfs. 按tab键，可以看出有哪些格式 [root@apptrace0011 ~]# mkfs. mkfs.btrfs mkfs.cramfs mkfs.ext2 mkfs.ext3 mkfs.ext4 mkfs.minix mkfs.xfs 5，进行挂载： mount /dev/sdb1 /data/（新目录或者老目录，如果没有需求提前创建） 6，开机生效，编辑 /etc/fstab /dev/mapper/centos-root / xfs defaults 0 0 UUID=3c94bedd-2b80-47d3-a3a4-05785847aa10 /boot xfs defaults 0 0 /dev/mapper/centos-home /home xfs defaults 0 0 /dev/mapper/centos-swap swap swap defaults 0 0 把刚才的/data 添加进去 /dev/sdb1 /data xfs defaults 0 0 或者 UUID=3c94bedd-2b80-47d3-a3a4-05785847aa10 /data xfs defaults 0 0 如果是在物理机上，增加硬盘后，最好填写uuid，分区是可以变化，uuid不会变； 7.其他命令 blkid查看挂载硬盘的UUID，如blkid | grep \"sdb*\"，查看现有分区cat /proc/partitions 怎么把原有硬盘扩充的存储都挂靠到/home（或其他已有目录） 1，查看新增硬盘情况，如下，原有硬盘从200G增加到300G [root@part-add ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 300G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 199G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 3.9G 0 lvm [SWAP] └─centos-home 253:2 0 145.1G 0 lvm /home sr0 11:0 1 1024M 0 rom 再查看fdisk情况 [root@part-add ~]# fdisk -l Disk /dev/sda: 322.1 GB, 322122547200 bytes, 629145600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000e999c Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 83 Linux /dev/sda2 2099200 419430399 208665600 8e Linux LVM Disk /dev/mapper/centos-root: 53.7 GB, 53687091200 bytes, 104857600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-swap: 4160 MB, 4160749568 bytes, 8126464 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-home: 155.8 GB, 155818393600 bytes, 304332800 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes 2，把增加的硬盘容量全部分到一个新分区sda3上 fdisk /dev/sda Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/sda: 322.1 GB, 322122547200 bytes, 629145600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000e999c Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 83 Linux /dev/sda2 2099200 419430399 208665600 8e Linux LVM Command (m for help): n Partition type: p primary (2 primary, 0 extended, 2 free) e extended Select (default p): p Partition number (3,4, default 3): First sector (419430400-629145599, default 419430400): Using default value 419430400 Last sector, +sectors or +size{K,M,G} (419430400-629145599, default 629145599): Using default value 629145599 Partition 3 of type Linux and of size 100 GiB is set Command (m for help): p Disk /dev/sda: 322.1 GB, 322122547200 bytes, 629145600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000e999c Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 83 Linux /dev/sda2 2099200 419430399 208665600 8e Linux LVM /dev/sda3 419430400 629145599 104857600 83 Linux Command (m for help): t Partition number (1-3, default 3): Hex code (type L to list all codes): 8e （注意lvm格式） Changed type of partition 'Linux' to 'Linux LVM' Command (m for help): p Disk /dev/sda: 322.1 GB, 322122547200 bytes, 629145600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000e999c Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 83 Linux /dev/sda2 2099200 419430399 208665600 8e Linux LVM /dev/sda3 419430400 629145599 104857600 8e Linux LVM Command (m for help): w The partition table has been altered! Calling ioctl() to re-read partition table. WARNING: Re-reading the partition table failed with error 16: Device or resource busy. The kernel still uses the old table. The new table will be used at the next reboot or after you run partprobe(8) or kpartx(8) Syncing disks. [root@part-add ~]# partprobe /dev/sda [root@part-add ~]# partprobe /dev/sda3 [root@part-add ~]# fdisk -l Disk /dev/sda: 322.1 GB, 322122547200 bytes, 629145600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000e999c Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 83 Linux /dev/sda2 2099200 419430399 208665600 8e Linux LVM /dev/sda3 419430400 629145599 104857600 8e Linux LVM Disk /dev/mapper/centos-root: 53.7 GB, 53687091200 bytes, 104857600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-swap: 4160 MB, 4160749568 bytes, 8126464 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-home: 155.8 GB, 155818393600 bytes, 304332800 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes 3，新增分区格式化（执行顺序可以和下面第4部互换） mkfs.xfs /dev/sda3 meta-data=/dev/sda3 isize=512 agcount=4, agsize=6553600 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=26214400, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=12800, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 4，增加物理卷： pvcreate 刚才创建的分区 [root@part-add ~]# pvcreate /dev/sda3 WARNING: xfs signature detected on /dev/sda3 at offset 0. Wipe it? [y/n]: y Wiping xfs signature on /dev/sda3. Physical volume \"/dev/sda3\" successfully created. 查看物理卷增加后的情况 [root@part-add ~]# pvdisplay 或者pvs --- Physical volume --- PV Name /dev/sda2 VG Name centos PV Size 5，将物理卷加入到卷组 1）先看卷组信息 [root@part-add ~]# vgdisplay 或者vgs --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 4 VG Access read/write VG Status resizable MAX LV 0 Cur LV 3 Open LV 2 Max PV 0 Cur PV 1 Act PV 1 VG Size 2）把新的分区加入到卷组 vgextend centos（VG Name） /dev/sda3 （新分区） [root@part-add ~]# vgextend centos /dev/sda3 Volume group \"centos\" successfully extended - 3）再次查看 验证 [root@part-add ~]# vgdisplay --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 5 VG Access read/write VG Status resizable MAX LV 0 Cur LV 3 Open LV 2 Max PV 0 Cur PV 2 Act PV 2 VG Size 298.99 GiB PE Size 4.00 MiB Total PE 76542 Alloc PE / Size 50942 / 198.99 GiB Free PE / Size 25600 / 100.00 GiB VG UUID iza5sY-YoTh-ihTR-dLzZ-2CBx-M0dR-33bzW5 此时VG Size 大小已有 298.99 GiB 6，扩充逻辑卷 1）先通过下面命令查看系统里有哪些逻辑卷。 ``` [root@part-add ~]# lvdisplay 或者lvs --- Logical volume --- LV Path /dev/centos/swap LV Name swap VG Name centos LV UUID m8Dc95-LYbI-cxuG-hNc7-COoH-4Axh-7mamwX LV Write Access read/write LV Creation host, time localhost, 2018-08-31 18:38:32 +0800 LV Status available LV Size currently set to 8192 Block device 253:1 --- Logical volume --- LV Path /dev/centos/home LV Name home VG Name centos LV UUID 1HItab-O0cU-CEkB-cplf-6axH-utqI-Fgxi5n LV Write Access read/write LV Creation host, time localhost, 2018-08-31 18:38:33 +0800 LV Status available LV Size 有/dev/centos/swap /dev/centos/home /dev/centos/root这三个逻辑卷, 其中逻辑卷/dev/centos/home （挂载点是home目录下）就是本次要扩充的对象（同理根目录/ 对应的 /dev/centos/root也可以安装此方法) 2)扩充逻辑卷/dev/centos/home lvextend -L +100G /dev/mapper/centos-home 或者：lvextend -l 提示数量（可以查看Current LE，如果提示太多，减少到提示的最多数量） /dev/mapper/centos-home lvextend -l +100%FREE /dev/mapper/centos-home [root@part-add ~]# lvextend -L +100G /dev/mapper/centos-home Size of logical volume centos/home changed from 3）扩充到文件系统（目录）中，xfs_growfs /dev/centos/home 如果是ext格式 则用resize2fs /dev/centos/home [root@part-add ~]# xfs_growfs /dev/centos/home meta-data=/dev/mapper/centos-home isize=512 agcount=4, agsize=9510400 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=38041600, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=18575, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 38041600 to 64256000 [root@part-add ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/centos-root 50G 1.5G 49G 3% / devtmpfs 1.9G 0 1.9G 0% /dev tmpfs 1.9G 0 1.9G 0% /dev/shm tmpfs 1.9G 8.9M 1.9G 1% /run tmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup /dev/sda1 1014M 184M 831M 19% /boot tmpfs 380M 0 380M 0% /run/user/0 /dev/mapper/centos-home 246G 33M 246G 1% /home 重启后再查看 df -h ，是扩充成功后的；一般不需重启。但最好mount -a 生效一下 新增磁盘挂载步骤概述 如何给服务器增加三块硬盘 ： 1，将三块硬盘增加到pv（物理卷） pvcreate /dev/sdb /dev/sdc /dev/sdd 2,将pv加入到vg（卷）组 vgcreate datavg /dev/sdb /dev/sdc /dev/sdd 3,分配逻辑卷 lvcreate -l 50%FREE -n lv1 datavg lvcreate -L +200M -n lv2 datavg 4,格式化逻辑卷 mkfs.xfs /dev/datavg/lv1 mkfs.ext4 /dev/datavg/lv2 5,挂载 mkdir /lv1 /lv2 mount /dev/datavg/lv1 /lv1 mount /dev/datavg/lv2 /lv2 扩展磁盘步骤概述 1，添加物理磁盘 ​ pvcreate /dev/sdd 2，扩展到现有vg组 ​ vgextend 现有卷组名称 /dev/sdd 3，扩充到现有逻辑卷中 ​ lvextend -L +100G /dev/mapper/centos-home lvextend -l +100%FREE(或者扩展数量) /dev/mapper/centos-home 4，扩充到文件系统中 ​ xfs_growfs /dev/centos/home resize2fs /dev/centos/home ​ vg组减小和迁移等步骤概述 1，减小vg ​ vgremove vg组名 /dev/sdd (或者其他要移动物理卷) 2，迁移vg ​ 迁移vg里面的物理卷，必须是在同一个vg组中 ​ vgmove /dev/sdb /dev/sdc (在线迁移) "},"linux/how_to_made_bin.html":{"url":"linux/how_to_made_bin.html","title":"二进制安装包的制作","keywords":"","body":"如何制作二进制安装包 二进制安装包相当于把脚本和压缩包合成一个可执行的bin文件。 准备脚本和压缩文件 脚本准备 准备两个脚本 起始脚本，用来和压缩包进行合并生成二进制文件的，命名为init.sh(也可以自主命名) #!/bin/bash export TARGET_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\" #或export TARGET_DIR=\"$(cd $(dirname $0) && pwd )\" AAABBB=`awk '/^__AAABBB_BELOW__/ {print NR + 1; exit 0; }' $0` echo \"TARGET_DIR is: $TARGET_DIR\" tail -n+$AAABBB $0 | tar xzv -C $TARGET_DIR &> /dev/null $TARGET_DIR/xx.sh | tee $TARGET_DIR/xx.log exit 0 __AAABBB_BELOW__ 代码注解： 变量AAABBB是获取__AAABBB_BELOW__ 下面一行的行数，然后从此行开始解压，解压完成后，执行解压出来的脚本，并把执行过程输出到指定日志。__AAABBB_BELOW__ 下面一定要空一行 功能脚本，放到压缩包中，待压缩包解压完成后，调用此脚本，实现目标功能的 这个脚本根据生产实际情况来编写，可以是安装各种工具，各种功能的shell脚本。 压缩包准备 压缩包也是根据具体情况来准备，一般可以放各种rpm、安装包（可以是压缩包）、给起始脚本调用的shell文件。 所有文件准备完成后，采用tar zcvf xxx.tgz xxx 格式进行压缩 合成 cat init.sh xxx.tgz > install_$(date +%Y%m%d).bin chmod a+x install_$(date +%Y%m%d).bin 测试和验证 找一个干净的linux环境，把合成后的bin文件copy过去，使用./install_$(date +%Y%m%d).bin进行安装验证，如果有问题，查看日志，然后进行排障。 "},"linux/use_heredoc.html":{"url":"linux/use_heredoc.html","title":"heredocument的巧用","keywords":"","body":"heredocument 的巧用 我们经常使用 cat >(或>>) xxx来把内容添加某个文件中，这就是heredocument 的一种用法，但heredocument 用法不仅仅限制于此 情景1： 我们需要先登录mysql后 才能执行sql脚本 这个时候 就可以利用heredocument mysql -uroot -pxxxx 情景2： :执行某个脚本实现登录后，然后执行相关操作 可以借鉴here document ，写成脚本 #!/bin/bash ./xxx.sh xxx.sh 也可以是可执行的二进制文件 注意： 另外eof包含的内容，含有变量的话 ，不被替换 可以写作 ./xxxx.sh eof块的内容会原封不动的保存 "},"docker/docker-install.html":{"url":"docker/docker-install.html","title":"docker 安装与优化","keywords":"","body":"Docker CE for CentOS 7.5/ubuntu19.04(centos7.5或以上版本) centos安装docker 安装yum⼯具 sudo yum install -y yum-utils 配置docker yum源 sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo sudo yum makecache fast 查询可⽤版本 sudo yum list docker-ce --showduplicates | sort -r 安装指定版本docker-ce-17.03.2.ce,要先安装docker-ce-selinux-17.03.2.ce，否则安装docker-ce会报错 1 sudo yum -y install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm 2 sudo yum -y install docker-ce-17.03.2.ce-1.el7.centos 如果只下载安装包（和依赖库）： yum install -y docker-ce-17.03.2.ce --downloadonly --downloaddir=./ 安装最新版的docker sudo yum -y install docker-ce centos（Redhat）离线安装 只需要下载对应版本RPM包 即可； 阿里镜像源地址 如安装docker-18.09.5，下载对应的3个RPM包即可 containerd.io-1.2.5-3.1.el7.x86_64.rpm docker-ce-18.09.5-3.el7.x86_64.rpm docker-ce-cli-18.09.5-3.el7.x86_64.rpm 利用rancher提供的安装脚本在线安装docker 1，首先下载安装脚本 curl -OS https://releases.rancher.com/install-docker/18.09.sh 2，如果有连接外网VPN，可以使用 curl https://releases.rancher.com/install-docker/18.09.sh |sudo sh直接安装，但这个不建议 如果直接在线执行，缺点是无法修改脚本,失败率高，不建议,因为在centos安装时失败，并提示yum 源找不到可以通过修改一行代码，改用阿里的镜像源 centos|fedora|redhat|oraclelinux) #yum_repo=\"https://download.docker.com/linux/centos/docker-ce.repo\" yum_repo=\"https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\" 3, 下载脚本后第一件事就是修改为阿里的镜像源 下载脚本 阿里镜像源地址：https://mirrors.aliyun.com/docker-ce/linux/ 所以把 https://download.docker.com/linux 全部替换为： https://mirrors.aliyun.com/docker-ce/linux vi 或者vim 18.09.sh :%s/https:\\/\\/download.docker.com\\/linux/https:\\/\\/mirrors.aliyun.com\\/docker-ce\\/linux/g 然后执行sh 18.09.sh 安装成功的 Ubuntu安装docker Ubuntu执行rancher提供docker安装脚本 执行时报错： + sh -c apt-get install -y -q docker-ce= Reading package lists... Building dependency tree... Reading state information... E: Version '' for 'docker-ce' was not found 这个可能Ubuntu版本太高，需要手动来安装docker了 可以把下面命令执行完成后，再来执行rancher提供的docker安装脚本 sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common bash-completion # step 2: 安装GPG证书 sudo curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - # Step 3: 写入软件源信息 sudo add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\" # Step 4: 更新并安装 Docker-CE sudo apt-get -y update ubuntu在线安装 sudo apt install docker.io 或sudo apt install docker-ce ubuntu离线安装 暂缺 rancher提供的docker安装脚本的GitHub：https://github.com/rancher/install-docker 改变docker的image存放⽬录（下面提供三种方式）、配置加速器等 修改配置文件，配置加速器 中写入如下内容（如果文件不存在请新建该文件） [root@yktapp1 dockersh]# cat /etc/docker/daemon.json { \"registry-mirrors\": [ \"https://registry.docker-cn.com\" ] } 也可以使用阿里的镜像加速器和本地仓库配置 #### 使用内部仓库地址的配置 [root@yktapp1 dockersh]# cat /etc/docker/daemon.json { \"registry-mirrors\": [\"https://registry.docker-cn.com\"], \"insecure-registries\": [\"0.0.0.0/0\"] } \"insecure-registries\": [\"0.0.0.0/0\"] 通配表示所有http协议仓库，如果只想通配某个http协议仓库，可以写成具体名称或者通配 \"insecure-registries\":[\"10.251.66.44:5000\"] \"insecure-registries\":[\"10.251.26.0/24\"] - 参考harbor GitHub文档说明 If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add --insecure-registry myregistrydomain.com to the daemon's start up arguments. In the case of HTTPS, if you have access to the registry's CA certificate, simply place the CA certificate at /etc/docker/certs.d/myregistrydomain.com/ca.crt . 自签名https和不安全http都可以用这种方式解决，自签名的证书，也可以把证书copy到相应目录解决，无需添加insecure-registries选项，具体方法参见harbor的使用说明 - 注意，一定要保证该文件符合 json 规范，否则 Docker 将不能启动。 ##### 1），指定docker存储位置，方法1（指定了overlay2，这种方式比overlay更高效，kernel较高的Linux默认就是这种） cat > /etc/docker/daemon.json https://wgaccbzr.mirror.aliyuncs.com\"], \"data-root\": \"/app/lib/docker\" } { \"storage-driver\": \"overlay2\", \"storage-opts\":[\"overlay2.override_kernel_check=true\"] } EOF #### 2），把指定的盘符挂载给docker 直接修改/etc/fstab中把一个逻辑卷挂到docker存储目录 /var/lib/docker中，但要确保原有的挂载没有东西，并取消之前的挂载关系， umout -a oldpath；可以通过下面查看原有的挂载和逻辑卷情况 [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT fd0 2:0 1 4K 0 disk sda 8:0 0 100G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 99G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 7.9G 0 lvm [SWAP] └─centos-home 253:2 0 41.1G 0 lvm /home #### 3），软链的方式 （不推荐，作为挽救使用） systemctl stop docker mv /var/lib/docker /home/docker ln -s /home/docker /var/lib/docker systemctl start docker ### 安装 docker-compose wget -c https://github.com/docker/compose/releases/download/1.22.0/docker-compose-`uname -s-uname -m` cp docker-compose-Linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose - 之后重新启动服务。 sudo systemctl daemon-reload sudo systemctl start docker sudo systemctl enable docker - 注意最好配置完成好了 再启动docker，避免启动docker产生文件存储在默认的路径 ### 测试安装是否成功 docker ps ### 非root用户操作docker sudo gpasswd -a your-user docker or sudo usermod -aG docker your-user 这两种方式 都是把非root用户添加到docker组 ## Kernel性能调优 cat >> /etc/sysctl.conf "},"docker/harbor-install.html":{"url":"docker/harbor-install.html","title":"harbor的安装和注意事项","keywords":"","body":"安装harbor说明 官方说明文档：https://github.com/goharbor/harbor/blob/release-1.7.0/docs/installation_guide.md 选择在线还是离线根据情况，这里不作为操作重点； 离线安装包下载地址 https://github.com/goharbor/harbor/releases 如果采用https协议，并使用自签名的证书的话，可以使用https://github.com/xiaoluhong/server-chart/blob/v2.2.2/create_self-signed-cert.sh 脚本生成证书 在线安装http协议的harbor(默认使用此协议的，简单) 下载完成在线安装包解压后，在harbor目录，备份 docker-compose.yml后再修改 container_name: nginx restart: always cap_drop: - ALL cap_add: - CHOWN - SETGID - SETUID - NET_BIND_SERVICE volumes: - ./common/config/nginx:/etc/nginx:z networks: - harbor dns_search: . ports: - 1180:80 - 5555:443 - 34443:4443 由于宿主机的80和443端口已经被用掉，只能映射成其他端口；如果是一台新机器 无需这么操作 harbor.cfg备份后修改如下两个参数 hostname = 172.16.35.31:1180 ui_url_protocol = http 修改完成后执行 ./install.sh 然后就可以在浏览器中访问了：http://172.16.35.31:1180 在harbor/common目录有两个子目录，分别是：config templates 其中templates是原始文件，config是执行install.sh 或prepare 后生成的，所以尽量修改templates目录下的文件； 如果要修改harbor的配置，按照官方文档“Managing Harbor's lifecycle” 来操作 执行install.sh 相当于 ./prepare docker-compose up -d 这两步，所以修改完成后 要么执行install.sh 要么执行这两步； 如果出现误操作，可以通过docker-compose down -v 再重新执行 如何使用http协议登录和推送镜像 最新版本docker 默认不允许使用http推送，所以我们要修改 在harbor宿主机和远程推送镜像的机器都增加如下配置 vi /etc/docker/daemon.json{ \"insecure-registries\":[\"172.16.35.31:1180\"] } 或者直接在已有配置后面添加 { \"registry-mirrors\": [ \"https://registry.docker-cn.com\" ], \"insecure-registries\":[\"172.16.35.31:1180\"] } 也可以使用0.0.0.0通配所有的http镜像仓库 \"insecure-registries\": [\"0.0.0.0/0\"] 修改harbor宿主机docker启动项配置/usr/lib/systemd/system/docker.service 如果docker17.3版本，直接参照下面方式添加ExecStart=/usr/bin/dockerd --insecure-registry=172.16.35.31:1180 如果docker是最新18.9或者以上版本，直接参照下面方式添加ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock --insecure-registry=172.16.35.31:1180 完成后，重启dockersystemctl daemon-reload systemctl restart docker 验证参数：ps aux|grep dockerd root 106472 31.5 0.0 2150816 71576 ? Ssl 17:06 0:02 /usr/bin/dockerd -H unix:///var/run/docker.sock --insecure-registry=172.16.35.31:1180 远程或者本地登录验证 docker login 172.16.35.31:1180 Username: admin Password: Login Succeeded 若登录时提示net/http: request canceled (Client.Timeout exceeded while awaiting headers) 这是代理搞的鬼，把代理/etc/systemd/system/docker.service.d/http-proxy.conf或者其他配置，里面[Service] Environment=\"HTTP_PROXY=http://10.xx.xx.83:3128\" Environment=\"NO_PROXY=localhost,127.0.0.0/8,10.42.*.*\" 把登录时的ip 加入到NO_PROXY中，支持通配符 开放管理端口映射（http协议，可选操作） 管理端口在 /lib/systemd/system/docker.service 文件中 将其中第11行的 ExecStart=/usr/bin/dockerd 替换为： ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock -H tcp://0.0.0.0:7654 （此处默认2375为主管理端口，unix:///var/run/docker.sock用于本地管理，7654是备用的端口） 将管理地址写入 /etc/profile echo 'export DOCKER_HOST=tcp://0.0.0.0:2375' >> /etc/profile 管理接口以备Jenkins等工具调用使用。 离线安装https协议的harbor 生成证书 1，执行create_self-signed-cert.sh 带上相应参数生成所需自签名证书（可以-h,查看用法；如果有证书，忽略此步） cat /etc/hosts 192.16.1.100 Centos76 reg.czl.com 具体名称和设置的自签名域名一致 ./create_self-signed-cert.sh --ssl-domain=reg.czl.com --ssl-trusted-ip=192.16.1.100 把生成的两个证书 copy到 /data/cert/ 2，配置harbor vi harbor.cfg hostname = 域名 （不能填写IP，如果用IP的话，采用http协议方式安装harbor） #自签名申请的域名如reg.czl.com 但采用了域名后，使用域名制作的证书，最好就填写域名避免出现 docker login ip 提示：certificate signed by unknown authority，linux终端用域名登录，浏览器端可以用域名也可以用https://ip登录， #打tag和推送也必须用域名 ui_url_protocol = https ssl_cert = /data/cert/tls.crt ssl_cert_key = /data/cert/tls.key #ssl_cert = /data/cert/server.crt #ssl_cert_key = /data/cert/server.key 3，执行安装命令 ### 4,如下操作和http协议的一样 但要注意，如果证书是自签名的 ，在其他机器登录此https协议的镜像库，还需要把tls.crt 推送过去 ```bash 非仓库机器： mkdir -p /etc/docker/certs.d/reg.czl.com/ 仓库机器把crt文件推送过去 scp tls.crt root@ip:/etc/docker/certs.d/reg.czl.com/ca.crt 如果是Ubuntu系统，root密码是随机的，可以分两步推送， 先推送给Ubuntu的常用账号，常用账号copy到指定目录 ` 服务器断电重启后，发现连接不上harbor 如何优化 如果断电或者机器重启后，harbor不可用，十有八九就是容器没有自动起来，我们可以完善一下，这个组件自带的docker-compose.yml 文件 如下所示，在每个容器策略里面加上restart选项，设置为unless-stopped，新版1.7.5版本harbor 全部组件restart 选项都设为always ,但还是需要手动停止部分组件后，再全部启动 注意事项 最好把harbor服务器的hostname或者别名加入到从harbor拉取镜像其他主机hosts文件中 vim /etc/hosts ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.16.35.31 szly-manage 常见参数解释说明 docker配置文件/etc/docker/daemon.json 或者启动脚本中 ExecStart参数 {\"hosts\": [\"fd://\", \"tcp://0.0.0.0:2375\"]} fd：// 指自动进行unix socket它 和-H unix:///var/run/docker.sock 、-H unix:// 作用相同，只是不同docker版本，写法略有差异。高版本的docker都已经采用了 -H 这种形式参数 小贴士 直接登录界面后，新建项目，在项目里面点击推送镜像，可以弹出镜像推送方法 更详细配置和证书生成脚本可以参考rancher提供的文档 https://www.cnrancher.com/docs/rancher/v2.x/cn/installation/registry/single-node-installation/ "},"docker/harbor-sync.html":{"url":"docker/harbor-sync.html","title":"harbor仓库镜像同步","keywords":"","body":"主备 Harbor 部署（harbor同步） 前提 两台或以上harbor服务器，其中一台为主服务器，其他为备份服务器 1，仓管管理中，新建目标 2，编辑目标 3，新建规则 选择复制管理选项，点击新建规则，然后填写主 harbor 需要同步复制的仓库和同步规则，最后点击保存，即可。 "},"docker/dockerfile-rule.html":{"url":"docker/dockerfile-rule.html","title":"如何制作出合适的镜像","keywords":"","body":"如何制作最合适docker镜像 控制镜像大小 优先选取基于alpine的镜像 查看tomcat镜像：https://hub.docker.com/_/tomcat?tab=tags （docker hup 中搜索tomcat镜像） 如tomcat8.5的镜像8.5-jre11 204 MB Last update: 13 days ago 而基于alpine的tomcat镜像8.5-jre8-alpine 72 MB Last update: 10 days ago 一眼就能看出来基于alpine的镜像比默认镜像少了130多M 但alpine镜像里面缺少了很多东西，设置不支持bash命令，我们可以在制作镜像时，一一给安装上，安装完成后删除缓存文件，就是安装文件； 选取国内的镜像源 如清华大学、阿里云等 Dockerfile示例： FROM alpine:3.7 MAINTAINER chenzhenglin #更新Alpine的软件源为国内（清华大学）的站点，因为从默认官源拉取实在太慢了。。。 RUN echo \"https://mirror.tuna.tsinghua.edu.cn/alpine/v3.7/main/\" > /etc/apk/repositories # 注意alpine的版本号 可能有差异，链接地址源最好用浏览器打开查看验证一下； 或用latest代替 RUN apk update \\ && apk upgrade \\ && apk add --no-cache bash \\ bash-doc \\ bash-completion \\ && apk add net-tools vim \\ && rm -rf /var/cache/apk/* \\ …… apk add --no-cache不使用本地缓存安装包数据库，直接从远程获取安装包信息安装。这样我们就不必通过apk update获取安装包数据库了，&&合并命令 好处是，多条命令合并成一条命令，减少docker images的层 如果基于centos或Ubuntu的系统 最好也要把软件源换成国内的 FROM centos:7.6.1810 RUN mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.bak && \\ curl -s http://mirrors.aliyun.com/repo/Centos-7.repo -o /etc/yum.repos.d/CentOS-Base.repo && \\ curl -s http://mirrors.aliyun.com/repo/epel-7.repo -o /etc/yum.repos.d/epel-7.repo && \\ yum repolist && \\ 注意：Alpine 镜像中的 telnet 在 3.7 版本后被转移至 busybox-extras 包中，所以apk add telnet 会报错 如下，一个基于tomcat:8.5-alpine镜像的Dockerfile，并验证通过 FROM tomcat:8.5-alpine MAINTAINER chenzhenglin #更新Alpine的软件源为国内（清华大学）的站点，因为从默认官源拉取实在太慢了…… alpine版本要从dockerfile里面查找到 RUN echo \"https://mirror.tuna.tsinghua.edu.cn/alpine/latest-stable/main/\" > /etc/apk/repositories # 注意alpine的版本号 可能有差异 RUN apk update \\ && apk upgrade \\ && apk add --update --no-cache net-tools vim busybox-extras curl \\ && rm -rf /var/cache/apk/* 查看镜像的Dockerfile 在docker hub 里面找到所需镜像后，在description的tab页，选择需要的版本，点击，一般会自动跳转到其githup源码界面； 如：https://hub.docker.com/_/tomcat?tab=description 如下Dockerfile文件FROM openjdk:8-jre ENV CATALINA_HOME /usr/local/tomcat ENV PATH $CATALINA_HOME/bin:$PATH RUN mkdir -p \"$CATALINA_HOME\" WORKDIR $CATALINA_HOME ……此处也省略不必要代码 RUN apt-get update && apt-get install -y --no-install-recommends \\ libapr1 \\ ……此处也省略不必要代码 apt-get install -y --no-install-recommends wget ca-certificates; \\ ……此处也省略不必要代码 # sh removes env vars it doesn't support (ones with periods) # https://github.com/docker-library/tomcat/issues/77 find ./bin/ -name '*.sh' -exec sed -ri 's|^#!/bin/sh$|#!/usr/bin/env bash|' '{}' +; \\ \\ ……此处也省略不必要代码 CMD [\"catalina.sh\", \"run\"] 从这个Dockerfile文件可以看出，已经安装了wget等工具，并把默认的sh换成了bash； 如何判断tomcat里面采用哪一版本的alpine呢,激活镜像后，查看其系统版本信息 cat /etc/os-release 同样其他linux也可以采用此方法查看版本号： cat /etc/redhat|centos-release alpine系统添加软件的常用方法，参见链接： https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management 其他精简镜像的方法 https://mp.weixin.qq.com/s/LOXNMYtZbnYeDR2lBI56fw "},"docker/save_load_images.html":{"url":"docker/save_load_images.html","title":"逐个和批量导出导入docker镜像","keywords":"","body":"逐个导出镜像 #!/bin/bash IMAGES_LIST=($(docker images | sed '1d' | awk '{print $1}')) IMAGES_NM_LIST=($(docker images | sed '1d' | awk '{print $1\"-\"$2}'| awk -F/ '{print $NF}')) IMAGES_NUM=${#IMAGES_LIST[*]} for((i=0;i'#'注意 这个慎用，如果一个镜像有多个版本，容易出现问题，采用下面的批量导入 批量导入到一个压缩包 #!/bin/bash IMAGES_LIST=($(docker images | sed '1d' | awk '{print $1\":\"$2}')) docker save ${IMAGES_LIST[*]} -o all-images.tar.gz 把一个压缩包所有镜像导出并上传到仓库 ./rancher-load-images.sh -l rancher-images.txt -i rancher-images.tar.gz -r reg.czl.com/library #仓库地址包含具体的项目 rancher-load-images.sh 脚本下载地址：https://github.com/rancher/rancher/releases 选择需要版本，点击assets 展开后就有这个脚本下载 逐个导入镜像 cd $DIR/images_file for image_name in $(ls ./) do docker load "},"k8s/kubectl-user-instruction.html":{"url":"k8s/kubectl-user-instruction.html","title":"kubectl常用命令汇总（以及curl注入）","keywords":"","body":"kubectl 常用的命令总结 kubectl 详细命令用法可以参考官网： https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands 常用命令： 查看 只显示默认命名空间的pods kubectl get pods 显示所有空间的pod kubectl get pods --all-namespaces 显示指定空间的pod kubectl get pods -o wide --namespace apm 其中--namespace 与-n 作用等同，后面接命名空间参数 kubectl get deployment -n apm kubectl get pods,svc,rc -n apm svc和services是一样的 这些命令都可以通过 kubectl get --help 来查看帮助 删除 只能删除默认命名空间的deployment kubectl delete deployment nginx 删除指定空间的deployment/其他资源等 kubectl delete TYPE RESOURCE -n NAMESPACE 具体如下： kubectl delete deployment shop-app -n test-shop kubectl delete TYPE --all -n NAMESPACE kubectl delete all -n NAMESPACE kubectl delete all --all 使用yaml文件创建pod kubectl apply -f apptrace-receiver-deployment.yaml apply 和 create 命令都可以后跟yaml，创建所需资源,初次创建pod时可以互相替换使用；如果已有pod只是用于更新的话，又可以和replace相互替换使用；本着化繁就简的原则，create和replace都使用apply; 而且apply属于申明式语法，这个更加灵活，多次执行不会报错，只会更新改变的部分；像Jenkinsfile也已经从脚本语法向申明式转变。 使用kubectl命令把pod、卷、各种资源导出为yaml格式： kubectl get pods podA -n NAMEAPSCE-A -o yaml --export> padA.yaml pod 可以换成其他申明式资源如卷、services等；如果不带上--export 生成文件会有很多无用的内容 现在很多产品如rancher openshift，等；UI界面 直接可视化操作导出各种资源，掌握命令很多时候，可以事半功倍。-o更详细用法 下面有单独说明。 查看命名空间apm的collector服务详情 ```kubectl describe service/apptrace-collector --namespace apm --namespace 和-n 作用相同 - 查看pod日志 ```kubectl logs podname --namespace apm (可以带上 -f 参数) 为节点机apm-docker001打标签 zookeeper-node=apm-docker001,查看标签等； 为节点机打标签和查看 kubectl label nodes apm-docker001 zookeeper-node=apm-docker001 kubectl get nodes --show-labels 为命名空间打标签和查看 kubectl label namespace $your-namesapce istio-injection=enabled kubectl get namespaces --show-labels 给名为foo的Pod添加label unhealthy=true kubectl label pods foo unhealthy=true 查看某种类型字段下有哪些参数； kubectl explain pods kubectl explain Deployment kubectl explain Deployment.spec kubectl explain Deployment.spec.spec kubectl explain Deployment.spec.template kubectl explain Deployment.spec.template.spec 如查看Deployment.spec.template 可以有哪些参数 [root@k8s-master ~]# kubectl explain Deployment.spec.template KIND: Deployment VERSION: extensions/v1beta1 RESOURCE: template DESCRIPTION: Template describes the pods that will be created. PodTemplateSpec describes the data a pod should have when created from a template FIELDS: metadata Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata spec Specification of the desired behavior of the pod. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status 在pod内部，执行shell命令 kubectl exec podname printenv | ps aux | cat、ls 某个文件（如果pod不在默认空间，用-n 指定相应空间） 如：kubectl exec app-demo-68b4bd9759-sfpcf -n test-shop printenv 或者在kubectl exec podname -- 再跟shell命令 如： kubectl exec -it spark-master-xksl -c spark-master -n spark -- mkdir -p /usr/local/spark shell命令前，要加-- 号，不然shell命令中的参数，不能识别 kubectl exec 后面只能是pod,目前还不支持deployment daemonnset等 创建与修改 创建和修改相对复杂，可以直接在rancherUI 进行部署或者修改，修改还可以通过：kubectl edit deploy/xxx -n namespace,也可以把资源导出为yaml后，修改完成后，再次kubectl apply 部署一遍完成修改的目的。本人推荐通过rancherUI来部署和修改，这里就不对创建和修改做详细说明了；参考另一篇博客rancher 中快速部署应用 \"-\"在kubectl中 用法说明 “-”它作为标准输入（池）非常灵活用法 ，和tar中用法非常类似 yaml - apiVersion: v1 data: kubernetes.yml: |- - type: docker containers.ids: - \"*\" processors: - add_kubernetes_metadata: in_cluster: true kind: ConfigMap 命令 kubectl get po --all-namespaces --show-all --field-selector 'status.phase==Pending' -o json | kubectl delete -f - curl --insecure -sfL https://your_domain/v3/import/f2gdpkvz42gqzm8vbrdpd99xgppjxgwct7wt86lzswwnf4p2d4vfd7.yaml | kubectl apply -f - 可以看出，“-”里面保存标准输入的内容。 巧用kubectl 帮助文件 如你只记得部分命令 get ,可以用 kubectl get --help 同理 kubectl create rolebinding 不知道后面接什么 也可以--help一下,记得关键字越多 带上后再使用help，如果只记得部分 就先help 如 kubectl create --help 这样create 所有类型的应用怎么创建 都有了 同样也可以直接 kubectl --help 这样kubectl 有哪些用法就显示出来， 我们要一级级的利用帮助 可能刚开始记住前面一个关键字，写完关键字 help一下 又有很多详细的用法 kubectl 命令规律总结 先看一组命令 kubectl delete sa metricbeat -n efk kubectl get sa --all-namespaces kubectl delete daemon-set metricbeat -n efk 1.会发现，kubectl 不管get 、delete describe等操作 后面跟资源类型 如果sa(serviceaccout) deployment pod,然后是资源名称，如果没有资源名称，则删除、获取此类型所有的资源；最后限定某个命名空间，或者全部命名空间；这个限定命名空间 可以放在kubectl 后面，也可以放在所有参数后面； 也可以写成资源类型/资源名称；如 kubectl delete daemon-set/metricbeat -n efk -o 是指定输出格式 输出格式 说明 -o=custom-columns= 根据自定义列名进行输出，以逗号分隔 -o=custom-colimns-file= 从文件中获取自定义列名进行输出 -o=json 以JSON格式显示结果 -o=jsonpath= 输出jsonpath表达式定义的字段信息 -o=jsonpath-file= 输出jsonpath表达式定义的字段信息，来源于文件 -o=name 仅输出资源对象的名称 -o=wide 输出额外信息。对于Pod，将输出Pod所在的Node名 -o=yaml 以yaml格式显示结果 如下： kubectl get sa -n efk -o yaml kubectl get sa efk-elaticsearch -n efk -o yaml >xxx.yaml kubectl get pod efk-elaticsearch-0 -n efk -o wide 因为k8s 采用的是REST API接口，所有命令都最终会转换成curl -X PUT POS等形式,为什么不直接使用curl命令，因为需要一堆相关授权，rancher UI里面 在deploy或其他资源中，选择api查看 就可以查到，也可以点击右侧的edit编辑后 通过curl命令提交 API Request cURL command line: curl -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\ -X PUT \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{\"annotations\":{\"cattle.io/timestamp\":\"\", \"cni.projectcalico.org/podIP\":\"10.42.1.44/32\"}, \"containers\":[{\"allowPrivilegeEscalation\":false, \"exitCode\":null, \"image\":\"172.16.35.31:1180/apm-images/gettoken:1.0\", \"imagePullPolicy\":\"IfNotPresent\", \"initContainer\":false, \"name\":\"genttoken\", \"ports\":[{\"containerPort\":8001, \"dnsName\":\"genttoken-nodeport\", \"kind\":\"NodePort\", \"name\":\"8001tcp301001\", \"protocol\":\"TCP\", \"sourcePort\":30100, \"type\":\"/v3/project/schemas/containerPort\"}], \"privileged\":false, \"procMount\":\"Default\", \"readOnly\":false, \"resources\":{\"type\":\"/v3/project/schemas/resourceRequirements\"}, \"restartCount\":0, \"runAsNonRoot\":false, \"state\":\"running\", \"stdin\":true, \"stdinOnce\":false, \"terminationMessagePath\":\"/dev/termination-log\", \"terminationMessagePolicy\":\" 等等 修改完成后 可以点击send request来提交 k8s所有资源类型和别名（缩写） ~]# kubectl api-resources NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatus configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node persistentvolumeclaims pvc true PersistentVolumeClaim persistentvolumes pv false PersistentVolume pods po true Pod podtemplates true PodTemplate replicationcontrollers rc true ReplicationController resourcequotas quota true ResourceQuota secrets true Secret serviceaccounts sa true ServiceAccount services svc true Service mutatingwebhookconfigurations admissionregistration.k8s.io false MutatingWebhookConfiguration validatingwebhookconfigurations admissionregistration.k8s.io false ValidatingWebhookConfiguration customresourcedefinitions crd,crds apiextensions.k8s.io false CustomResourceDefinition apiservices apiregistration.k8s.io false APIService controllerrevisions apps true ControllerRevision daemonsets ds apps true DaemonSet deployments deploy apps true Deployment replicasets rs apps true ReplicaSet statefulsets sts apps true StatefulSet tokenreviews authentication.k8s.io false TokenReview localsubjectaccessreviews authorization.k8s.io true LocalSubjectAccessReview selfsubjectaccessreviews authorization.k8s.io false SelfSubjectAccessReview selfsubjectrulesreviews authorization.k8s.io false SelfSubjectRulesReview subjectaccessreviews authorization.k8s.io false SubjectAccessReview horizontalpodautoscalers hpa autoscaling true HorizontalPodAutoscaler cronjobs cj batch true CronJob jobs batch true Job certificatesigningrequests csr certificates.k8s.io false CertificateSigningRequest clusterauthtokens cluster.cattle.io true ClusterAuthToken clusteruserattributes cluster.cattle.io true ClusterUserAttribute leases coordination.k8s.io true Lease bgpconfigurations crd.projectcalico.org false BGPConfiguration clusterinformations crd.projectcalico.org false ClusterInformation felixconfigurations crd.projectcalico.org false FelixConfiguration globalnetworkpolicies crd.projectcalico.org false GlobalNetworkPolicy globalnetworksets crd.projectcalico.org false GlobalNetworkSet hostendpoints crd.projectcalico.org false HostEndpoint ippools crd.projectcalico.org false IPPool networkpolicies crd.projectcalico.org true NetworkPolicy events ev events.k8s.io true Event daemonsets ds extensions true DaemonSet deployments deploy extensions true Deployment ingresses ing extensions true Ingress networkpolicies netpol extensions true NetworkPolicy podsecuritypolicies psp extensions false PodSecurityPolicy replicasets rs extensions true ReplicaSet nodes metrics.k8s.io false NodeMetrics pods metrics.k8s.io true PodMetrics alertmanagers monitoring.coreos.com true Alertmanager prometheuses monitoring.coreos.com true Prometheus prometheusrules monitoring.coreos.com true PrometheusRule servicemonitors monitoring.coreos.com true ServiceMonitor networkpolicies netpol networking.k8s.io true NetworkPolicy poddisruptionbudgets pdb policy true PodDisruptionBudget podsecuritypolicies psp policy false PodSecurityPolicy clusterrolebindings rbac.authorization.k8s.io false ClusterRoleBinding clusterroles rbac.authorization.k8s.io false ClusterRole rolebindings rbac.authorization.k8s.io true RoleBinding roles rbac.authorization.k8s.io true Role priorityclasses pc scheduling.k8s.io false PriorityClass storageclasses sc storage.k8s.io false StorageClass volumeattachments storage.k8s.io false VolumeAttachment k8s的资源类型，其中常用的资源一般有SHORTNAMES，在输入命令时，输入这种简称，无疑能提高效率。 kubectl get cs，要比kubectl get componentstatuses 书写快的多，更容易记住。 更多用法可以参照官网或者国内翻译的博客 https://blog.csdn.net/xingwangc2014/article/details/51204224 "},"k8s/docker-run-and-k8s-command.html":{"url":"k8s/docker-run-and-k8s-command.html","title":"docker run image -args对应yaml语法/rancher UI操作方式","keywords":"","body":"如何区分image与container中的entrypoint、cmd关系 不管是用kubernetes还是docker-compose来管理容器，其command参数相当于覆盖了镜像的entrypoint，args相当于覆盖了镜像的CMD；若采用编排工具管理容器，如果没有重新定义entrypoint和cmd，就默认使用镜像的entrypoint和cmd；如果容器编排工具中只使用了args参数，相当于image的entrypoint+编排工具定义args参数；如果容器中同时定义了 command和args ，容器入口就变成了command+args，image中定义的entrypoint和cmd会完全被覆盖掉。 如何对着docker run image -args,来填写容器编排配置（kubernetes、docker-compose)文件 kubectl 与 Docker 命令关系 可以参考：http://docs.kubernetes.org.cn/70.html 我们经常能在某个image官方文档中看到 像docker run image -args这种用法，这就行相当于改写了其entrypoint或cmd，那这些args若是在kubernetes或docker-compose的yaml中怎么配置呢，如：docker run -it xxx /bin/sh，相当于把入口变为/bin/sh; 还有像 redis 镜像官网说明: docker run redis --requirepass passwd 运行一个带密码的容器；若在k8s的yaml、rancherUI（甚至是docker-compose）里面怎么配置呢，最简单办法 是查看这个镜像的dockerfile中的entrypoint ：docker-entrypoint.sh(只展示了部分代码) if [ \"${1#-}\" != \"$1\" ] || [ \"${1%.conf}\" != \"$1\" ]; then set -- redis-server \"$@\" fi 从上面代码，可以看出如果参数不是“-”开头或者\".conf\"结尾，会自动变成redis-server + args（args为自定义的参数），若参数是“-”开头或“.conf”结尾，会直接用CMD命令+参数；所以在k8s（docker-compose)配置文件args里面可以直接跟参数，如果第一个参数写成了redis-server也没关系，镜像入口文件已做处理；也可以直接定义command，但必须以redis-server开头+参数；为何是redis-server开头呢，这个是从redis的镜像cmd中获得的。 如果你使用rancher，直接在 UI界面，点击编辑-更多就会显示命令(Command)输入框，有两个输入框，分别是entrypoint和command；entrypoint对应yaml配置是command，command对应的yaml配置是args； 像redis加密，可以直接在rancher UI界面的command填写:--requirepass \"自己的密码\" 或redis-server --requirepass \"自己的密码\" ，甚至可以再entrypoint里面填写：redis-server --requirepass \"自己的密码\" 都可以，但不能在entrypoint中填写：--requirepass \"自己的密码\"。完成后可以验证，密码是否生效； 有一种情况：如果没有找到镜像的dockerfile，当run镜像后，到容器中的默认的目录，查看是否有个可执行的二进制文件，然后在command 里面设置 “二进制文件 -args ”但这个需要验证； 注意如果在args里面应用环境变量，要写成$(VALUE),不能写成$VALUE,如引用hostname，要写成$(HOSTNAME)，这相当于填写的是此变量的value ，而不是value name； 实战： packetbeat 官网镜像 使用参考 docker run -d \\ --name=packetbeat \\ --user=packetbeat \\ --volume=\"/etc/localtime:/etc/localtime:ro\" \\ --cap-add=\"NET_RAW\" \\ --cap-add=\"NET_ADMIN\" \\ --network=host \\ docker.elastic.co/beats/packetbeat:7.1.1 \\ --strict.perms=false -e \\ -E setup.kibana.host=ip:port \\ -E output.elasticsearch.hosts=ip:port 镜像后面的参数转换成配置文件 args: [ \"--strict.perms=false\", \"-e\", \"-E\",\"setup.kibana.host=ip:port\", \"-E\",\"output.elasticsearch.hosts=ip:port\" ] 如果使用rancher 直接UI上编辑 即可修改 "},"k8s/rancher_online_installation.html":{"url":"k8s/rancher_online_installation.html","title":"在线安装高可用的rancher","keywords":"","body":"Rancher server 高可用集群在线安装 搭建集群前提是 服务器数量为大于1的奇数 本案例用三台机器，采用rke搭建k8s集群,然后利用helm在此集群中部署七层负载的rancher server。 通过 RKE 工具部署 Rancher Kubernetes 集群 部署前准备工作 首先安装前需要提前准备必要的工具软件， 所需工具软件如下（只限于 Rancher 集群的三台节点使用） ✓ kubectl - Kubernetes 命令行工具。 ✓ rke - Rancher Kubernetes Engine 用于构建 Kubernetes 集群。 ✓ helm - Kubernetes 的包管理。 可以通过：https://www.cnrancher.com/docs/rancher/v2.x/cn/install-prepar e/download/ 下载相关文件 把事先下载完成的 rke 、helm 、kubectl 二进制文件 copy 到/usr/bin 目录下 chmod 777 rke_linux-amd64 kubectl_linux-amd64 helm mv rke_linux-amd64 /usr/bin/rke mv kubectl_linux-amd64 /usr/bin/kubectl mv helm /usr/bin/helm Rancher 集群的三台主机分别修改本机的 hosts 文件 cat > /etc/hosts ip1 hostname1 ip2 hostname2 ip3 hostname3 EOF 配置 Rancher 集群三台服务器之间的 ssh 免密登录 配置 Rancher 集群三台服务器之间的 ssh 免密登录 Rancher 集群的三台主机分别修改本机的 hosts 文件 cat > /etc/hosts ip1 hostname1 ip2 hostname2 ip3 hostname3 EOF 配置 Rancher 集群三台服务器之间的 ssh 免密登录 生成 ssh 免密访问公钥 ssh-keygen -t rsa -C \"备注信息\" 生成公私钥 将免密公钥推送集群内三台主机的 rancher账户(或其他可以使用docker的账号，centos不能使用root账号） ssh-copy-id -i ~/.ssh/id_rsa.pub rancher@ip1 以此类推 三台机器都推送，包括自身 完成后验证是否可以和集群内三台主机完成免密登录 RKE 创建 Rancher Kubernetes 集群 创建 rancher-cluster.yml 文件，用于 rke 推送集群配置使用，配置如下： nodes: - address: 10.128.119.47 user: was role: [controlplane,worker,etcd] - address: 10.128.119.48 user: was role: [controlplane,worker,etcd] - address: 10.128.119.49 user: was role: [controlplane,worker,etcd] services: etcd: snapshot: true creation: 6h 给提前下载好的 rke 工具配置可执行权限 chmod +x rke 执行集群部署操作 ./rke up --config rancher-cluster.yaml 完成后（如果镜像拉取快，15分钟左右），正常显示：Finished building Kubernetes cluster successfully。 使用 Kubectl 验证集群健康状态 mkdir -p ~/.kube cp kube_config_rancher-cluster.yml ~/.kube/config kubectl get nodes kubectl get cs 至此k8s集群安装完成 通过 helm 工具在k8s集群部署 Rancher Server 配置 helm kubernetes 集群权限 安装 helm 工具包 tar -xvf helm-v2.13.1-linux-amd64.tar.gz chmod +x linux-amd64/helm cp linux-amd64/helm /usr/bin/ 创建 helm ServiceAccount kubectl -n kube-system create serviceaccount tiller 创建 ClusterRoleBinding 以授予 tiller 帐户对集群的访问权限 kubectl create clusterrolebinding tiller \\ --clusterrole cluster-admin --serviceaccount=kube-system:tiller 初始化 helm 并安装 tiller（helm server 端） 查看 helm client 端的版本号 helm version helm init --skip-refresh --service-account tiller \\ --tiller-image \\ registry.cn-shanghai.aliyuncs.com/rancher/tiller:v2.13.1 tiller要和helm版本保持一致 准备安装 Rancher Server 相关证书 如果有自己官方证书，就用官方证书，如果没有证书，制作证书参考“证书申请与制作“章节 使用 helm 工具部署 Rancher Server 创建 namespaces kubectl create namespace cattle-system 创建 tls 密文 kubectl -n cattle-system \\ create secret tls tls-rancher-ingress --cert=./tls.crt --key=./tls.key 创建 ca 密文 kubectl -n cattle-system \\ create secret generic tls-ca --from-file=cacerts.pem 添加 helm chart 仓库地址 helm repo add rancher-stable https://releases.rancher.com/server-charts/latest 查看是否添加成功 helm repo list 执行 Rancher Server 创建操作 helm install rancher-latest/rancher \\ --name rancher \\ --namespace cattle-system \\ --set hostname=自己的域名 \\ --set ingress.tls.source=secret \\ --set privateCA=true 注：自定义域名和证书申请的域名要保持一致 验证 Rancher Server 运行情况 查看是否部署成功 helm list 查看 pod kubectl get deployment -n cattle-system 安装rancher server出错，可以用helm delete --purge rancher 删除后 重新执行 为Agent Pod添加主机别名 采用的是自定义域名而不是全局域名的话，需要在agent 里面配置自定义域名 如果是通过在/etc/hosts添加自定义域名方式指定的Rancher server访问URL，那么不管通过哪种方式(自定义、导入、Host驱动等)创建K8S集群，K8S集群运行起来之后，因为cattle-cluster-agent Pod和cattle-node-agent无法通过DNS记录找到Rancher server,最终导致无法通信。 可以通过给cattle-cluster-agent Pod和cattle-node-agent添加主机别名来解决； 在rancher UI界面，在system项目分别点击升级，进行编辑cattle-cluster-agent Pod和cattle-node-agent 点击显示高级选项；然后再网络里面 添加hosts别名，填写rancher server 服务器ip和自定义域名，完成后保存即可，待pod恢复后，状态会变成Active "},"k8s/rancher_offline_installation.html":{"url":"k8s/rancher_offline_installation.html","title":"离线helm安装高可用rancher","keywords":"","body":"rancher-server离线的高可用部署 本博客基于https://www.cnrancher.com/docs/rancher/v2.x/cn/installation/air-gap-installation/ha/ 官方文档补充和完善，主要是博主按照官网文档操作时，遇到一些问题，这里作为重点说明 1，提前搭建后本地仓库 harbor离线仓库搭建，参见harbor官网和我的install-harbor 文档，注意的时，如果用https协议，需要启用域名的，自定义域名要配置/etc/hosts 把自定义域名加进去 2，把rancher所需的镜像导入到镜像库 可以利用脚本执行 ./rancher-load-images.sh -l rancher-images.txt -i rancher-images.tar.gz -r 仓库地址/library 或者其他项目名称，如提前创建一个rancher ./rancher-load-images.sh -l rancher-images.txt -i ./rancher-images.tar.gz -r 172.16.35.31:1180/rancher 上面脚本和所需镜像获取，参考官方文档 https://www.cnrancher.com/docs/rancher/v2.x/cn/installation/air-gap-installation/ha/prepare-private-registry/ 3，二进制文件 kubectl rke helm等copy指定位置 这些二进制文件，下载地址https://www.cnrancher.com/docs/rancher/v2.x/cn/install-prepare/download/ 4，把镜像库的域名添加到其他机器hosts中、推送公钥等 需要注意如果仓库是https协议，自签名证书，需要把制作仓库证书生成的tls.crt 复制到其他docker主机上/etc/docker/certs.d/custom_domain/ca.crt 如：/etc/docker/certs.d/reg.czl.com/ca.crt 5，制作自签名证书 ./create_self-signed-cert.sh --ssl-domain=rancher.czl.com --ssl-trusted-ip=192.16.1.101 6，注意事项 剩下步骤按照官网文档：https://www.cnrancher.com/docs/rancher/v2.x/cn/installation/air-gap-installation/ha/install-kube/ 但是特别要注意下面事项 下面罗列一些注意事项 yaml配置文件 模板： nodes: - address: 192.16.1.101 # node air gap network IP user: zhenglin role: [ \"controlplane\", \"etcd\", \"worker\" ] private_registries: - url: reg.czl.com/library #如果用的私有签名https协议的harbor，此处用域名，不用IP，另外如果rancher镜像导入到library项目，URL需要带上library；如果是http协议的直接填ip就可以 user: admin password: \"P@ssw0rd\" is_default: true 安装Helm Server(Tiller) 这一步 可以从https://hub.docker.com/r/hongxiaolu/tiller/tags/ 获取相应版本tiller 也可以直接从 registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1 拉取，这个版本可以根据情况变化；然后再修改tag ,推送到私有仓库中 [root@Centos76 cert]# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1 reg.czl.com/library/rancher/tiller:v2.13.1 [root@Centos76 cert]# docker push reg.czl.com/library/rancher/tiller:v2.13.1 安装tiller helm init --skip-refresh \\ --service-account tiller \\ --tiller-image reg.czl.com/library/rancher/tiller:v2.13.1 先在chart解压目录下执行以下命令进行rancher安装，再来配置证书： helm install ../chart/rancher \\ --name rancher \\ --namespace cattle-system \\ --set hostname=rancher.czl.com \\ --set ingress.tls.source=secret \\ --set privateCA=true \\ --set rancherImage=reg.czl.com/library/rancher/rancher 一定要注意 其官方文档中--set ingress.tls.source=secret 没有换行符 所以不能直接粘贴用,set rancherImage时，后面rancher的版本不要带上tag，因为默认他会自动寻找，以下是带上v2.2.2的tag 出现的异常rancher-6f78c5bc69-g9djv 0/1 InvalidImageName 0 2m57s kubectl -n cattle-system edit deploy rancher image: reg.czl.com/library/rancher/rancher:v2.2.2:v2.2.2 可以看出 镜像的tag 被追加了 如果执行错了，可以用helm delete --purge rancher 删除后 重新执行 然后再到证书生成的目录 执行 kubectl -n cattle-system create \\ secret tls tls-rancher-ingress \\ --cert=./tls.crt \\ --key=./tls.key kubectl -n cattle-system \\ create secret generic tls-ca \\ --from-file=cacerts.pem 如下操作和在线安装文档一样 "},"k8s/deploy_app_in_rancher.html":{"url":"k8s/deploy_app_in_rancher.html","title":"rancher中快速部署应用","keywords":"","body":"rancher中快速部署应用 阅读前提：有docker知识储备，并对kubernetes有一定的了解 通过UI 部署应用 rancher在命名空间又抽象出一层，项目的概念（这个只是便于管理rancher抽象出来的，kubernetes中并没有这层）；我们可以先创建一个项目，然后在里面创建命名空间；这里为方便演示直接使用默认default的项目和default命名空间。 工作负载（deployment daemonset statefulset job等）部署 工作负载主要有deployment、statefulset、daemonset、job等类型，其实它们就是pod控制器，直接部署pod，很不容易控制；用不同类型控制器来生成和控制pod，就来的很方便了。如deployment是无状态的pod控制器，statefulset是有状态的控制器，一般用来部署数据库；daemonset是每个worker节点上都部署一个，一般用来采集worker节点的数据； job一次性的，完成后状态是completed，不再工作。这些控制器都有缩写的别名，具体可以参考博主的kubectl 常见命令使用。 1，在workloads tab中点击deploy 2，填写配置信息 填写镜像名称，副本数量、选择负载类型等关键信息 只有deployment和statefulset可以设置多个副本集； 3，端口映射 如图除\"Layer-4 Load Balancer\"外，还有三种选项；nodeport相当于四层负载一样，选择此选项后，在服务发现中会生成一个deployment同名-nodeport，这种格式的service；这个比较方便；访问集群任意节点ip+此nodeport端口 都能调度此deployment控制的pod上。hostport稍显鸡肋，pod调度到那个节点上，就可以通过节点ip+端口进行访问，如果没有调度的节点ip+端口是访问不了的。Cluster IP最好理解，就是基于它所在namespace组成的局域网，其他namespace或者外部是无法访问的；同一个namespace是可以直接访问的。 4，环境变量 除了可以使用自定义环境变量外，还可以使用内部的资源环境变量，如resource和field 5，调度规则 可以指定调度到（或不到）某个节点 也可以自定义匹配规则 如图中设置 调度只匹配amd64位的节点 在节点调度模块点击show advanced options后，还可以设置容忍度、亲和度匹配规则 6，健康检查 可以设置存活和就绪检查，默认是readliness 检查，设置完成后 存活检查也使用此规则；也可以点击右侧Define a separate liveness check增加一个存活检查。设置http和htpps方式检查 相对友好一下；tcp方式呢，可能应用占有端口还在，但已经不工作了；命令退出状态检查：需要提前把脚本放到镜像或挂载到容器中，执行此脚本后，返回码是否为0 作为判断的依据。 7，卷 这个不做特殊说明了，基本都是docker的知识；这是强调一下，可以把证书、configmap、证书挂载到容器中；如果选择挂载新的持久卷，会让创建一个持久卷；如果挂载一个已存在的持久卷，需要先创建一个持久卷。挂载空数据卷（ephemeral volume）这个比较简单。 8，更新策略 这个选择合适自己就行，如果这三种策略满足不了自己日常使用，可以自定义策略；默认选择\" Rolling: start new pods, then stop old\" 这个一般就能保证应用平滑更新；而且rancher UI上还支持回滚操作，很好保证了，升级错误能快速退回。 点击底部show advanced options后 ，能显示出剩下 使用频率相对较低配置项 9，command 关于entrypoint和command用法 有单独一章博客讲解docker run image -args对应yaml语法/rancher UI操作方式，这里不再赘述。 10，网络（networking） 网络这块，可以修改使用主机网络空间，一些host别名，以及一些DNS解析等配置； 11，标签和注解 主要是给此deploy下的pod打标签和注解的 12，安全 这一块配置也很重要，由于容器为了安全，是和外部环境是隔离的，想使用某些资源，就需要关闭或开启一些安全策略； 此模块还集成了资源限制和提权（最后的双侧复选框）；如果pod采用了HPA（水平扩展），一定要做好存活和资源限制配置。这样一旦触发了资源配置上线，就会自动扩展。 13，菜单功能 右上角菜单，也有很多功能；主要是编辑：修改deploy用的；clone：可以复制此deploy，然后做修改，保存。redeploy：重新部署，这个一般在configmap等更新后，可以重新部署一下。增加边车：可以增加一个辅助容器或者init容器；剩下的根据字面意思 就能知道它作用。最重要的是自己动手操作一遍，加上有一定k8s基础，很容易就找到对应关系并掌握UI上操作的方法。 负载均衡配置（ingress） ingress其实也是一种service，相当于增加一个支持7层负载的nginx，并带有域名；nginx（也可以是其他代理软件甚至是硬件）把接收到请求反向代理到匹配的pod上。新增ingress时，可以选自定义生成的域名，也可以使用自己购买的域名； 选择目标，可以是service或者工作负载（workload）；证书、标签注解都能在此UI配置。 服务发现配置 service可以这么理解：通过标签的键值对，选择匹配的pod，然后用户或其他应用访问入口是service，service调度满足匹配条件的pod上。 \"One or more external IP addresses\"和\"An external hostname\" 类似,把外部ip或者域名定义为内部dns，前者可以一次性定义多个ip，后者只能定义一个。如选择\"One or more external IP addresses\"类型，名称填写my-data;Target IP Addresses填写 172.17.1.4 ；此集群内部pod可以直接访问my-data，dns自动解析到172.17.1.4 \"One or more workloads\"和\"The set of pods which match a selector\"类似，前者相当于工作负载选择器，后者相当于标签选择器；前者通过标签选择工作负载（deployment、daemon-set、statefulset)，后者通过标签选择pod。 在rancherUI部署应用如果采用nodeport，会自动产生两个service，一同名service，一个是加上“-nodeport”的service；如果不想使用rancherUI自动生成的service，可以自己定义一个“The set of pods which match a selector” 标签选择器,这里面再来定义cluster_ip 或nodeport； \"Alias of another DNS record's value\" 这个暂时没未深入研究。 数据卷 数据卷就是持久化存储的 添加数据卷，填写名称，选择提前定义好持久卷，就可以使用了。 定义持久卷可以在storage中定义，有持久卷和存储类两种类型 存储类是可以动态扩展的；持久卷一般定义多少，最大就能使用这么多。存储技术现在很多样，kubernetes（rancher）支持 这么多，可以提前搭建好后，直接使用（如何搭建不同存储平台，这里不做描述）。 其他配置 资源 证书 这个一般是存放CA或其他机构颁发证书、甚至是自定义的证书文件，如tls.key tls.pem这类文件的；最后可以通过volume的形式挂载到容器 configmap 这个configmap 最常用的作用有两个，一个是编写环境变量，然后在工作负载部署时，环境变量可以直接引用此configmap，另一个就是存储应用配置文件，如上图；key就是配置文件名称，value就是配置文件内容；然后通过volume挂载到容器程序存放配置文件的地方。 如部署nginx时有三个配置文件，都写到configmap后，再把三个配置文件（键值对） 都挂载容器同一个目录，并且覆盖掉容器里面配置文件，可以在Mount Point中直接填写路径（只保留配置文件路径，不接配置文件名），Sub Path in Volume中不填写任何内容；如果想把configmap的文件挂载到容器，同时容器中默认的文件不被删除（有同名文件除外），Sub Path in Volume 填写挂载到容器中文件要显示的名称即可； Optional和items是匹配的，Optional为true，items 才可以设置，这个一般是用来只把configmap中 部分文件挂载到容器中使用的； 密文 这个一般用来存储敏感信息，比如密码；然后和configmap一样，可以通过环境变量引用，也可以通过volume挂载也可以；相对简单，操作一两遍 即可掌握。 命名空间和工具成员比较简单，略过 rancherUI 能配置的就这么多，配置不了，可以通过编辑yaml，来手动填写 通过导入yaml部署应用 这个是最简单的，把写好的yaml文件，导入进来就可以了；导入后，如果有问题，根据日志或者event提示，修改调整一下即可。需要说明的时，如果导入时，没有配置文件里面命名空间，需要提前创建命名空间。如果导入yaml文件，有创建命名空间操作，不能和已有命名空间重名；并且导入后，需要给移动到某个项目中。 如何验证部署应用是否运行正常 验证方法有很多，能达到目的就行。一定要灵活，不能把自己限定死。 除了常规的查看日志，通过浏览器直接访问，或进入shell ，进行验证外；我这里重点讲的是借助其他容器进行验证。 推荐如下两款镜像 hwchiu/netutils busybox 在待验证的应用同一个命名空间中，直接rancherUI中直接部署其中一款镜像，无需暴露端口；只需要在部署成功后，进入shell界面；busybox一般支持 nslookup 、telnet等命名，可以直接nslookup service-name/telnet service:ip 而netutils镜像集成工具相对更丰富一些，比如mongo-client redis-client有了这些数据库的客户端，就可以用它连接数据库，测试数据库是否能正常连接，加密是否生效；如redis-cli -h service_name 看看能连接部署redis，连接成功后可以再执行一些命令，如ping ，看看能不能执行成功；如果redis加密了，输入密码后再次执行ping，是否能成功等。 以上就是提供验证的思路，具体操作，需要自己根据情况来实践。 "},"k8s/use_appstore_deploy_es_in_rancher.html":{"url":"k8s/use_appstore_deploy_es_in_rancher.html","title":"利用rancher商店搭建es和beat","keywords":"","body":"rancher应用商店的使用 ​ 应用商店可以理解为helm源或charts仓库；启用后，找到自己要部署的应用，直接通过helm模板进行部署。在部署的时候 有些问答就是重置默认key-value配置的。 配置应用商店 全局启用、配置应用商店 默认只开启了基于Library源的应用商店，这个源由rancher官方维护，稳定性较强。\"Helm Stable\"由helm官方维护，稳定性也行，但没有针对rancher进行优化；\"Helm Incubator\"这个呢是helm社区维护的，稳定性一般。我们可以都设为enable后，进行使用 项目中启用应用商店 直接点击Launch，进行启用，启用后，会发现很多应用的helm源 添加其他charts仓库 可以添加自己私有chart仓库、以及其他企业的（如阿里、elastic、bitnami）到应用商店； 可以选择为全局、集群、项目三个范围内的资源，根据情况灵活添加，如果添加为项目范围的，只能在此项目中使用此helm源（商店）；使用私有helm源，需要提供用户名和密码； 下面罗列了几个比较重要的charts仓库地址。 阿里charts仓库 https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts bitnami https://charts.bitnami.com elastic 如添加elastic helm源 应用商店搜索elastic 发现有很多Elastic-Helm提供的。 说了这么多，接下来真正的实战。 实战 利用rancher 应用商店搭建 elasticsearch+kibana+apm-server 应用商店搜索所需应用 rancher应用商店 搜索efk；我们选择来自Library，这个已经集成了elasticsearch+kibana+fluent；点击View Details,进入配置自定义页面。 命名空间，这些可以指定如果不知道就会部署到默认的efk中。 选项配置： jvm也可以根据情况来调整 ，默认512 m，就可以；调整1g的话，如果运行多个实例在一台机器 较吃力，如果机器性能好，可以适当调整大一些。使用默认镜像选择false后，就可以指定镜像，这里面镜像版本相对落后，从elastic官网镜像https://www.docker.elastic.co/#获取到当前最新稳定版，然后填写进去（这里镜像名称和tag输入框不在一起）。 这里不需要f(luent),就不启用，镜像名称也无需修改了。 调整与完善 elasticsearch参数调整 自定义配置完成后点击launch，待镜像都拉取完成后；根据提示继续完善和调整。这里面elasticsearch组件默认使用的是空数据，我们如果修改为持久卷的话 ，不管是主机是映射，还是持久申明的卷，都要给777权限 我这里没有搭建存储平台，就采用了本地Local Node Path做的持久卷（和主机目录映射差别不大）， 主机映射那个目录只能被1个elasticsearch使用，所以configmap中，变量discovery.zen.minimum_master_nodes 此处修改为1；（这样把elasticsearch主机调度到一台机器上就行了）；discovery.type = single-node 如果上个参数修改为1的话，discovery.zen.ping.unicast.hosts 这个配置在7.0版本后已经不建议使用了 ，也可以去掉 若继续使用默认的空数据卷，elasticsearch单pod也可以按照这种方式修改，具体环境变量参数看截图（在Resources-config maps中可以找到这些变量） 注：node.data=true 这个参数要添加上，避免多次安装出现的问题 kibana参数调整 首先查看对应镜像版本，说明文档https://www.elastic.co/guide/en/kibana/7.1/docker.html#environment-variable-config Docker defaultsedit The following settings have different default values when using the Docker images: server.name kibana server.host \"0\" elasticsearch.hosts http://elasticsearch:9200 xpack.monitoring.ui.container.elasticsearch.enabled true 然后和kibana的configmaps中定义环境变量参数对比一下 若还想调整es和kibana日志 也可以在环境变量中设置 kibana的设置： LOGGING_QUIET true LOGGING_VERBOSE false es设置： logger.org.elasticsearch.transport warn 部署apm-server 应用商店继续搜索：apm-server，直接进入查看详情页面，然后把此应用 指定到和efk 应用同一个命名空间里 apm-server 配置文件和环境变量的调整 到secrets（有的是config map）进行配置， 有个名称为apm-server.yml 的key，其value按照如下修改，把数据写入文件的配置改为写入到elasticsearch中。 另外，需要在环境变量中 加入两个参数： setup.dashboards.enabled=true config.output.file.enabled=false 因为这个在应用商店安装的apm-server，默认是把数据存到file的，前面修改了输出为elasticsearch，这里再把输入到文件设为false；（自己手动部署的 并不需要，应用商店的需要检查一下） 查看apm-server，进行验证 然后点击Launch APM，就可以使用了。如果根据页面提示把apm-agent 嵌入被测应用后，再查看apm-server采集到的数据 若探针采集的数据已经发送过来了 点击进去可以查看详细数据； 手动部署filebeat+metricbeat+packetbeat向es发送数据 部署filebeat 这个其镜像详情里面就提供k8s部署的yaml，稍作修改就可以使用 https://www.elastic.co/guide/en/beats/filebeat/版本号/running-on-kubernetes.html 下载所需yaml后，修改里面命名空间到自己制定的空间 ，直接在rancher中部署完成后修改输出elasticsearch地址（config maps中）,如果是在环境变量中定义elasticsearch地址，直接就在环境变量中修改，这个视情况而定。 部署metricbeat 这个有点复杂些，官方也提供了(具体下载根据版本情况）k8s所需的yaml文件： https://www.elastic.co/guide/en/beats/metricbeat/6.6/running-on-kubernetes.html 把官网提供的yaml下载下来后，用rancher导入到kube-system空间中； 他会部署两个metricbeat，一个daemonset 一个deployment，deployment可以暂停；如果没有kube-state-metrics组件，因为设定了依赖关系，他也会自动部署一个；然后修改配置文件config-map ,第一个配置文件metricbeat.yml：设置抓取的信息要输入到正确的elasticsearch和kibana中 或es输入直接设置为 output.elasticsearch.hosts: ['${ELASTICSEARCH_HOST}'] 把多余的变量删除掉，直接把ELASTICSEARCH_HOST 在环境变量中定义为 http://es_ip:port 第二个配置文件kubernetes.yml module: kubernetes metricsets: ​ \\- node ​ \\- system ​ \\- pod ​ \\- container ​ \\- volume period: 10s host: ${NODE_NAME} \\#hosts: [\"localhost:10255\"] \\# If using Red Hat OpenShift remove the previous hosts entry and \\# uncomment these settings: hosts: [\"https://${HOSTNAME}:10250\"] ssl.verification_mode: \"none\" ssl.certificate_authorities: ​ \\- /etc/kubernetes/ssl/certs/serverca ssl.certificate: \"/etc/kubernetes/ssl/kube-node.pem\" ssl.key: \"/etc/kubernetes/ssl/kube-node-key.pem\" 需要注意地方: 第一个hosts: [\"localhost:10255\"] 修改为hosts: [\"https://${HOSTNAME}:10250\"] 参照https://www.elastic.co/guide/en/beats/metricbeat/current/running-on-kubernetes.html红帽的配置方法 然后把/etc/kubernetes/ssl/certs/serverca、/etc/kubernetes/ssl/kube-node.pem和/etc/kubernetes/ssl/kube-node-key.pem 在工作负载界面通过主机映射挂载进去就行了 （如果没有这些证书文件，是无法从相关接口获取数据，这些文件都是用来鉴权的）。 最后在环境变量中把正确的es地址信息填写进去就可以了 部署kube-state-metrics 如上图中用到kube-state-metrics的数据，没有的话，会自动部署一个；但kube-state-metrics没有部署成功，后者提供的镜像有问题怎么解决呢； 我们可以从https://github.com/kubernetes/kube-state-metrics/blob/master/kubernetes/kube-state-metrics-deployment.yaml 找到正确的镜像文件名称，对照修改镜像名称为：quay.io/coreos/kube-state-metrics:v1.5.0 即可完成部署（现在可能是1.6版本了）。这个地址也有部署需要的各种yaml文件，需要注意不同的版本；我们把 kube-state-metrics-service.yaml、kube-state-metrics-service-account.yaml、kube-state-metrics-deployment.yaml 等所有yaml文件下载下来，先导入SA(service-account)类的yaml，剩下的yaml再逐个导入； 由于master版本可能不确定是那个版本，建议选用release中的具体版本；通过rancher把这些yaml导入到系统空间\"kube-system\" yaml文件里面多个镜像仓库是谷歌的，如果你没有科学上网的方法，就像我一样，换成其他公司同名镜像；如image: k8s.gcr.io/addon-resizer:1.8.3，可以在dockerhub上搜到其他公司提供的;image: siriuszg/addon-resizer:1.8.4，我没有搜索到1.8.3 就选更高版本的1.8.4的了，一般原则是相同版本，然后dockerhub排名较高的镜像 部署完成后，把kube-state-metrics的8080端口映射出来，我们通过浏览器访问验证一下： 访问kube-state-metrics页面会显示健康 \"/health\" 相关指标。 最终采集到的数据在kibana呈现验证： 部署：packetbeat 这个相对简单，官方只提供的了docker的部署方式，我们对着参数 部署到rancher即可： https://www.elastic.co/guide/en/beats/packetbeat/current/load-kibana-dashboards.html docker run --net=\"host\" docker.elastic.co/beats/packetbeat:7.2.0 setup --dashboards 入口命令怎么在rancherUI里面配置，单独一章博客有介绍image和container中的entrypoint、cmd关系 另外还要增加 --strict.perms=false -e -E output.elasticsearch.hosts=ip:port使数据发送到之前搭建的es里面，--net=host 在rancherUI对应的操作是：networking模块把\"Use Host's Network Namespace\"设为真; packetbeat是用来采集host（宿主）机器的数据，部署到容器中，有些数据可能抓取不到;可能需要提权,也可以在rancherUI最后配置项\"Add Capabilities\" 里面把下面两项增加上。 --cap-add=\"NET_RAW\" \\ --cap-add=\"NET_ADMIN\" \\ 经过多次验证，把如下文件挂载到容器，基本和在host（宿主）机器上部署packetbeat效果一样。 volumeMounts: - name: cgroup mountPath: /host/sys/fs/cgroup/ readOnly: true - name: proc mountPath: /host/proc/ readOnly: true - name: docker-sock mountPath: /var/run/docker.sock readOnly: true volumes: - name: cgroup hostPath: path: /sys/fs/cgroup/ - name: proc hostPath: path: /proc/ - name: docker-sock hostPath: path: /var/run/docker.sock 最后有一点要注意 把主机时间也映射到所有的容器中，避免容器时间不一致导致的一系列问题。 "},"k8s/how_to_use_kubectl_noserver.html":{"url":"k8s/how_to_use_kubectl_noserver.html","title":"rancher平台不可用下如何使用kubectl命令","keywords":"","body":"如何在rancher平台宕机情况下，继续使用k8s集群 前提 首先保障rancher管理k8s集群的config文件还在，可以查看~/.kube/config 进行验证 kubectl config view kubectl config use（或者use-context ） xxx ；切换到不同配置项（master上的配置），来使用kubectl #xxx是contexts的name值 验证： kubectl get nodes kubectl get ns 验证是否可以看到节点和命名空间 使用kubectl命令来操作集群 ]$ kubectl set image deployment/reactor reactor=myimage:2.7.1 -n myns --dry-run(空跑一遍 进行验证） ]$ kubectl set image deployment/reactor reactor=myimage:2.7.1 -n myns 验证 ]$ kubectl get deployment/reactor -n myns -o yaml 若rancher彻底不可恢复 待kubectl命令可以使用后，在新搭建rancher平台中，把此集群导入进去，导入完成后 要重建项目，把之前的命名空间移过去即可。 具体导入命令，参考rancher官网 "},"jenkins/jenkins-slave-for-docker.html":{"url":"jenkins/jenkins-slave-for-docker.html","title":"Jenkins调用docker编译程序","keywords":"","body":"如果用docker 容器编译程序 有两种方案可供选择 1，激活镜像作为slave编译 采用Jenkins提供的jnlp-slave 或ssh-slave 标准镜像二次封装，或者初始镜像，然后通过label 选择镜像后进行编译； 这种编译的原理是：Jenkins通过标签选择相关docker镜像，并激活成容器，把此容器当做slave（节点机）使用； ​ Jenkins提供的镜像地址：https://hub.docker.com/u/jenkinsci ​ jnlp-slave 和ssh-slave 镜像都能激活作为slave节点使用，区别是采用不同协议连接到容器内部：ssh和jnlp； 使用这种方式需要额外在Jenkins里面配置，插件里面安装docker插件，然后配置Docker Host URL,来找到可以使用的docker，如果Jenkins和docker在同一台服务器可以直接填写为：unix:///var/run/docker.sock ，如果不在同一台机器要填写docker所在的机器ip：tcp://ip:2375，并且要在docker所在机器的 /etc/docker/daemon.json 里面添加2375端口 \"hosts\": [ ​ \"tcp://0.0.0.0:2375\", ​ \"unix:///var/run/docker.sock\" ​ ] 具体怎么调用这里不作为重点，这里重点说明的方案的选择，可以参考 https://blog.csdn.net/qq_31977125/article/details/82999872 这位博主有详细的操作步骤 下面是重点说明编译方式 2，直接docker run编译 把源码下载到宿主机，通过-v 挂载到容器中，然后指定入口命令编译此目录，编译完成后 销毁容器 docker run --rm -v `pwd`:/mypro -w /mypro nodeshift/centos7-s2i-nodejs:10.16.0 /bin/bash -c \"npm install\" docker run --rm -v `pwd`:/opt/mypro -w /opt/mypro goenv-centos:test7 /bin/bash -c \"GOPROXY=https://goproxy.io /usr/local/go/bin/go build\" 上面两个是例子，goenv-centos:test7是自制作的镜像 注意：提前制作好镜像，镜像里面把各种编译依赖放进去，制作镜像有两种方式 ​ 激活一个基础镜像，编译下载各种依赖库，能正式编译后，commit容器为镜像，这种方式不推荐 ​ 建议把操作步骤整理起来 编写到dockerfile中 实战案例 基于centos7.6 镜像制作出可以编译go/rust程序的镜像（生产中最好一个镜像只编译一种语言程序，并且是基于此语言的镜像） Dockerfile FROM centos:7.6.1810 WORKDIR /opt ADD https://github.com/facebook/zstd/releases/download/v1.4.0/zstd-1.4.0.tar.gz ./ ADD https://github.com/edenhill/librdkafka/archive/master.zip ./ ADD https://studygolang.com/dl/golang/go1.12.5.linux-amd64.tar.gz ./ COPY expect.sh ./ RUN mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.bak && \\ curl -s http://mirrors.aliyun.com/repo/Centos-7.repo -o /etc/yum.repos.d/CentOS-Base.repo && \\ curl -s http://mirrors.aliyun.com/repo/epel-7.repo -o /etc/yum.repos.d/epel-7.repo && \\ yum repolist && \\ yum -y install wget make expect gcc gcc-c++ unzip openssl-devel clang-devel libpcap-devel perl.x86_64 \\ which.x86_64 git && \\ curl -sSf https://sh.rustup.rs -o rustup.sh && chmod a+x rustup.sh && \\ chmod a+x expect.sh && /bin/bash ./expect.sh && \\ source $HOME/.cargo/env && \\ tar xzf zstd-1.4.0.tar.gz && \\ tar xzf go1.12.5.linux-amd64.tar.gz -C /usr/local && \\ unzip master.zip && \\ cd zstd-1.4.0 && CFLAGS=\"-O3 -fPIC\" make install && \\ cd ../librdkafka-master && ./configure --prefix=/usr && make && make install && \\ cp -rf /usr/lib/librdkafka* /usr/lib/pkgconfig /usr/lib64 && \\ chmod a+x -R /usr/local/go && \\ echo -e 'export GOPROXY=https://goproxy.io' >> /etc/profile && \\ echo -e 'export GOROOT=/usr/local/go' >> /etc/profile && \\ echo -e 'export GOPATH=/opt/mypro' >> /etc/profile && \\ echo -e 'export export PATH=$GOROOT/bin:/root/.cargo/bin:$PATH' >> /etc/profile && \\ echo -e 'export LIBRARY_PATH=$LIBRARY_PATH:/usr/lib64/llvm' >> /etc/profile && \\ echo -e 'export LD_LIBRARY_PATH=/usr/local/lib/:${LD_LIBRARY_PATH}' >> /etc/profile && \\ source /etc/profile && \\ yum clean all && \\ rm -rf /opt/* && \\ mkdir -p /opt/mypro COPY ustc-config /root/.cargo/config ustc-config [source.crates-io] registry = \"https://github.com/rust-lang/crates.io-index\" replace-with = 'ustc' [source.ustc] registry = \"git://mirrors.ustc.edu.cn/crates.io-index\" expect.sh #!/usr/bash expect 把Dockerfile，和expect.sh、ustc-config放在同一目录，然后在此目录执行build命令： docker build -t 172.16.35.31:1180/apm-images/centos7-goenv:1:1 . go程序编译： docker run --rm -v `pwd`:/opt/mypro -w /opt/mypro 172.16.35.31:1180/apm-images/centos7-goenv:1 /bin/bash -c \"GOPROXY=https://goproxy.io /usr/local/go/bin/go build\" rust程序编译 docker run --rm -v `pwd`:/opt/mypro -w /opt/mypro 172.16.35.31:1180/apm-images/centos7-goenv:1 /bin/bash -c \"/root/.cargo/bin/cargo build --release\" 这种方式编译最大优点，不管是开发、运维还是测试都无需搭建编译环境（有时候搭建一套编译环境是费时费力的事情，而且移植性差），直接从docker仓库拉取此镜像后就能编译； "},"jenkins/install-jenkins.html":{"url":"jenkins/install-jenkins.html","title":"利用docker 镜像，快速搭建Jenkins环境","keywords":"","body":"利用docker image快速搭建Jenkins环境 关于安装部署Jenkins，网上一大堆资料，这里不做详细说明了；可以下载Jenkins的war包直接放到tomcat（其他Java容器也行）通过ip:8080/jenkins即可访问，也可能通过 java –jar Jenkins.war来安装或者带上--ajp13Port=-1 --httpPort=8081参数指定端口就行； 这里重点说的是采用docker化部署，这种方案更快捷和灵活： docker run \\ --name myjenkins \\ --cpus=4 \\ --restart=unless-stopped \\ -u root \\ -d \\ -p 88:8080 \\ -p 50002:50000 \\ -v /home/jenkinsci/jenkins:/var/jenkins_home \\ -v /opt/jenkins-bak-file:/opt/jenkins-bak-file \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /etc/localtime:/etc/localtime:ro \\ -v /etc/timezone:/etc/timezone:ro \\ -v $HOME/.ssh:/root/.ssh:ro \\ jenkinsci/blueocean /etc/timezone 如果没有可以创建一个:echo Asia/Shanghai >/etc/timezone 这个 一定要设置，要不然就是GMT时区了； /opt/jenkins-bak-file目录是为以后Jenkins自身备份预留的目录； 把.ssh挂载进去，让容器使用宿主机的私钥； 如果使用jenkins:2.60.3镜像的话，宿主机的/home/Jenkins 目录一定要sudo chown -R 1000 处理一下；但不推荐这种镜像，已经不支持blueocean了； "},"shell/get_dir_in_shell.html":{"url":"shell/get_dir_in_shell.html","title":"shell 脚本中获取脚本所在路径","keywords":"","body":"shell脚本中获取路径两种方法 第一种 DIR=$(cd $(dirname $0) && pwd ) echo $DIR 第二种 DIR2=$(cd $(dirname \"${BASH_SOURCE[0]}\") && pwd ) echo $DIR2 上面两种方法都可以获取到当前路径，但第二种方法只适用于含有bash命令的系统，若系统只有sh命令，建议采用第一种方式 "},"shell/sed_use_hard.html":{"url":"shell/sed_use_hard.html","title":"sed不常见用法","keywords":"","body":"1 sed -i '/ADMIN URL/s/localhost/172.17.0.1/; /DATA URL/s/localhost/172.17.0.1/; s/localhost:8082/172.17.0.1:8082/; N;/16379/s/localhost/172.17.0.1/;P;D;' $CONFIG_DIR/fusion_config/production.js sed 可以一次性执行多行代码，每行代码用分号隔开；上面代码块，前两行表示定位后进行替换操作，第三行代码匹配后就替换；第四行代码是模式空间指针到16379后（模式空间有两行），把模式空间里面的localhost替换掉。大写PD 表示打印模式空间第一行和删除第一行； 2 sed -i '/^\\s$/d' test.txt 删除空白行 sed -i 's/\\s//' test.txt 表示删除每行开头的空格 sed -i 's/\\s//g' test.txt 表示删除每行所有的空格 sed -i 's/\\s//;s/\\s$//g' test.txt 删除行首和行尾的空格 也可以拆开写 把删除行尾的空格单独为：sed -i 's/\\s$//g' 正则表达式\\s表示任意空白字符 不管是tab 还是空格 sed 模式匹配的g参数： g是起到一个全局的作用，这个范围是每一行，也就是说是一行为单位，作为一个全局。+g :匹配每一行有行首到行尾的所有字符 所以慎用参数g 3 指定行修改，如第4行末尾追加一行，内容为test sed -i 'N;4atest' test.txt 第4行行首追加一行，内容为test sed -i 'N;4itest' test.txt "},"shell/getopt_and_getopts_use.html":{"url":"shell/getopt_and_getopts_use.html","title":"getopt与getopts用法","keywords":"","body":"getopt 与 getopts用法详解 我们经常使用脚本 后面跟参数这种用法，这个时候使用getopt/getopts再合适不过了；下面就来详细说明 getopt （系统外部用法，后来增加的）与 getopts（内部，不支持长选项 只能是单个字符的短选项）的用法 先看一段代码 #!/bin/bash set -e set -o pipefail cmd=$(basename $0) defaultHost=\"127.0.0.1\" defaultPort=\"2222\" defaultFusion=\"http://$defaultHost:3333\" defaultUser=\"myuser\" defaultPassword=\"123456\" host=$defaultHost port=$defaultPort fusion=$defaultHost user=$defaultUser password=$defaultPassword if which getopt &>/dev/null; then optExist=\"true\" fi Usage() { echo \"帮助说明\" echo \"这个脚本如何使用，如参数 a f h 后面跟什么值，或者长参数appid fusion help 后面跟什么值\" } if [[ \"${optExist}\"x = \"true\"x ]]; then ARGS=$(getopt -o \"a:f:h\" -l \"appid:,fusion:,help\" -n \"${cmd}\" -- \"$@\") eval set -- \"${ARGS}\" while true; do case \"${1}\" in -a|--appid) shift ; if [[ -n \"${1}\" ]]; then appid=\"${1}\" shift ; fi ;; -f|--fusion) shift ; if [[ -n \"${1}\" ]]; then fusion=${1} shift ; fi ;; -h|--help) Usage exit 0 ;; --) shift ; break ; ;; *) Usage exit 0 ;; esac done else while getopts a:f:h opt; do case $opt in a) appid=$OPTARG ;; f) fusion=$OPTARG ;; h) Usage exit 0 ;; ?) Usage exit 1 ;; esac done fi if ! [[ $appid =~ [0-9a-fA-F]{24} ]]; then echo -e \"\\033[1;41;37mInvalid appid!\\033[0m\\n\" Usage exit 1 fi if ! [[ $fusion =~ ^http:// ]]; then fusion=\"http://${fusion}:3000\" fi echo -e \"\\033[32mSending request...\\033[0m\" curl -H \"Content-Type:application/json\" -X PUT --data \"{\\\"app_id\\\": \\\"$appid\\\"}\" \"$fusion/api/v1/app/indexes\" echo -e \"\\033[32mFinished!\\033[0m\" exit 0 代码详解： 先看getopt 与getopts 帮助文档 #[root@gitbook ~]# getopt --help #Usage: # getopt optstring parameters # getopt [options] [--] optstring parameters # getopt [options] -o|--options optstring [options] [--] parameters #Options: # -a, --alternative Allow long options starting with single - # -h, --help This small usage guide # -l, --longoptions Long options to be recognized # -n, --name The name under which errors are reported # -o, --options Short options to be recognized # -q, --quiet Disable error reporting by getopt(3) # -Q, --quiet-output No normal output # -s, --shell Set shell quoting conventions # -T, --test Test for getopt(1) version # -u, --unquoted Do not quote the output # -V, --version Output version information …… #[root@gitbook ~]# getopts # getopts: usage: getopts optstring name [arg] getopts a:f:h opt 表示a f h 必须取值，使用选项取值时，必须使用变量OPTARG保存该值 basename $0值显示当前脚本或命令的名字 shift 命令每执行一次，变量的个数($#)减一，而变量值提前一位 比如shift 3表示原来的$4现在变成$1，原来的$5现在变成$2等等，原来的$1、$2、$3丢弃，$0不移动。不带参数的shift命令相当于shift 1 $#: 参数的个数 $*: 参数列表 所有的参数 作为一个整体 如 ./xx.sh 1 2 3, $*值为“1 2 3” $@: 参数列表 所有参数 逐个输出 如 ./xx.sh 1 2 3 $@ 值为\"1\" \"2\" \"3\" set -- \"${ARGS}\" 相当于重置了 执行脚本 后跟参数，把执行脚本后跟参数‘$@’ 变成了 ${ARGS} 代码大意 先判断是否有getopt命令，如果有就用这个命令 没有的话 采用getopts命令，这个命令简单，不再做特殊说明； getopt 判断体中 首先判断 第一个参数如果是 -a|--appid，则shift 后移一位 也是就是-a 参数后面的值（如果有的话）保存起来； shift 执行一次，然后$1(${1}写法也可以)就变成第三个参数了， 如果 -f|--fusion是第一个参数，操作效果和上面一样；如果第一个参数是 -h|--help 的话调用Usage方法并执行exit 0 退出； 如果第一个参数是 -- 则shift 一下，break 跳出整个循环，（continue一般是 跳出当前循环，然后开始新的循环）； 如果第一个参数是为其他(?通配其他内容)调用Usage方法并执行exit 1退出 其他代码较简单 不做讲解说明 "},"shell/ps3_use.html":{"url":"shell/ps3_use.html","title":"选择项用法","keywords":"","body":"界面选择项用法 PS3=\"Enter option: \" select option in \"Install All\" \"Install Elasticsearch\" \"Install Mongo\" \"Install Nginx\" \"Install Kafka\" \"Install Redis\" \\ \"Install Druid\" \"Reboot System\" \"Exit Program\" do case $option in \"Exit Program\") break ;; \"Install All\") install_all ;; \"Install Elasticsearch\") install_es ;; \"Install Mongo\") install_mongodb ;; \"Install Nginx\") install_nginx_only ;; \"Install Redis\") install_redis_only ;; \"Install Kafka\") install_kafka_only ;; \"Install Druid\") install_druid ;; \"Reboot System\") reboot_system ;; *) echo \"Sorry. Wrong selection\" esac done 最终形成界面多选项，选择某一项的序号，就可以执行此选项脚本代码 "},"jmeter/use_jmeter_test_app.html":{"url":"jmeter/use_jmeter_test_app.html","title":"利用jmeter模拟手机接口测试","keywords":"","body":"利用jmeter模拟手机接口测试 本文示例是从网上找到的月光茶人APP程序 首先手机操作月光茶人app一个完整的购买支付流程 我们在监听平台中查看其产生的url（接口），下列列表为手机操作支付流程时，监听平台采集到数据 现实测试的APP，我们可以通过开发提供的api文档、抓包工具如fiddler，抓取app的访问请求，都可以获取到接口URL；如何获取具体接口需要灵活应变；通过浏览器访问的程序可以直接通过Chrome调试network就能获取到接口URL. 上面列表是手机操作月光茶人APP：登录、首页列表、产品列表、加入购物车、成功加入到购物车、加入预购订单、预购订单详情、选择支付、订单提交成功产生的URL接口； 这9步构成一个完整的流程；我们把这9步的http请求加入到jmeter里面 通过监控平台采集到URL进行分析，发现其他步骤会用到登录后产生的返回体里面appCartCookieId和appLoginToken动态参数，所以我们要在登录请求后面加入正则表达式提取器 来提取，它返回的参数 .\"appCartCookieId\":\"(.+?)\". 这个正则表达式 要提取appCartCookieId：后面\"\"里面包含的内容 $1$表示 当有多个正则表达式时，只获取第一个，匹配数字1，表示从第一个开始；匹配数字，-1表示取出所有匹配值 0是随机，1 、2 表示匹配第几个 ​ 如果有多个值和appCartCookieId匹配，一定要用$1$这种形式来选择值，若有极端情况，有多个匹配值且位置不定 如：“address\":{\"area\":{\"store_id\":\"1\",\"shippingGroup\":\"\",\"pathNames\":\"中国/广东省/深圳市/宝安区/福永/福围-下沙南\",\"name\":\"福围-下沙南\",\"id\":\"1000000\",\"pathNames4Print\":\"深圳市宝安区福永福围-下沙南\"},\"isDefault\":\"1\",\"telephone\":\"18812341234\",\"id\":\"100347013e14430696ec765ff464429c\" 取\"18812341234后面的id，可以写成\"18812341234\"\\,\"id\":\"(.+?)\".* ​ 如果手机号是变动的，可以写成\"1[3|4|5|8][0-9]\\d{4,8}\"\\,\"id\":\"(.+?)\".* 手机号正则表达式不能写成：^1[3|4|5|8][0-9]\\d{4,8}$，^和$表示行开始和结束，要去掉，这里手机号并不是独占一行。 接下的步骤就可以引用这两个参数，如下图可以写成parameters里面引用参数，也可以直接在body data里面编写多个参数，多个参数用&来连接 如果想在路径里面使用上一个请求产生的参数，body data或者parameters必须带上这个参数，哪怕请求body体用不上这个参数 最后添加用于查看结果的“查看结果树”和“聚合报告”，在“查看结果树”里面可以详细看到响应的数据、请求数据、取样结果等信息 聚合报告汇总了接口访问总量错误信息等关键指标 也可以把抓包获取的header添加到jmeter里面（模拟的更真实一些，表头一般是存储设备等信息的） 这样发送过来的请求,监控平台上设备就显示为iphone了 模拟登录月光茶人APP后选购支付流程大量并发的实现 如果APP对登录有限制，同一账号只能同时登录一次，且手里没有多余的账号如何进行并发测试呢，这个时候只需单独对登录http请求进行控制即可；其他请求操作可以放在一块进行并发测试； 新建一个setUp Thread Group ​ 使用这个进程组的好处时，他可以和tearDown Thread Group一起使用，构成一个 登录+中间各种操作/请求+退出的流程（单独使用setUp、tearDown也可以），登录请求放在setUp Thread Group,退出请求放在tearDown Thread Group里面，剩下的各种操作http请求放在线程组里面，我们此处没有用到退出操作就不需要新建tearDown Thread Group线程组了； 如下图，在setUp Thread Group里面添加登录http请求后，我们需要获取appCartCookieId和 appLoginToken参数并且要全局化，下面其他进程中的http请求能继续使用；首先用正则表达式提取器提取相关参数，具体操作步骤前面有说过，不再赘述 “(.+?)”.* , (.+?)表示惰性匹配，表示从“开始，然后匹配到” 然后存起来；用\\1 \\2 或者$1 $2 取出第一个 第二个字符； 使用全局变量 添加后置处理器BeanShell PostProcessor，把上一步正则表达式提取器提取参数全局化；如下图 parameters参数填写正则表达式提取器提取的参数，然后在script模块进行全局化申明： String appCartCookieId = bsh.args[0]; ​ print (appCartCookieId); ​ ${__setProperty(newappCartCookieId,${appCartCookieId},)} 引用全局化参数 在其他进程组里面，进行引用全局化参数，引用格式：${__P(newappCartCookieId,)} 上图除了全局变量外，还引用了其他参数： _terminal-type=ios&appCartCookieId=${P(newappCartCookieId,)}&appLoginToken=${P(newappLoginToken,)}&userId=e19fd14f3ebf48bcbc79d09d6775ff04；也可以写成parameters的形式，详细讲解可以参考：http://www.cnblogs.com/allen-zml/p/6552535.html 可以在登录线程组里面添加http信息头管理，填写设备信息tid、uid等这样模拟出来的请求更接近iOS移动设备发出的请求； 控制吞吐量 确定要添加控制吞吐量的位置后，添加-定时器-Constant Throughput Timer，然后填写如图相关信息 如果想控制每秒2个并发，红色区域1填写120即可，如果Constant Throughput Timer添加到所有线程组的前面，都要用到此控制器，下拉选择all active threads选项；如果放到某一进程组，只供此进程组使用，可以选择this thread only； "},"ca/make_key.html":{"url":"ca/make_key.html","title":"私有证书制作","keywords":"","body":"制作自签名证书 1，手动制作自签名证书（NGINX用） openssl req -newkey rsa:2048 -nodes -keyout tls.key -x509 -days 3650 -out tls.pem -subj /C=CN/ST=BJ/L=CY/O=DCLINGCLOUD/OU=APM/CN=apptrace/emailAddress=ca@dclingcloud.cc 注：-keyout 和 -out 可以修改为输出路径+文件名称，名称可以自定义 字段说明： C=CN // 国家代号，中国输入CN ST=BJ // 州（省）名 L=CY // 所在地市的名称 O=DCLINGCLOUD // 组织或者公司名称 OU=APM // 部门名称 CN=apptrace // 通用名，可以是服务器域控名称，或者个人的名字 emailAddress=ca@dclingcloud.cc // 管理邮箱名 2，利用脚本制作所需证书（可以用在harbor和kubernetes、rancher部署上） 脚本下载地址（rancher中国提供） https://github.com/xiaoluhong/server-chart/blob/v2.2.4/create_self-signed-cert.sh 脚本使用示例 cat /etc/hosts 192.16.1.100 Centos76 reg.czl.com 具体名称和设置的自签名域名一致 ./create_self-signed-cert.sh --ssl-domain=reg.czl.com --ssl-trusted-ip=192.16.1.100 详细的用法可以参考脚本下载链接 "},"nginx/direct_static_web.html":{"url":"nginx/direct_static_web.html","title":"如何利用nginx指向本地静态网页","keywords":"","body":"NGINX指向静态网页 server { listen 8081; #也可以指派其他端口 server_name localhost; root /home/apptrace/tracing/dashboard; #如果NGINX非root用户运行，不要放在root目录下 autoindex on; #开启索引功能 autoindex_exact_size off; # 关闭计算文件确切大小（单位bytes），只显示大概大小（单位kb、mb、gb） autoindex_localtime on; # 显示本机时间而非 GMT 时间 location / { try_files $uri $uri/ /index.html; } "},"nginx/load_balance.html":{"url":"nginx/load_balance.html","title":"如何利用nginx实现负载均衡和反向代理","keywords":"","body":"nginx实现负载均衡 upstream ygcr { server 192.16.35.30:38080 weight=1; server 192.16.33.20:38080 weight=1; ip_hash; } 利用upstream（轮询） 可以进行负载均衡，通过weight值的大小决定权重。 ip_hash 来进行会话保持，保证同一个客户访问时 被调度到同一台服务器。 反向代理 server { listen 80; server_name localhost; location ^~ /admin { proxy_pass http://ygcr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; } location ^~ /static/ { proxy_pass http://ygcr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; } location = /product { proxy_pass http://ygcr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; } } 通过url请求访问此服务器，当端口后面是/admin、/static开头以及是/product的都转交给http://ygcr 来处理，ygcr这个在上面的负载均衡中已经定义。如访问http://ip:port/admin ，NGINX监听到这个端口后，匹配admin，然后交给http://ygcr 处理，最终访问的是 http://ygcr/admin 形成反向代理的效果。 "},"nginx/nginx_other.html":{"url":"nginx/nginx_other.html","title":"nginx其他知识","keywords":"","body":"一些常用nginx小知识汇总 nginx自动调整进程数（调整到和核数同样大小） worker_processes auto; 调整客户端最大body大小 client_max_body_size 20m; 包含其他配置（这样就更便于模块化配置） include /etc/nginx/conf.d/*.conf; 某一个访问（80或其他）端口，强制跳转到https(443)协议端口上 rewrite (.*) https://$server_name$request_uri redirect; 如果是非http协议的 负载均衡，可以直接使用stream，然后里面包含upstream（轮询）模块 stream { upstream dns { server 127.0.0.1:3002; #server otherip:3002 weight=5; 此处用来负载均衡指派 } 保留客户端真实ip 客户访问服务器经过nginx多层代理后，如何保留客户的真实ip呢 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; 第一行代码在层层代理时，保留住上级和本级代理的ip，如果只有一层代理，则保留客户端的ip和代理的ip，第二行可以保留客户端的ip。 重新加载配置项 nginx -s reload "},"git/git-use.html":{"url":"git/git-use.html","title":"git日常使用命令","keywords":"","body":"git常见用法 正常提交流程 1，首先从GitHub/gitlab服务器拉下来 git clone 仓库地址 如果使用ssh协议，还需要生成公私钥对，把公钥存储到仓库中 2，然后编辑和添加文件后 git add . （所有文件） git add filename(具体某个文件） 3，然后提交 git commit -m \"message更新信息\" 若没有新增文件，只修改文件，上面两步可以合并为 git commit -am \"message\" 4，最后上传 git push [-u origin your_branch] 把某个分支的文件copy到其他分支，如A分支文件copy到B分支 1,首先切换到B分支 git checkout branchB 2,在B分支下执行 git checkout branchA（A分支） file1 file2 … 3，剩下的步骤是提交和推送，和上面一样； 合并分支 把A分支所有的提交合并到B 首先同步A分支，使A分支本地和远程仓库保持一致 切换到B分支后 git checkout B git merge A 然后A分支的内容就merge到B上了 如果有冲突，通过 git status 查看一下冲突文件，解决冲突，git add 冲突文件后， 剩下的提交和推送操作参照上面步骤 把A 分支某（几）次提交也提交到 B分支 在 A分支下执行 git log 查找到相关提交记录 切换到B分支，git checkout B 把A的某次提交，也提交到B： git cherry-pick 7f00fe9ebb（提交号） 把A的某几次提交，也提交到B： git cherry-pick 7f00fe9ebb..7f00fe9ebb（提交范围） 如果有冲突，通过 git status 查看一下冲突文件，解决冲突，git add 冲突文件后，剩下的提交和推送操作参照上面步骤 没有冲突直接执行 git push [-u origin your_branch] 针对某次提交 打tag 1，使用git log查看提交日志，找出你需要的那个commit。假设提交的commit id git checkout 使用git tag进行打标签，例如：git tag -a v1.4 -m ‘xxxx’ git push origin --tags或者git push origin [tagname] git checkout -b newbranch git push origin newbranch git fetch 与 git pull git fetch origin master:tmp 在本地新建一个temp分支，并将远程origin仓库的master分支代码下载到本地temp分支 git push origin tmp 把tmp分支推送到远程仓库 git pull 相当于 git fetch + git merge 如 git pull tmp相当于 git fetch origin/tmp + git merge origin/tmp 把远程tmp merge到本地tmp分支中 本地仓库回退到某个版本　　 git　　reset　　–hard　　bae168 创建空白新分支 git branch git checkout git rm --cached -r . git clean -f -d 创建空的commit git commit --allow-empty -m \"[empty] initial commit\" 推送新分支 git push origin 把修改存到缓存中 git stash git stash pop(这个会把缓存拿出来，删除缓存） 或 git stash git stash list(查看stash列表） git stash apply [列表名称] .gitignore 的设置： 很多时候，有些文件不需要提交，如开发人员本地环境配置，就用到了.gitignore 只忽略dbg目录，不忽略dbg文件 dbg/ 只忽略dbg文件，不忽略dbg目录 dbg !dbg/ 若把某些目录或文件加入忽略规则，按照上述方法定义后发现并未生效，原因是.gitignore只能忽略那些原来没有被追踪的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。那么解决方法就是先把本地缓存删除（改变成未被追踪状态），然后再提交，这样就不会出现忽略的文件了。git清除本地缓存命令如下： git rm -r --cached . git add . git commit -m 'message' 最后说一个 强制推送（操作前要慎之又慎） git push -f [-u origin your_branch] 这些功能 基本能覆盖到git日常90%使用，剩下的，可以网上搜索资料，或者在git bash 中使用帮助，如 git commit --help; "},"git/install_and_bak_gitlab.html":{"url":"git/install_and_bak_gitlab.html","title":"gitlab搭建和备份","keywords":"","body":"gitlab 在线安装与备份 gitlab 安装方法说明 在线安装文档：https://mirror.tuna.tsinghua.edu.cn/help/gitlab-ce/ 编辑yum gitlab源 vim /etc/yum.repos.d/gitlab-ce.repo [gitlab-ce] name=Gitlab CE Repository baseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/ gpgcheck=0 enabled=1 注：镜像地址：https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/ 这个地址根据具体情况来，centos7 就填写el7 repo源生效 sudo yum makecache sudo yum install gitlab-ce 安装指定版本 sudo yum install gitlab-ce-8.11.5 -y 完成后， 放开 80 8080端口，或者指定的端口。 修改默认配置 安装完成后 要先执行gitlab-ctl reconfigure 再来修改配置文件 vim /etc/gitlab/gitlab.rb 修改备份文件路径 gitlab_rails['backup_path'] = '/mnt/backups' 设置只保存最近7天的备份 gitlab_rails['backup_keep_time'] = 604800 修改gitlab 存储路径 git_data_dirs({\"default\" => \"/home/git-data\"}) 修改访问url external_url 'http://172.18.35.66' 或者使用域名 再次执行gitlab-ctl reconfigure，使配置生效 恢复与备份 恢复gitlab gitlab-rake gitlab:backup:restore BACKUP=备份文件编号 备份gitlab gitlab-rake gitlab:backup:create CRON=1 定时任务脚本 #!/bin/bash /usr/bin/gitlab-rake gitlab:backup:create CRON=1 sleep 10s rsync -e \"ssh -p 22\" -avz /home/gitlab-backups 远端服务器IP:/gitlab-backups --delete 注：不使用 -e \"ssh -p 22\"这个 参数也可以同步，但本人远程服务器只有22端口是放开的 定时任务 （crontab -e后编辑) 0 2 * * 2-6 /path/xxx.sh &>/dev/null "},"autotech/monkey_android.html":{"url":"autotech/monkey_android.html","title":"monkey测试","keywords":"","body":"安卓设备monkey测试 1 adb shell 2 logcat -v threadtime -n 1000 -r 512000 -f /mnt/sdcard/logcat & ​ 这个命令限制每个logcat文件大小为500MB以内，便于查看；保存位置是手机存储中 3 monkey -p com.xxx.auto -s 1000 --ignore-crashes --ignore-timeouts --ignore-security-exceptions --pct-trackball 0 --pct-nav 0 --pct-majornav 0 --pct-anyevent 0 -v -v -v --throttle 500 1200000000 > /mnt/sdcard/monkey.log 2>&1 & ​ com.xxx.auto为APP的包名，这个可以通过开启手机log，然后操作此app，从日志中获取到这个包名。 "}}