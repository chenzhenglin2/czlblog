{"./":{"url":"./","title":"关于博客说明","keywords":"","body":"博客简介： 博客是博主多年的自动化测试和运维工作的知识总结，主要涉及测试、运维中的docker容器编排和部署、java开发等方面知识，也会涉及到cicd、Linux方面知识；甚至有多种工具和shell语言组合后的东西；博主希望通过对自己掌握的知识进行汇总，展示出来，给需要的人提供帮助或者思路。若能帮助到大家，也就达到博主的目的。 如果大家是用手机浏览 本网站，下拉滑动后点击左上角，即可看到目录树，来查看所需的文章。 本人csdn博客地址：https://blog.csdn.net/qq_39919755 会不定期进行同步，毕竟csdn被百度抓取可能性较大，能帮助到更多利用搜索器寻找答案的人； 博客主要分为Linux、shell、Jenkins、docker、kubernetes（rancher）、NGINX、git、jmeter、自动化测试、大数据处理等几大类，绝大部分都是原创，甚至很多是博主亲身踩过的坑，借博客形式展示出来，在运维和测试人员遇到类似问题、场景提供解决思路或者灵感 若要转载博客，请注明出处；若商用，请联系博主 "},"bat/bat-use-sedawk.html":{"url":"bat/bat-use-sedawk.html","title":"bat脚本中巧用sed grep awk命令","keywords":"","body":"在bat脚本中巧用linux中的sed awk grep命令三巨头 场景描述 有批安卓设备，需要刷机后，生成一个系统序列号文件，推送到这台安卓设备；由于时间原因、刷机人员能力限制，要快速搞出一套程序，能特别简单的生成这个序列文件，并推送到设备上； 这种急需又简单的功能，可以考虑用bat脚本来快速实现，让操作人员只需双击就能完成上面需求。 序列号文件（名称为xlh.txt）： {\"product_key\":\"11个字符\",\"DeviceName\":\"16个字符\",\"DeviceSecret\":\"32个字符\"} 这种json格式的txt文档； 序列号列表（名称为aaa.txt)： product_key DeviceName DeviceSecret 11个字符 16个字符 32个字符 11个字符 16个字符 32个字符 11个字符 16个字符 32个字符 11个字符 16个字符 32个字符 …… 如果有shell基础的，一看这个需求，用sed 和 grep等命令很容易就编写出shell 脚本，但Windows中不支持这几个命令； 而且也不能让操作人员进行其他复杂操作。 所以就采用bat脚本实现，根据提供的序列号表生成新的序列号文件，然后推送到设备后，删除掉一行记录； Windows系统安装git后gitbash中带有awk、sed、grep等工具打包放到脚本同级目录，然后cmd中使用awk 、sed 、grep命令看看，如果提示缺少dll库，把对应的库也copy进来。下面就是利用bat+sed等命令实现上述需求的脚本： chcp 65001 @echo off awk 'NR==1 {printf $0}' aaa.txt > aaa.tmpp for /f %%i in ('awk '{print $1}' aaa.tmpp') do (set var=%%i) del aaa.tmpp if \"%var%\"==\"DeviceName\" (sed -i '1d' aaa.txt) adb devices | awk 'END{printf NR}' > num.tmpp for /f %%j in ('type num.tmpp') do (set num=%%j) del num.tmpp ::echo %num% if %num% lss 3 ( echo 没有发现设备，请检查设备是否连接到电脑 pause exit ) for %%a in (aaa.txt) do ( if \"%%~za\" equ \"0\" ( echo aaa.txt列表为空了 pause exit ) ) awk 'NR==1 {printf $1}' aaa.txt > device.tmpp awk 'NR==1 {printf $2}' aaa.txt > secret.tmpp awk 'NR==1 {printf $3}' aaa.txt > key.tmpp for /f %%x in ('type device.tmpp') do (set DeviceName=%%x) for /f %%y in ('type secret.tmpp') do (set DeviceSecret=%%y) for /f %%z in ('type key.tmpp') do (set ProductKey=%%z) del *.tmpp sed -ri \"s|\\\"product_key\\\":\\\"[A-Za-z0-9]{11}\\\"|\\\"product_key\\\":\\\"%ProductKey%\\\"|;s|\\\"DeviceName\\\":\\\"[A-Za-z0-9]{16}\\\"|\\\"DeviceName\\\":\\\"%DeviceName%\\\"|;s|\\\"DeviceSecret\\\":\\\"[A-Za-z0-9]{32}\\\"|\\\"DeviceSecret\\\":\\\"%DeviceSecret%\\\"|\" xlh.txt TIMEOUT /T 1 /NOBREAK sed -i '1d' aaa.txt call push_xlh_and_install_flow.bat pause exit 上面脚本 ，为了保存变量都存到临时文件中，待取出变量值后，删除临时变量；把aaa.txt 列表文件抬头文件删除后，通过sed 命令生成新的的xlh.txt 序列号文件，并删除一行记录；调用push_xlh_and_install_flow.bat （这个脚本中实现了xlh.txt 推送到设备，并通过adb 等命令启动应用验证序列号等功能）； 对操作人员来说没有什么技术要求，只需把设备接入电脑后，双击这个脚本就能完成；没有接入设备，也会有相应的提示。 "},"linux/tar-deal.html":{"url":"linux/tar-deal.html","title":"\"-\"在tar命令中的巧用","keywords":"","body":"\"-\"在tar命令中的巧用 首先来看示例： tar -cvf - /home | tar -xvf - 前面把压缩结果存到-，后面通过管道 | 把存到-中的文件解压，如果纯粹看这个，觉得这不瞎折腾么，下面从实战来说明使用它的好处 实战案例1： 海量小文件传输方法 接收机：nc -l 8888 | tar xzf - -C /dest-dir 发送机：tar czf - /source-dir/ | nc 接收机ip 8888 在 GNU 指令中，如果单独使用 - 符号，不加任何该加的文件名称时，代表\"标准输入/输出\"的意思,上面命令把结果输入到-,然后再解压-; 接收机可以带上参数v,如 xzvf（便于可视化）；如果发送机压缩命令带有z接收机也必须带上参数z 另外8888或其他端口，一定要放开，或者关闭防火墙；通过这种方式传递文件，相等于把先把文件压缩、然后通过scp 传输、再解压这几步合并成两步，并且省去了等待压缩和解压的时间。 实战案例2： find /directory -type f -name \"mypattern\" | tar -cf archive.tar -T - 找到匹配的文件后，直接压缩 实战案例3 docker cp 命令 docker cp命令中用法 ]# docker cp --help Usage: docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- ​ docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH 官网解释： Use ‘-‘ as the source to read a tar archive from stdin and extract it to a directory destination in a container. Use ‘-‘ as the destination to stream a tar archive of a container source to stdout. 所以我们可以 docker cp cfcc20077ad1:/opt/aa.tgz - | tar xvf - -C ./xx tar czf - anaconda-ks.cfg initial-setup-ks.cfg | docker cp - cfcc20077ad1:/opt/mydir tar成对使用，前面一个是stdin 后面一个是stdout , 最终文件不会改变，-里面存的是压缩包，传输完成后还是压缩包，如果是文件传输完成后还是文件 ，只不过tar 传输比较快 \"-\"不光在tar命令中，可以这么使用；在yaml中，kubectl中 都有类似用法，kubectl中用法，可以参考我的kubectl命令文档。 "},"linux/limit_ip.html":{"url":"linux/limit_ip.html","title":"利用iptables或防火墙指定ip访问服务器某个端口","keywords":"","body":"如何利用iptables或防火墙指定ip访问服务器某个端口 有如下两种方式： 1 firewall-cmd --permanent --add-rich-rule 'rule family=ipv4 source address=10.219.82.83 port port=12181 protocol=tcp accept' systemctl restart firewalld 移除--add-rich-rule规则 用--remove-rich-rule= 帮助文档显示： --list-rich-rules List rich language rules added for a zone [P] [Z] --add-rich-rule= ​ Add rich language rule 'rule' for a zone [P] [Z] [T] --remove-rich-rule= ​ Remove rich language rule 'rule' from a zone [P] [Z] 2 iptables-save > iptables.rules /sbin/iptables -A INPUT -s 172.16.35.31 -p tcp --dport 12181 -j ACCEPT # 中控白名单 /sbin/iptables -A OUTPUT -d 172.16.35.31 -p tcp --sport 12181 -j ACCEPT /sbin/iptables -A INPUT -p tcp --dport 12181 -j DROP #其他服务器无法访问12181端口 /sbin/iptables -A OUTPUT -p tcp --sport 12181 -j DROP \\#iptables --list-rules #查看规则 ，根据情况删除下面规则 \\#iptables -D INPUT -j REJECT --reject-with icmp-host-prohibited # \\#iptables -D FORWARD -j REJECT --reject-with icmp-host-prohibited # service iptables save systemctl restart iptables "},"linux/set_ip.html":{"url":"linux/set_ip.html","title":"如何设置centos7.6和Ubuntu19.4的ip4","keywords":"","body":"centos7.6和Ubuntu19.04设置ip centos7任何版本都可以通过nmtui命令来设置ip， 但centos7.6ip的设置需要注意，在网卡配置文件/etc/sysconfig/network-scripts/ifcfg-eth0 ,需要配置如下两项 #网卡eth0名称因环境而已 ONBOOT=yes IPV6_PRIVACY=no 一定要增加 IPV6_PRIVACY=no ，centos7.6 默认采用IP6技术了 Ubuntu19.4 配置静态IP sudo vim /etc/netplan/50-cloud-init.yaml #文件名称因环境而异，但一定是*.yaml network: ​ ethernets: ​ ens33: ​ dhcp4: no ​ addresses: [192.16.1.101/24, ] ​ gateway4: 192.16.1.2 ​ nameservers: ​ addresses: [192.16.1.2,8.8.8.8] ​ version: 2 完成后执行 sudo netplan apply 即可生效,博主参考了：https://www.tecmint.com/configure-network-static-ip-address-in-ubuntu/ "},"linux/extend_disk.html":{"url":"linux/extend_disk.html","title":"Linux中磁盘扩展与缩减","keywords":"","body":"磁盘扩展和缩减知识汇总 新增分区，挂靠到新的目录方法 1,首先通过命令lsblk 查看增加分区的情况； [root@apptrace0011 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 501G 0 disk ├─sda1 8:1 0 1G 0 part /boot ├─sda2 8:2 0 199G 0 part │ ├─centos-root 253:0 0 50G 0 lvm / │ ├─centos-swap 253:1 0 3.9G 0 lvm [SWAP] │ └─centos-home 253:2 0 445.1G 0 lvm /home └─sda3 8:3 0 301G 0 part └─centos-home 253:2 0 445.1G 0 lvm /home sdb 8:16 0 10G 0 disk sr0 11:0 1 1024M 0 rom 关于sda/sdb说明，如果通过vmmare 虚拟机控制台等工具，直接在原有的1个硬盘扩充的存储空间；如原有硬盘是200G， 扩充到500G扩充后，扩充的存储还是在sda分区下；如果新增一个硬盘，是在sdb分区，依次类推sdc…… 2，通过命令fdisk -l [root@apptrace0011 ~]# fdisk -l Disk /dev/sda: 537.9 GB, 537944653824 bytes, 1050673152 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000e999c Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 1 FAT12 /dev/sda2 2099200 419430399 208665600 8e Linux LVM /dev/sda3 419430400 1050673151 315621376 8e Linux LVM Disk /dev/sdb: 10.7 GB, 10737418240 bytes, 20971520 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-root: 53.7 GB, 53687091200 bytes, 104857600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-swap: 4160 MB, 4160749568 bytes, 8126464 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-home: 477.9 GB, 477940940800 bytes, 933478400 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes 查看到sda 硬盘和sdb硬盘的情况，sda都已经做分区（但还有空间可以进行分区，下种类型讲） sdb还未有分区；如果新增了硬盘，没有看到可以执行命令： partprobe /dev/sdb ,没有这个命令，自行安装 yum -y install parted 3，如果新增硬盘在sdb下 可以按照如下方式直接挂载 fdisk /dev/sdb 输入m 查看用法 最常用几个用法 p 打印分区情况 n 新增分区； d删除分区；w保存 t改变格式 输入p 打印分区情况 输入 n新增分区 Partition number (1-4, default 1): 正常情况默认选中1；，如果上步p打印时已经有sdb1，输入2 然后输入t 改变分区格式 Command (m for help): t Selected partition 1 Partition type (type L to list all types): L 0 Empty 24 NEC DOS 81 Minix / old Lin bf Solaris 1 FAT12 27 Hidden NTFS Win 82 Linux swap / So c1 DRDOS/sec (FAT- 2 XENIX root 39 Plan 9 83 Linux c4 DRDOS/sec (FAT- 3 XENIX usr 3c PartitionMagic 84 OS/2 hidden or c6 DRDOS/sec (FAT- 4 FAT16 选择8e Linux LVM 这个格式，(有的是83、linux格式的) 最后输入w 保存退出（不能漏掉） Partition type (type L to list all types): 8e Changed type of partition 'Linux' to 'Linux LVM'. 如果保存出现错误,可以 partprobe /dev/sdb (没有数字） 然后再进入 fdisk /dev/sdb 继续上面的操作 甚至重启 4，接着格式化： centos7 可以用mkfs.xfs /dev/sdb1 ，Ubuntu或者centos6 用mkfs.ext4 /dev/sdb1 来格式 输入mkfs. 按tab键，可以看出有哪些格式 [root@apptrace0011 ~]# mkfs. mkfs.btrfs mkfs.cramfs mkfs.ext2 mkfs.ext3 mkfs.ext4 mkfs.minix mkfs.xfs 5，进行挂载： mount /dev/sdb1 /data/（新目录或者老目录，如果没有需求提前创建） 6，开机生效，编辑 /etc/fstab /dev/mapper/centos-root / xfs defaults 0 0 UUID=3c94bedd-2b80-47d3-a3a4-05785847aa10 /boot xfs defaults 0 0 /dev/mapper/centos-home /home xfs defaults 0 0 /dev/mapper/centos-swap swap swap defaults 0 0 把刚才的/data 添加进去 /dev/sdb1 /data xfs defaults 0 0 或者 UUID=3c94bedd-2b80-47d3-a3a4-05785847aa10 /data xfs defaults 0 0 如果是在物理机上，增加硬盘后，最好填写uuid，分区是可以变化，uuid不会变； 7.其他命令 blkid查看挂载硬盘的UUID，如blkid | grep \"sdb*\"，查看现有分区cat /proc/partitions 怎么把原有硬盘扩充的存储都挂靠到/home（或其他已有目录） 1，查看新增硬盘情况，如下，原有硬盘从200G增加到300G [root@part-add ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 300G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 199G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 3.9G 0 lvm [SWAP] └─centos-home 253:2 0 145.1G 0 lvm /home sr0 11:0 1 1024M 0 rom 再查看fdisk情况 [root@part-add ~]# fdisk -l Disk /dev/sda: 322.1 GB, 322122547200 bytes, 629145600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000e999c Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 83 Linux /dev/sda2 2099200 419430399 208665600 8e Linux LVM Disk /dev/mapper/centos-root: 53.7 GB, 53687091200 bytes, 104857600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-swap: 4160 MB, 4160749568 bytes, 8126464 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-home: 155.8 GB, 155818393600 bytes, 304332800 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes 2，把增加的硬盘容量全部分到一个新分区sda3上 fdisk /dev/sda Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): p Disk /dev/sda: 322.1 GB, 322122547200 bytes, 629145600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000e999c Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 83 Linux /dev/sda2 2099200 419430399 208665600 8e Linux LVM Command (m for help): n Partition type: p primary (2 primary, 0 extended, 2 free) e extended Select (default p): p Partition number (3,4, default 3): First sector (419430400-629145599, default 419430400): Using default value 419430400 Last sector, +sectors or +size{K,M,G} (419430400-629145599, default 629145599): Using default value 629145599 Partition 3 of type Linux and of size 100 GiB is set Command (m for help): p Disk /dev/sda: 322.1 GB, 322122547200 bytes, 629145600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000e999c Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 83 Linux /dev/sda2 2099200 419430399 208665600 8e Linux LVM /dev/sda3 419430400 629145599 104857600 83 Linux Command (m for help): t Partition number (1-3, default 3): Hex code (type L to list all codes): 8e （注意lvm格式） Changed type of partition 'Linux' to 'Linux LVM' Command (m for help): p Disk /dev/sda: 322.1 GB, 322122547200 bytes, 629145600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000e999c Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 83 Linux /dev/sda2 2099200 419430399 208665600 8e Linux LVM /dev/sda3 419430400 629145599 104857600 8e Linux LVM Command (m for help): w The partition table has been altered! Calling ioctl() to re-read partition table. WARNING: Re-reading the partition table failed with error 16: Device or resource busy. The kernel still uses the old table. The new table will be used at the next reboot or after you run partprobe(8) or kpartx(8) Syncing disks. [root@part-add ~]# partprobe /dev/sda [root@part-add ~]# partprobe /dev/sda3 [root@part-add ~]# fdisk -l Disk /dev/sda: 322.1 GB, 322122547200 bytes, 629145600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000e999c Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 83 Linux /dev/sda2 2099200 419430399 208665600 8e Linux LVM /dev/sda3 419430400 629145599 104857600 8e Linux LVM Disk /dev/mapper/centos-root: 53.7 GB, 53687091200 bytes, 104857600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-swap: 4160 MB, 4160749568 bytes, 8126464 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-home: 155.8 GB, 155818393600 bytes, 304332800 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes 3，新增分区格式化（执行顺序可以和下面第4部互换） mkfs.xfs /dev/sda3 meta-data=/dev/sda3 isize=512 agcount=4, agsize=6553600 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=26214400, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=12800, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 4，增加物理卷： pvcreate 刚才创建的分区 [root@part-add ~]# pvcreate /dev/sda3 WARNING: xfs signature detected on /dev/sda3 at offset 0. Wipe it? [y/n]: y Wiping xfs signature on /dev/sda3. Physical volume \"/dev/sda3\" successfully created. 查看物理卷增加后的情况 [root@part-add ~]# pvdisplay 或者pvs --- Physical volume --- PV Name /dev/sda2 VG Name centos PV Size 5，将物理卷加入到卷组 1）先看卷组信息 [root@part-add ~]# vgdisplay 或者vgs --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 4 VG Access read/write VG Status resizable MAX LV 0 Cur LV 3 Open LV 2 Max PV 0 Cur PV 1 Act PV 1 VG Size 2）把新的分区加入到卷组 vgextend centos（VG Name） /dev/sda3 （新分区） [root@part-add ~]# vgextend centos /dev/sda3 Volume group \"centos\" successfully extended - 3）再次查看 验证 [root@part-add ~]# vgdisplay --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 5 VG Access read/write VG Status resizable MAX LV 0 Cur LV 3 Open LV 2 Max PV 0 Cur PV 2 Act PV 2 VG Size 298.99 GiB PE Size 4.00 MiB Total PE 76542 Alloc PE / Size 50942 / 198.99 GiB Free PE / Size 25600 / 100.00 GiB VG UUID iza5sY-YoTh-ihTR-dLzZ-2CBx-M0dR-33bzW5 此时VG Size 大小已有 298.99 GiB 6，扩充逻辑卷 1）先通过下面命令查看系统里有哪些逻辑卷。 ``` [root@part-add ~]# lvdisplay 或者lvs --- Logical volume --- LV Path /dev/centos/swap LV Name swap VG Name centos LV UUID m8Dc95-LYbI-cxuG-hNc7-COoH-4Axh-7mamwX LV Write Access read/write LV Creation host, time localhost, 2018-08-31 18:38:32 +0800 LV Status available LV Size currently set to 8192 Block device 253:1 --- Logical volume --- LV Path /dev/centos/home LV Name home VG Name centos LV UUID 1HItab-O0cU-CEkB-cplf-6axH-utqI-Fgxi5n LV Write Access read/write LV Creation host, time localhost, 2018-08-31 18:38:33 +0800 LV Status available LV Size 有/dev/centos/swap /dev/centos/home /dev/centos/root这三个逻辑卷, 其中逻辑卷/dev/centos/home （挂载点是home目录下）就是本次要扩充的对象（同理根目录/ 对应的 /dev/centos/root也可以安装此方法) 2)扩充逻辑卷/dev/centos/home lvextend -L +100G /dev/mapper/centos-home 或者：lvextend -l 提示数量（可以查看Current LE，如果提示太多，减少到提示的最多数量） /dev/mapper/centos-home lvextend -l +100%FREE /dev/mapper/centos-home [root@part-add ~]# lvextend -L +100G /dev/mapper/centos-home Size of logical volume centos/home changed from 3）扩充到文件系统（目录）中，xfs_growfs /dev/centos/home 如果是ext格式 则用resize2fs /dev/centos/home [root@part-add ~]# xfs_growfs /dev/centos/home meta-data=/dev/mapper/centos-home isize=512 agcount=4, agsize=9510400 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=38041600, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=18575, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 38041600 to 64256000 [root@part-add ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/centos-root 50G 1.5G 49G 3% / devtmpfs 1.9G 0 1.9G 0% /dev tmpfs 1.9G 0 1.9G 0% /dev/shm tmpfs 1.9G 8.9M 1.9G 1% /run tmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup /dev/sda1 1014M 184M 831M 19% /boot tmpfs 380M 0 380M 0% /run/user/0 /dev/mapper/centos-home 246G 33M 246G 1% /home 重启后再查看 df -h ，是扩充成功后的；一般不需重启。但最好mount -a 生效一下 新增磁盘挂载步骤概述 如何给服务器增加三块硬盘 ： 1，将三块硬盘增加到pv（物理卷） pvcreate /dev/sdb /dev/sdc /dev/sdd 2,将pv加入到vg（卷）组 vgcreate datavg /dev/sdb /dev/sdc /dev/sdd 3,分配逻辑卷 lvcreate -l 50%FREE -n lv1 datavg lvcreate -L +200M -n lv2 datavg 4,格式化逻辑卷 mkfs.xfs /dev/datavg/lv1 mkfs.ext4 /dev/datavg/lv2 5,挂载 mkdir /lv1 /lv2 mount /dev/datavg/lv1 /lv1 mount /dev/datavg/lv2 /lv2 扩展磁盘步骤概述 1，添加物理磁盘 ​ pvcreate /dev/sdd 2，扩展到现有vg组 ​ vgextend 现有卷组名称 /dev/sdd 3，扩充到现有逻辑卷中 ​ lvextend -L +100G /dev/mapper/centos-home lvextend -l +100%FREE(或者扩展数量) /dev/mapper/centos-home 4，扩充到文件系统中 ​ xfs_growfs /dev/centos/home resize2fs /dev/centos/home ​ vg组减小和迁移等步骤概述 1，减小vg ​ vgremove vg组名 /dev/sdd (或者其他要移动物理卷) 2，迁移vg ​ 迁移vg里面的物理卷，必须是在同一个vg组中 ​ vgmove /dev/sdb /dev/sdc (在线迁移) "},"linux/how_to_made_bin.html":{"url":"linux/how_to_made_bin.html","title":"二进制安装包的制作","keywords":"","body":"如何制作二进制安装包 二进制安装包相当于把脚本和压缩包合成一个可执行的bin文件。 准备脚本和压缩文件 脚本准备 准备两个脚本 起始脚本，用来和压缩包进行合并生成二进制文件的，命名为init.sh(也可以自主命名) #!/bin/bash export TARGET_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\" #或export TARGET_DIR=\"$(cd $(dirname $0) && pwd )\" AAABBB=`awk '/^__AAABBB_BELOW__/ {print NR + 1; exit 0; }' $0` echo \"TARGET_DIR is: $TARGET_DIR\" tail -n+$AAABBB $0 | tar xzv -C $TARGET_DIR &> /dev/null $TARGET_DIR/xx.sh | tee $TARGET_DIR/xx.log exit 0 __AAABBB_BELOW__ 代码注解： 变量AAABBB是获取__AAABBB_BELOW__ 下面一行的行数，然后从此行开始解压，解压完成后，执行解压出来的脚本，并把执行过程输出到指定日志。__AAABBB_BELOW__ 下面一定要空一行 功能脚本，放到压缩包中，待压缩包解压完成后，调用此脚本，实现目标功能的 这个脚本根据生产实际情况来编写，可以是安装各种工具，各种功能的shell脚本。 压缩包准备 压缩包也是根据具体情况来准备，一般可以放各种rpm、安装包（可以是压缩包）、给起始脚本调用的shell文件。 所有文件准备完成后，采用tar zcvf xxx.tgz xxx 格式进行压缩 合成 cat init.sh xxx.tgz > install_$(date +%Y%m%d).bin chmod a+x install_$(date +%Y%m%d).bin 测试和验证 找一个干净的linux环境，把合成后的bin文件copy过去，使用./install_$(date +%Y%m%d).bin进行安装验证，如果有问题，查看日志，然后进行排障。 "},"linux/use_heredoc.html":{"url":"linux/use_heredoc.html","title":"heredocument的巧用","keywords":"","body":"heredocument 的巧用 我们经常使用 cat >(或>>) xxx来把内容添加某个文件中，这就是heredocument 的一种用法，但heredocument 用法不仅仅限制于此 情景1： 我们需要先登录mysql后 才能执行sql脚本 这个时候 就可以利用heredocument mysql -uroot -pxxxx 情景2： :执行某个脚本实现登录后，然后执行相关操作 可以借鉴here document ，写成脚本 #!/bin/bash ./xxx.sh xxx.sh 也可以是可执行的二进制文件 注意： 另外eof包含的内容，含有变量的话 ，不被替换 可以写作 ./xxxx.sh eof块的内容会原封不动的保存 "},"docker/docker-install.html":{"url":"docker/docker-install.html","title":"docker 安装与优化","keywords":"","body":"Docker CE for CentOS 7.5/ubuntu19.04(centos7.5或以上版本) centos安装docker 安装yum⼯具 sudo yum install -y yum-utils 配置docker yum源 sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo sudo yum makecache fast 查询可⽤版本 sudo yum list docker-ce --showduplicates | sort -r 安装指定版本docker-ce-17.03.2.ce, 先安装docker-ce-selinux-17.03.2.ce，否则安装docker-ce会报错 sudo yum -y install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm 2 安装docker-ce sudo yum -y install docker-ce-17.03.2.ce-1.el7.centos 如果只下载安装包（和依赖库）： yum install -y docker-ce-17.03.2.ce --downloadonly --downloaddir=./ 安装最新版的docker sudo yum -y install docker-ce centos（Redhat）离线安装 只需要下载对应版本RPM包 即可； 阿里镜像源地址 如安装docker-18.09.5，下载对应的3个RPM包即可 containerd.io-1.2.5-3.1.el7.x86_64.rpm docker-ce-18.09.5-3.el7.x86_64.rpm docker-ce-cli-18.09.5-3.el7.x86_64.rpm 利用rancher提供的安装脚本在线安装docker 1，首先下载安装脚本 curl -OS https://releases.rancher.com/install-docker/18.09.sh 2，如果有连接外网VPN，可以使用 curl https://releases.rancher.com/install-docker/18.09.sh |sudo sh直接安装，但这个不建议 如果直接在线执行，缺点是无法修改脚本,失败率高，不建议,因为在centos安装时失败，并提示yum 源找不到可以通过修改一行代码，改用阿里的镜像源 centos|fedora|redhat|oraclelinux) #yum_repo=\"https://download.docker.com/linux/centos/docker-ce.repo\" yum_repo=\"https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\" 3, 下载脚本后第一件事就是修改为阿里的镜像源 下载脚本 阿里镜像源地址：https://mirrors.aliyun.com/docker-ce/linux/ 所以把 https://download.docker.com/linux 全部替换为： https://mirrors.aliyun.com/docker-ce/linux vi 或者vim 18.09.sh :%s/https:\\/\\/download.docker.com\\/linux/https:\\/\\/mirrors.aliyun.com\\/docker-ce\\/linux/g 然后执行sh 18.09.sh 安装成功的 Ubuntu安装docker Ubuntu执行rancher提供docker安装脚本 执行时报错： + sh -c apt-get install -y -q docker-ce= Reading package lists... Building dependency tree... Reading state information... E: Version '' for 'docker-ce' was not found 这个可能Ubuntu版本太高，需要手动来安装docker了 可以把下面命令执行完成后，再来执行rancher提供的docker安装脚本 sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common bash-completion # step 2: 安装GPG证书 sudo curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - # Step 3: 写入软件源信息 sudo add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\" # Step 4: 更新并安装 Docker-CE sudo apt-get -y update ubuntu在线安装 sudo apt install docker.io 或sudo apt install docker-ce ubuntu离线安装 暂缺 rancher提供的docker安装脚本的GitHub：https://github.com/rancher/install-docker 改变docker的image存放⽬录（下面提供三种方式）、配置加速器等 修改配置文件，配置加速器 中写入如下内容（如果文件不存在请新建该文件） [root@yktapp1 dockersh]# cat /etc/docker/daemon.json { \"registry-mirrors\": [ \"https://registry.docker-cn.com\" ] } 也可以使用阿里的镜像加速器和本地仓库配置\"https://8m0vweth.mirror.aliyuncs.com\" 使用内部仓库地址的配置 [root@yktapp1 dockersh]# cat /etc/docker/daemon.json { \"registry-mirrors\": [\"https://registry.docker-cn.com\"], \"insecure-registries\": [\"0.0.0.0/0\"] } \"insecure-registries\": [\"0.0.0.0/0\"]通配表示所有http协议仓库，如果只想通配某个http协议仓库，可以写成具体名称或者通配 \"insecure-registries\":[\"10.251.66.44:5000\"] \"insecure-registries\":[\"10.251.26.0/24\"] 参考harbor GitHub文档说明 If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add --insecure-registry myregistrydomain.com to the daemon's start up arguments. In the case of HTTPS, if you have access to the registry's CA certificate, simply place the CA certificate at /etc/docker/certs.d/myregistrydomain.com/ca.crt . 自签名https和不安全http都可以用这种方式解决，自签名的证书，也可以把证书copy到相应目录解决，无需添加insecure-registries选项，具体方法参见harbor的使用说明 注意，一定要保证该文件符合 json 规范，否则 Docker 将不能启动。 1），指定docker存储位置，方法1（指定了overlay2，这种方式比overlay更高效，kernel较高的Linux默认就是这种） cat > /etc/docker/daemon.json 2），把指定的盘符挂载给docker 直接修改/etc/fstab中把一个逻辑卷挂到docker存储目录 /var/lib/docker中，但要确保原有的挂载没有东西，并取消之前的挂载关系， umout -a oldpath；可以通过下面查看原有的挂载和逻辑卷情况 [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT fd0 2:0 1 4K 0 disk sda 8:0 0 100G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 99G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 7.9G 0 lvm [SWAP] └─centos-home 253:2 0 41.1G 0 lvm /home 3），软链的方式 （不推荐，作为挽救使用） systemctl stop docker mv /var/lib/docker /home/docker ln -s /home/docker /var/lib/docker systemctl start docker 安装 docker-compose wget -c https://github.com/docker/compose/releases/download/1.22.0/docker-compose-`uname -s`-`uname -m` cp docker-compose-Linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 之后重新启动服务。 sudo systemctl daemon-reload sudo systemctl start docker sudo systemctl enable docker 注意最好配置完成好了 再启动docker，避免启动docker产生文件存储在默认的路径 测试安装是否成功 docker ps 非root用户操作docker sudo gpasswd -a your-user docker or sudo usermod -aG docker your-user 这两种方式 都是把非root用户添加到docker组 Kernel性能调优 cat >> /etc/sysctl.conf "},"docker/harbor-install.html":{"url":"docker/harbor-install.html","title":"harbor的安装和注意事项","keywords":"","body":"安装harbor说明 官方说明文档：https://github.com/goharbor/harbor/blob/release-1.7.0/docs/installation_guide.md 选择在线还是离线根据情况，这里不作为操作重点； 离线安装包下载地址 https://github.com/goharbor/harbor/releases 如果采用https协议，并使用自签名的证书的话，可以使用https://github.com/xiaoluhong/server-chart/blob/v2.2.2/create_self-signed-cert.sh 脚本生成证书 在线安装http协议的harbor(默认使用此协议的，简单) 下载完成在线安装包解压后，在harbor目录，备份 docker-compose.yml后再修改 container_name: nginx restart: always cap_drop: - ALL cap_add: - CHOWN - SETGID - SETUID - NET_BIND_SERVICE volumes: - ./common/config/nginx:/etc/nginx:z networks: - harbor dns_search: . ports: - 1180:80 - 5555:443 - 34443:4443 由于宿主机的80和443端口已经被用掉，只能映射成其他端口；如果是一台新机器 无需这么操作 harbor.cfg备份后修改如下两个参数 hostname = 172.16.35.31:1180 ui_url_protocol = http 修改完成后执行 ./install.sh 然后就可以在浏览器中访问了：http://172.16.35.31:1180 在harbor/common目录有两个子目录，分别是：config templates 其中templates是原始文件，config是执行install.sh 或prepare 后生成的，所以尽量修改templates目录下的文件； 如果要修改harbor的配置，按照官方文档“Managing Harbor's lifecycle” 来操作 执行install.sh 相当于 ./prepare docker-compose up -d 这两步，所以修改完成后 要么执行install.sh 要么执行这两步； 如果出现误操作，可以通过docker-compose down -v 再重新执行 如何使用http协议登录和推送镜像 最新版本docker 默认不允许使用http推送，所以我们要修改 在harbor宿主机和远程推送镜像的机器都增加如下配置 vi /etc/docker/daemon.json{ \"insecure-registries\":[\"172.16.35.31:1180\"] } 或者直接在已有配置后面添加 { \"registry-mirrors\": [ \"https://registry.docker-cn.com\" ], \"insecure-registries\":[\"172.16.35.31:1180\"] } 也可以使用0.0.0.0通配所有的http镜像仓库 \"insecure-registries\": [\"0.0.0.0/0\"] 修改harbor宿主机docker启动项配置/usr/lib/systemd/system/docker.service 如果docker17.3版本，直接参照下面方式添加ExecStart=/usr/bin/dockerd --insecure-registry=172.16.35.31:1180 如果docker是最新18.9或者以上版本，直接参照下面方式添加ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock --insecure-registry=172.16.35.31:1180 完成后，重启dockersystemctl daemon-reload systemctl restart docker 验证参数：ps aux|grep dockerd root 106472 31.5 0.0 2150816 71576 ? Ssl 17:06 0:02 /usr/bin/dockerd -H unix:///var/run/docker.sock --insecure-registry=172.16.35.31:1180 远程或者本地登录验证 docker login 172.16.35.31:1180 Username: admin Password: Login Succeeded 若登录时提示net/http: request canceled (Client.Timeout exceeded while awaiting headers) 这是代理搞的鬼，把代理/etc/systemd/system/docker.service.d/http-proxy.conf或者其他配置，里面[Service] Environment=\"HTTP_PROXY=http://10.xx.xx.83:3128\" Environment=\"NO_PROXY=localhost,127.0.0.0/8,10.42.*.*\" 把登录时的ip 加入到NO_PROXY中，支持通配符 开放管理端口映射（http协议，可选操作） 管理端口在 /lib/systemd/system/docker.service 文件中 将其中第11行的 ExecStart=/usr/bin/dockerd 替换为： ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock -H tcp://0.0.0.0:7654 （此处默认2375为主管理端口，unix:///var/run/docker.sock用于本地管理，7654是备用的端口） 将管理地址写入 /etc/profile echo 'export DOCKER_HOST=tcp://0.0.0.0:2375' >> /etc/profile 管理接口以备Jenkins等工具调用使用。 离线安装https协议的harbor 生成证书 1，执行create_self-signed-cert.sh 带上相应参数生成所需自签名证书（可以-h,查看用法；如果有证书，忽略此步） cat /etc/hosts 192.16.1.100 Centos76 reg.czl.com 具体名称和设置的自签名域名一致 ./create_self-signed-cert.sh --ssl-domain=reg.czl.com --ssl-trusted-ip=192.16.1.100 把生成的两个证书 copy到 /data/cert/ 2，配置harbor vi harbor.cfg hostname = 域名 （不能填写IP，如果用IP的话，采用http协议方式安装harbor） #自签名申请的域名如reg.czl.com 但采用了域名后，使用域名制作的证书，最好就填写域名避免出现 docker login ip 提示：certificate signed by unknown authority，linux终端用域名登录，浏览器端可以用域名也可以用https://ip登录， #打tag和推送也必须用域名 ui_url_protocol = https ssl_cert = /data/cert/tls.crt ssl_cert_key = /data/cert/tls.key #ssl_cert = /data/cert/server.crt #ssl_cert_key = /data/cert/server.key 3，执行安装命令 ### 4,如下操作和http协议的一样 但要注意，如果证书是自签名的 ，在其他机器登录此https协议的镜像库，还需要把tls.crt 推送过去 ```bash 非仓库机器： mkdir -p /etc/docker/certs.d/reg.czl.com/ 仓库机器把crt文件推送过去 scp tls.crt root@ip:/etc/docker/certs.d/reg.czl.com/ca.crt 如果是Ubuntu系统，root密码是随机的，可以分两步推送， 先推送给Ubuntu的常用账号，常用账号copy到指定目录 ` 服务器断电重启后，发现连接不上harbor 如何优化 如果断电或者机器重启后，harbor不可用，十有八九就是容器没有自动起来，我们可以完善一下，这个组件自带的docker-compose.yml 文件 如下所示，在每个容器策略里面加上restart选项，设置为unless-stopped，新版1.7.5版本harbor 全部组件restart 选项都设为always ,但还是需要手动停止部分组件后，再全部启动 注意事项 最好把harbor服务器的hostname或者别名加入到从harbor拉取镜像其他主机hosts文件中 vim /etc/hosts ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.16.35.31 szly-manage 常见参数解释说明 docker配置文件/etc/docker/daemon.json 或者启动脚本中 ExecStart参数 {\"hosts\": [\"fd://\", \"tcp://0.0.0.0:2375\"]} fd：// 指自动进行unix socket它 和-H unix:///var/run/docker.sock 、-H unix:// 作用相同，只是不同docker版本，写法略有差异。高版本的docker都已经采用了 -H 这种形式参数 小贴士 直接登录界面后，新建项目，在项目里面点击推送镜像，可以弹出镜像推送方法 更详细配置和证书生成脚本可以参考rancher提供的文档 https://www.cnrancher.com/docs/rancher/v2.x/cn/installation/registry/single-node-installation/ "},"docker/harbor-sync.html":{"url":"docker/harbor-sync.html","title":"harbor仓库镜像同步","keywords":"","body":"主备 Harbor 部署（harbor同步） 前提 两台或以上harbor服务器，其中一台为主服务器，其他为备份服务器 1，仓管管理中，新建目标 2，编辑目标 3，新建规则 选择复制管理选项，点击新建规则，然后填写主 harbor 需要同步复制的仓库和同步规则，最后点击保存，即可。 "},"docker/dockerfile-rule.html":{"url":"docker/dockerfile-rule.html","title":"如何制作出合适的镜像","keywords":"","body":"如何制作最合适docker镜像 控制镜像大小 优先选取基于alpine的镜像 查看tomcat镜像：https://hub.docker.com/_/tomcat?tab=tags （docker hup 中搜索tomcat镜像） 如tomcat8.5的镜像8.5-jre11 204 MB Last update: 13 days ago 而基于alpine的tomcat镜像8.5-jre8-alpine 72 MB Last update: 10 days ago 一眼就能看出来基于alpine的镜像比默认镜像少了130多M 但alpine镜像里面缺少了很多东西，设置不支持bash命令，我们可以在制作镜像时，一一给安装上，安装完成后删除缓存文件，就是安装文件； 选取国内的镜像源 如清华大学、阿里云等 Dockerfile示例： FROM alpine:3.7 MAINTAINER chenzhenglin #更新Alpine的软件源为国内（清华大学）的站点，因为从默认官源拉取实在太慢了。。。 RUN echo \"https://mirror.tuna.tsinghua.edu.cn/alpine/v3.7/main/\" > /etc/apk/repositories # 注意alpine的版本号 可能有差异，链接地址源最好用浏览器打开查看验证一下； 或用latest代替 RUN apk update \\ && apk upgrade \\ && apk add --no-cache bash \\ bash-doc \\ bash-completion \\ && apk add net-tools vim \\ && rm -rf /var/cache/apk/* \\ …… apk add --no-cache不使用本地缓存安装包数据库，直接从远程获取安装包信息安装。这样我们就不必通过apk update获取安装包数据库了，&&合并命令 好处是，多条命令合并成一条命令，减少docker images的层 如果基于centos或Ubuntu的系统 最好也要把软件源换成国内的 FROM centos:7.6.1810 RUN mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.bak && \\ curl -s http://mirrors.aliyun.com/repo/Centos-7.repo -o /etc/yum.repos.d/CentOS-Base.repo && \\ curl -s http://mirrors.aliyun.com/repo/epel-7.repo -o /etc/yum.repos.d/epel-7.repo && \\ yum repolist && \\ 注意：Alpine 镜像中的 telnet 在 3.7 版本后被转移至 busybox-extras 包中，所以apk add telnet 会报错 如下，一个基于tomcat:8.5-alpine镜像的Dockerfile，并验证通过 FROM tomcat:8.5-alpine MAINTAINER chenzhenglin #更新Alpine的软件源为国内（清华大学）的站点，因为从默认官源拉取实在太慢了…… alpine版本要从dockerfile里面查找到 RUN echo \"https://mirror.tuna.tsinghua.edu.cn/alpine/latest-stable/main/\" > /etc/apk/repositories # 注意alpine的版本号 可能有差异 RUN apk update \\ && apk upgrade \\ && apk add --update --no-cache net-tools vim busybox-extras curl \\ && rm -rf /var/cache/apk/* 查看镜像的Dockerfile 在docker hub 里面找到所需镜像后，在description的tab页，选择需要的版本，点击，一般会自动跳转到其githup源码界面； 如：https://hub.docker.com/_/tomcat?tab=description 如下Dockerfile文件FROM openjdk:8-jre ENV CATALINA_HOME /usr/local/tomcat ENV PATH $CATALINA_HOME/bin:$PATH RUN mkdir -p \"$CATALINA_HOME\" WORKDIR $CATALINA_HOME ……此处也省略不必要代码 RUN apt-get update && apt-get install -y --no-install-recommends \\ libapr1 \\ ……此处也省略不必要代码 apt-get install -y --no-install-recommends wget ca-certificates; \\ ……此处也省略不必要代码 # sh removes env vars it doesn't support (ones with periods) # https://github.com/docker-library/tomcat/issues/77 find ./bin/ -name '*.sh' -exec sed -ri 's|^#!/bin/sh$|#!/usr/bin/env bash|' '{}' +; \\ \\ ……此处也省略不必要代码 CMD [\"catalina.sh\", \"run\"] 从这个Dockerfile文件可以看出，已经安装了wget等工具，并把默认的sh换成了bash； 如何判断tomcat里面采用哪一版本的alpine呢,激活镜像后，查看其系统版本信息 cat /etc/os-release 同样其他linux也可以采用此方法查看版本号： cat /etc/redhat|centos-release alpine系统添加软件的常用方法，参见链接： https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management 其他精简镜像的方法 https://mp.weixin.qq.com/s/LOXNMYtZbnYeDR2lBI56fw "},"docker/save_load_images.html":{"url":"docker/save_load_images.html","title":"逐个和批量导出导入docker镜像","keywords":"","body":"逐个导出镜像 #!/bin/bash IMAGES_LIST=($(docker images | sed '1d' | awk '{print $1}')) IMAGES_NM_LIST=($(docker images | sed '1d' | awk '{print $1\"-\"$2}'| awk -F/ '{print $NF}')) IMAGES_NUM=${#IMAGES_LIST[*]} for((i=0;i'#'注意 这个慎用，如果一个镜像有多个版本，容易出现问题，采用下面的批量导入 批量导入到一个压缩包 #!/bin/bash IMAGES_LIST=($(docker images | sed '1d' | awk '{print $1\":\"$2}')) docker save ${IMAGES_LIST[*]} -o all-images.tar.gz 把一个压缩包所有镜像导出并上传到仓库 ./rancher-load-images.sh -l rancher-images.txt -i rancher-images.tar.gz -r reg.czl.com/library #仓库地址包含具体的项目 rancher-load-images.sh 脚本下载地址：https://github.com/rancher/rancher/releases 选择需要版本，点击assets 展开后就有这个脚本下载 逐个导入镜像 cd $DIR/images_file for image_name in $(ls ./) do docker load "},"k8s/kubectl-user-instruction.html":{"url":"k8s/kubectl-user-instruction.html","title":"kubectl常用命令汇总（以及curl注入）","keywords":"","body":"kubectl 常用的命令总结 kubectl 详细命令用法可以参考官网： https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands 常用命令： 查看 只显示默认命名空间的pods kubectl get pods 显示所有空间的pod kubectl get pods --all-namespaces 显示指定空间的pod kubectl get pods -o wide --namespace apm 其中--namespace 与-n 作用等同，后面接命名空间参数 kubectl get deployment -n apm kubectl get pods,svc,rc -n apm svc是services简称 这些命令都可以通过 kubectl get --help 来查看帮助 删除 只能删除默认命名空间的deployment kubectl delete deployment nginx 删除指定空间的deployment/其他资源等 kubectl delete TYPE RESOURCE -n NAMESPACE 具体如下： kubectl delete deployment shop-app -n test-shop kubectl delete TYPE --all -n NAMESPACE kubectl delete all -n NAMESPACE kubectl delete all --all 创建、修改、打标签 使用yaml文件创建pod kubectl apply -f apptrace-receiver-deployment.yaml apply 和 create 命令都可以后跟yaml，创建所需资源,初次创建pod时可以互相替换使用；如果已有pod只是用于更新的话，又可以和replace相互替换使用；本着化繁就简的原则，create和replace都使用apply; 而且apply属于申明式语法，这个更加灵活，多次执行不会报错，只会更新改变的部分；像Jenkinsfile也已经从脚本语法向申明式转变。 使用kubectl命令把pod、卷、各种资源导出为yaml格式： kubectl get pods podA -n NAMEAPSCE-A -o yaml --export> padA.yaml pod 可以换成其他申明式资源如卷、services等；如果不带上参数--export 目前没有发现有特别大的不同 kubectl get all -n ns -o yaml --export> padA.yaml #导出某个命令空间所有资源 现在很多产品如rancher openshift，等；UI界面 直接可视化操作导出各种资源，掌握命令很多时候，可以事半功倍。-o更详细用法 下面有单独说明。 查看命名空间apm的collector服务详情 ```kubectl describe service/apptrace-collector --namespace apm --namespace 和-n 作用相同 - 查看pod日志 ```kubectl logs podname --namespace apm (可以带上 -f 参数) 为节点机apm-docker001打标签 zookeeper-node=apm-docker001,查看标签等； 为节点机打标签和查看 kubectl label nodes apm-docker001 zookeeper-node=apm-docker001 kubectl get nodes --show-labels 为命名空间打标签和查看 kubectl label namespace $your-namesapce istio-injection=enabled kubectl get namespaces --show-labels 给名为foo的Pod添加label unhealthy=true kubectl label pods foo unhealthy=true 查看某种类型字段下有哪些参数； kubectl explain pods kubectl explain Deployment kubectl explain Deployment.spec kubectl explain Deployment.spec.spec kubectl explain Deployment.spec.template kubectl explain Deployment.spec.template.spec 如查看Deployment.spec.template 可以有哪些参数 [root@k8s-master ~]# kubectl explain Deployment.spec.template KIND: Deployment VERSION: extensions/v1beta1 RESOURCE: template DESCRIPTION: Template describes the pods that will be created. PodTemplateSpec describes the data a pod should have when created from a template FIELDS: metadata Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata spec Specification of the desired behavior of the pod. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status 在pod内部，执行shell命令 kubectl exec podname printenv | ps aux | cat、ls 某个文件（如果pod不在默认空间，用-n 指定相应空间） 如：kubectl exec app-demo-68b4bd9759-sfpcf -n test-shop printenv 或者在kubectl exec podname -- 再跟shell命令 如： kubectl exec -it spark-master-xksl -c spark-master -n spark -- mkdir -p /usr/local/spark shell命令前，要加-- 号，不然shell命令中的参数，不能识别 kubectl exec 后面只能是pod,目前还不支持deployment daemonnset等 特别说明 创建和修改相对复杂些，可以直接在rancherUI 进行部署或者修改，修改还可以通过：kubectl edit deploy/xxx -n namespace,也可以把资源导出为yaml后，修改完成后，再次kubectl apply 部署一遍完成修改的目的。本人推荐通过rancherUI来部署和修改，这里就不对创建和修改做详细说明了；参考另一篇博客rancher 中快速部署应用 \"-\"在kubectl中 用法说明 “-”它作为标准输入（池）非常灵活用法 ，和tar中用法非常类似 yaml - apiVersion: v1 data: kubernetes.yml: |- - type: docker containers.ids: - \"*\" processors: - add_kubernetes_metadata: in_cluster: true kind: ConfigMap 命令 kubectl get po --all-namespaces --show-all --field-selector 'status.phase==Pending' -o json | kubectl delete -f - curl --insecure -sfL https://your_domain/v3/import/f2gdpkvz42gqzm8vbrdpd99xgppjxgwct7wt86lzswwnf4p2d4vfd7.yaml | kubectl apply -f - 可以看出，“-”里面保存标准输入的内容。 巧用kubectl 帮助文件 如你只记得部分命令 get ,可以用 kubectl get --help 同理 kubectl create rolebinding 不知道后面接什么 也可以--help一下,记得关键字越多 带上后再使用help，如果只记得部分 就先help 如 kubectl create --help 这样create 所有类型的应用怎么创建 都有了 同样也可以直接 kubectl --help 这样kubectl 有哪些用法就显示出来， 我们要一级级的利用帮助 可能刚开始记住前面一个关键字，写完关键字 help一下 又有很多详细的用法 kubectl 命令规律总结 先看一组命令 kubectl delete sa metricbeat -n efk kubectl get sa --all-namespaces kubectl delete daemon-set metricbeat -n efk 1.会发现，kubectl 不管get 、delete describe等操作 后面跟资源类型 如果sa(serviceaccout) deployment pod,然后是资源名称，如果没有资源名称，则删除、获取此类型所有的资源；最后限定某个命名空间，或者全部命名空间；这个限定命名空间 可以放在kubectl 后面，也可以放在所有参数后面； 也可以写成资源类型/资源名称；如 kubectl delete daemon-set/metricbeat -n efk -o 是指定输出格式 输出格式 说明 -o=custom-columns= 根据自定义列名进行输出，以逗号分隔 -o=custom-colimns-file= 从文件中获取自定义列名进行输出 -o=json 以JSON格式显示结果 -o=jsonpath= 输出jsonpath表达式定义的字段信息 -o=jsonpath-file= 输出jsonpath表达式定义的字段信息，来源于文件 -o=name 仅输出资源对象的名称 -o=wide 输出额外信息。对于Pod，将输出Pod所在的Node名 -o=yaml 以yaml格式显示结果 如下： kubectl get sa -n efk -o yaml kubectl get sa efk-elaticsearch -n efk -o yaml >xxx.yaml kubectl get pod efk-elaticsearch-0 -n efk -o wide 因为k8s 采用的是REST API接口，所有命令都最终会转换成curl -X PUT POS等形式,为什么不直接使用curl命令，因为需要一堆相关授权，rancher UI里面 在deploy或其他资源中，选择api查看 就可以查到，也可以点击右侧的edit编辑后 通过curl命令提交 API Request cURL command line: curl -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\ -X PUT \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{\"annotations\":{\"cattle.io/timestamp\":\"\", \"cni.projectcalico.org/podIP\":\"10.42.1.44/32\"}, \"containers\":[{\"allowPrivilegeEscalation\":false, \"exitCode\":null, \"image\":\"172.16.35.31:1180/apm-images/gettoken:1.0\", \"imagePullPolicy\":\"IfNotPresent\", \"initContainer\":false, \"name\":\"genttoken\", \"ports\":[{\"containerPort\":8001, \"dnsName\":\"genttoken-nodeport\", \"kind\":\"NodePort\", \"name\":\"8001tcp301001\", \"protocol\":\"TCP\", \"sourcePort\":30100, \"type\":\"/v3/project/schemas/containerPort\"}], \"privileged\":false, \"procMount\":\"Default\", \"readOnly\":false, \"resources\":{\"type\":\"/v3/project/schemas/resourceRequirements\"}, \"restartCount\":0, \"runAsNonRoot\":false, \"state\":\"running\", \"stdin\":true, \"stdinOnce\":false, \"terminationMessagePath\":\"/dev/termination-log\", \"terminationMessagePolicy\":\" 等等 修改完成后 可以点击send request来提交 k8s所有资源类型和别名（缩写） ~]# kubectl api-resources NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatus configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node persistentvolumeclaims pvc true PersistentVolumeClaim persistentvolumes pv false PersistentVolume pods po true Pod podtemplates true PodTemplate replicationcontrollers rc true ReplicationController resourcequotas quota true ResourceQuota secrets true Secret serviceaccounts sa true ServiceAccount services svc true Service mutatingwebhookconfigurations admissionregistration.k8s.io false MutatingWebhookConfiguration validatingwebhookconfigurations admissionregistration.k8s.io false ValidatingWebhookConfiguration customresourcedefinitions crd,crds apiextensions.k8s.io false CustomResourceDefinition apiservices apiregistration.k8s.io false APIService controllerrevisions apps true ControllerRevision daemonsets ds apps true DaemonSet deployments deploy apps true Deployment replicasets rs apps true ReplicaSet statefulsets sts apps true StatefulSet tokenreviews authentication.k8s.io false TokenReview localsubjectaccessreviews authorization.k8s.io true LocalSubjectAccessReview selfsubjectaccessreviews authorization.k8s.io false SelfSubjectAccessReview selfsubjectrulesreviews authorization.k8s.io false SelfSubjectRulesReview subjectaccessreviews authorization.k8s.io false SubjectAccessReview horizontalpodautoscalers hpa autoscaling true HorizontalPodAutoscaler cronjobs cj batch true CronJob jobs batch true Job certificatesigningrequests csr certificates.k8s.io false CertificateSigningRequest clusterauthtokens cluster.cattle.io true ClusterAuthToken clusteruserattributes cluster.cattle.io true ClusterUserAttribute leases coordination.k8s.io true Lease bgpconfigurations crd.projectcalico.org false BGPConfiguration clusterinformations crd.projectcalico.org false ClusterInformation felixconfigurations crd.projectcalico.org false FelixConfiguration globalnetworkpolicies crd.projectcalico.org false GlobalNetworkPolicy globalnetworksets crd.projectcalico.org false GlobalNetworkSet hostendpoints crd.projectcalico.org false HostEndpoint ippools crd.projectcalico.org false IPPool networkpolicies crd.projectcalico.org true NetworkPolicy events ev events.k8s.io true Event daemonsets ds extensions true DaemonSet deployments deploy extensions true Deployment ingresses ing extensions true Ingress networkpolicies netpol extensions true NetworkPolicy podsecuritypolicies psp extensions false PodSecurityPolicy replicasets rs extensions true ReplicaSet nodes metrics.k8s.io false NodeMetrics pods metrics.k8s.io true PodMetrics alertmanagers monitoring.coreos.com true Alertmanager prometheuses monitoring.coreos.com true Prometheus prometheusrules monitoring.coreos.com true PrometheusRule servicemonitors monitoring.coreos.com true ServiceMonitor networkpolicies netpol networking.k8s.io true NetworkPolicy poddisruptionbudgets pdb policy true PodDisruptionBudget podsecuritypolicies psp policy false PodSecurityPolicy clusterrolebindings rbac.authorization.k8s.io false ClusterRoleBinding clusterroles rbac.authorization.k8s.io false ClusterRole rolebindings rbac.authorization.k8s.io true RoleBinding roles rbac.authorization.k8s.io true Role priorityclasses pc scheduling.k8s.io false PriorityClass storageclasses sc storage.k8s.io false StorageClass volumeattachments storage.k8s.io false VolumeAttachment k8s的资源类型，其中常用的资源一般有SHORTNAMES，在输入命令时，输入这种简称，无疑能提高效率。 kubectl get cs，要比kubectl get componentstatuses 书写快的多，更容易记住。 更多用法可以参照官网或者国内翻译的博客 https://blog.csdn.net/xingwangc2014/article/details/51204224 "},"k8s/docker-run-and-k8s-command.html":{"url":"k8s/docker-run-and-k8s-command.html","title":"docker run image -args对应yaml语法/rancher UI操作方式","keywords":"","body":"如何区分image与container中的entrypoint、cmd关系 不管是用kubernetes还是docker-compose来管理容器，其command参数相当于覆盖了镜像的entrypoint，args相当于覆盖了镜像的CMD；若采用编排工具管理容器，如果没有重新定义entrypoint和cmd，就默认使用镜像的entrypoint和cmd；如果容器编排工具中只使用了args参数，相当于image的entrypoint+编排工具定义args参数；如果容器中同时定义了 command和args ，容器入口就变成了command+args，image中定义的entrypoint和cmd会完全被覆盖掉。 如何对着docker run image -args,来填写容器编排配置（kubernetes、docker-compose)文件 kubectl 与 Docker 命令关系 可以参考：http://docs.kubernetes.org.cn/70.html 我们经常能在某个image官方文档中看到 像docker run image -args这种用法，这就行相当于改写了其entrypoint或cmd，那这些args若是在kubernetes或docker-compose的yaml中怎么配置呢，如：docker run -it xxx /bin/sh，相当于把入口变为/bin/sh; 还有像 redis 镜像官网说明: docker run redis --requirepass passwd 运行一个带密码的容器；若在k8s的yaml、rancherUI（甚至是docker-compose）里面怎么配置呢，最简单办法 是查看这个镜像的dockerfile中的entrypoint ：docker-entrypoint.sh(只展示了部分代码) if [ \"${1#-}\" != \"$1\" ] || [ \"${1%.conf}\" != \"$1\" ]; then set -- redis-server \"$@\" fi 从上面代码，可以看出如果参数不是“-”开头或者\".conf\"结尾，会自动变成redis-server + args（args为自定义的参数），若参数是“-”开头或“.conf”结尾，会直接用CMD命令+参数；所以在k8s（docker-compose)配置文件args里面可以直接跟参数，如果第一个参数写成了redis-server也没关系，镜像入口文件已做处理；也可以直接定义command，但必须以redis-server开头+参数；为何是redis-server开头呢，这个是从redis的镜像cmd中获得的。 如果你使用rancher，直接在 UI界面，点击编辑-更多就会显示命令(Command)输入框，有两个输入框，分别是entrypoint和command；entrypoint对应yaml配置是command，command对应的yaml配置是args； 像redis加密，可以直接在rancher UI界面的command填写:--requirepass \"自己的密码\" 或redis-server --requirepass \"自己的密码\" ，甚至可以再entrypoint里面填写：redis-server --requirepass \"自己的密码\" 都可以，但不能在entrypoint中填写：--requirepass \"自己的密码\"。完成后可以验证，密码是否生效； 有一种情况：如果没有找到镜像的dockerfile，当run镜像后，到容器中的默认的目录，查看是否有个可执行的二进制文件，然后在command 里面设置 “二进制文件 -args ”但这个需要验证； 注意如果在args里面应用环境变量，要写成$(VALUE),不能写成$VALUE,如引用hostname，要写成$(HOSTNAME)，这相当于填写的是此变量的value ，而不是value name； 实战： packetbeat 官网镜像 使用参考 docker run -d \\ --name=packetbeat \\ --user=packetbeat \\ --volume=\"/etc/localtime:/etc/localtime:ro\" \\ --cap-add=\"NET_RAW\" \\ --cap-add=\"NET_ADMIN\" \\ --network=host \\ docker.elastic.co/beats/packetbeat:7.1.1 \\ --strict.perms=false -e \\ -E setup.kibana.host=ip:port \\ -E output.elasticsearch.hosts=ip:port 镜像后面的参数转换成配置文件 args: [ \"--strict.perms=false\", \"-e\", \"-E\",\"setup.kibana.host=ip:port\", \"-E\",\"output.elasticsearch.hosts=ip:port\" ] 如果使用rancher 直接UI上编辑 即可修改 "},"k8s/rancher_online_installation.html":{"url":"k8s/rancher_online_installation.html","title":"在线安装高可用的rancher","keywords":"","body":"Rancher server 高可用集群在线安装 搭建集群前提是 服务器数量为大于1的奇数 本案例用三台机器，采用rke搭建k8s集群,然后利用helm在此集群中部署七层负载的rancher server。 通过 RKE 工具部署 Rancher Kubernetes 集群 部署前准备工作 首先安装前需要提前准备必要的工具软件， 所需工具软件如下（只限于 Rancher 集群的三台节点使用） ✓ kubectl - Kubernetes 命令行工具。 ✓ rke - Rancher Kubernetes Engine 用于构建 Kubernetes 集群。 ✓ helm - Kubernetes 的包管理。 可以通过：https://www.cnrancher.com/docs/rancher/v2.x/cn/install-prepar e/download/ 下载相关文件 把事先下载完成的 rke 、helm 、kubectl 二进制文件 copy 到/usr/bin 目录下 chmod 777 rke_linux-amd64 kubectl_linux-amd64 helm mv rke_linux-amd64 /usr/bin/rke mv kubectl_linux-amd64 /usr/bin/kubectl mv helm /usr/bin/helm Rancher 集群的三台主机分别修改本机的 hosts 文件 cat > /etc/hosts ip1 hostname1 ip2 hostname2 ip3 hostname3 EOF 配置 Rancher 集群三台服务器之间的 ssh 免密登录 配置 Rancher 集群三台服务器之间的 ssh 免密登录 Rancher 集群的三台主机分别修改本机的 hosts 文件 cat > /etc/hosts ip1 hostname1 ip2 hostname2 ip3 hostname3 EOF 配置 Rancher 集群三台服务器之间的 ssh 免密登录 生成 ssh 免密访问公钥 ssh-keygen -t rsa -C \"备注信息\" 生成公私钥 将免密公钥推送集群内三台主机的 rancher账户(或其他可以使用docker的账号，centos不能使用root账号） ssh-copy-id -i ~/.ssh/id_rsa.pub rancher@ip1 以此类推 三台机器都推送，包括自身 完成后验证是否可以和集群内三台主机完成免密登录 RKE 创建 Rancher Kubernetes 集群 创建 rancher-cluster.yml 文件，用于 rke 推送集群配置使用，配置如下： nodes: - address: 10.128.119.47 user: was role: [controlplane,worker,etcd] - address: 10.128.119.48 user: was role: [controlplane,worker,etcd] - address: 10.128.119.49 user: was role: [controlplane,worker,etcd] services: etcd: snapshot: true creation: 6h 给提前下载好的 rke 工具配置可执行权限 chmod +x rke 执行集群部署操作 ./rke up --config rancher-cluster.yaml 完成后（如果镜像拉取快，15分钟左右），正常显示：Finished building Kubernetes cluster successfully。 使用 Kubectl 验证集群健康状态 mkdir -p ~/.kube cp kube_config_rancher-cluster.yml ~/.kube/config kubectl get nodes kubectl get cs 至此k8s集群安装完成 通过 helm 工具在k8s集群部署 Rancher Server 配置 helm kubernetes 集群权限 安装 helm 工具包 tar -xvf helm-v2.13.1-linux-amd64.tar.gz chmod +x linux-amd64/helm cp linux-amd64/helm /usr/bin/ 创建 helm ServiceAccount kubectl -n kube-system create serviceaccount tiller 创建 ClusterRoleBinding 以授予 tiller 帐户对集群的访问权限 kubectl create clusterrolebinding tiller \\ --clusterrole cluster-admin --serviceaccount=kube-system:tiller 初始化 helm 并安装 tiller（helm server 端） 查看 helm client 端的版本号 helm version helm init --skip-refresh --service-account tiller \\ --tiller-image \\ registry.cn-shanghai.aliyuncs.com/rancher/tiller:v2.13.1 tiller要和helm版本保持一致 准备安装 Rancher Server 相关证书 如果有自己官方证书，就用官方证书，如果没有证书，制作证书参考“证书申请与制作“章节 使用 helm 工具部署 Rancher Server 创建 namespaces kubectl create namespace cattle-system 创建 tls 密文 kubectl -n cattle-system \\ create secret tls tls-rancher-ingress --cert=./tls.crt --key=./tls.key 创建 ca 密文 kubectl -n cattle-system \\ create secret generic tls-ca --from-file=cacerts.pem 添加 helm chart 仓库地址 helm repo add rancher-stable https://releases.rancher.com/server-charts/latest 查看是否添加成功 helm repo list 执行 Rancher Server 创建操作 helm install rancher-latest/rancher \\ --name rancher \\ --namespace cattle-system \\ --set hostname=自己的域名 \\ --set ingress.tls.source=secret \\ --set privateCA=true 注：自定义域名和证书申请的域名要保持一致 验证 Rancher Server 运行情况 查看是否部署成功 helm list 查看 pod kubectl get deployment -n cattle-system 安装rancher server出错，可以用helm delete --purge rancher 删除后 重新执行 为Agent Pod添加主机别名 采用的是自定义域名而不是全局域名的话，需要在agent 里面配置自定义域名 如果是通过在/etc/hosts添加自定义域名方式指定的Rancher server访问URL，那么不管通过哪种方式(自定义、导入、Host驱动等)创建K8S集群，K8S集群运行起来之后，因为cattle-cluster-agent Pod和cattle-node-agent无法通过DNS记录找到Rancher server,最终导致无法通信。 可以通过给cattle-cluster-agent Pod和cattle-node-agent添加主机别名来解决； 在rancher UI界面，在system项目分别点击升级，进行编辑cattle-cluster-agent Pod和cattle-node-agent 点击显示高级选项；然后再网络里面 添加hosts别名，填写rancher server 服务器ip和自定义域名，完成后保存即可，待pod恢复后，状态会变成Active "},"k8s/rancher_offline_installation.html":{"url":"k8s/rancher_offline_installation.html","title":"离线helm安装高可用rancher","keywords":"","body":"rancher-server离线的高可用部署 本博客基于https://www.cnrancher.com/docs/rancher/v2.x/cn/installation/air-gap-installation/ha/ 官方文档补充和完善，主要是博主按照官网文档操作时，遇到一些问题，这里作为重点说明 1，提前搭建后本地仓库 harbor离线仓库搭建，参见harbor官网和我的install-harbor 文档，注意的时，如果用https协议，需要启用域名的，自定义域名要配置/etc/hosts 把自定义域名加进去 2，把rancher所需的镜像导入到镜像库 可以利用脚本执行 ./rancher-load-images.sh -l rancher-images.txt -i rancher-images.tar.gz -r 仓库地址/library 或者其他项目名称，如提前创建一个rancher ./rancher-load-images.sh -l rancher-images.txt -i ./rancher-images.tar.gz -r 172.16.35.31:1180/rancher 上面脚本和所需镜像获取，参考官方文档 https://www.cnrancher.com/docs/rancher/v2.x/cn/installation/air-gap-installation/ha/prepare-private-registry/ 3，二进制文件 kubectl rke helm等copy指定位置 这些二进制文件，下载地址https://www.cnrancher.com/docs/rancher/v2.x/cn/install-prepare/download/ 4，把镜像库的域名添加到其他机器hosts中、推送公钥等 需要注意如果仓库是https协议，自签名证书，需要把制作仓库证书生成的tls.crt 复制到其他docker主机上/etc/docker/certs.d/custom_domain/ca.crt 如：/etc/docker/certs.d/reg.czl.com/ca.crt 5，制作自签名证书 ./create_self-signed-cert.sh --ssl-domain=rancher.czl.com --ssl-trusted-ip=192.16.1.101 6，注意事项 剩下步骤按照官网文档：https://www.cnrancher.com/docs/rancher/v2.x/cn/installation/air-gap-installation/ha/install-kube/ 但是特别要注意下面事项 下面罗列一些注意事项 yaml配置文件 模板： nodes: - address: 192.16.1.101 # node air gap network IP user: zhenglin role: [ \"controlplane\", \"etcd\", \"worker\" ] private_registries: - url: reg.czl.com/library #如果用的私有签名https协议的harbor，此处用域名，不用IP，另外如果rancher镜像导入到library项目，URL需要带上library；如果是http协议的直接填ip就可以 user: admin password: \"P@ssw0rd\" is_default: true 安装Helm Server(Tiller) 这一步 可以从https://hub.docker.com/r/hongxiaolu/tiller/tags/ 获取相应版本tiller 也可以直接从 registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1 拉取，这个版本可以根据情况变化；然后再修改tag ,推送到私有仓库中 [root@Centos76 cert]# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1 reg.czl.com/library/rancher/tiller:v2.13.1 [root@Centos76 cert]# docker push reg.czl.com/library/rancher/tiller:v2.13.1 安装tiller helm init --skip-refresh \\ --service-account tiller \\ --tiller-image reg.czl.com/library/rancher/tiller:v2.13.1 先在chart解压目录下执行以下命令进行rancher安装，再来配置证书： helm install ../chart/rancher \\ --name rancher \\ --namespace cattle-system \\ --set hostname=rancher.czl.com \\ --set ingress.tls.source=secret \\ --set privateCA=true \\ --set rancherImage=reg.czl.com/library/rancher/rancher 一定要注意 其官方文档中--set ingress.tls.source=secret 没有换行符 所以不能直接粘贴用,set rancherImage时，后面rancher的版本不要带上tag，因为默认他会自动寻找，以下是带上v2.2.2的tag 出现的异常rancher-6f78c5bc69-g9djv 0/1 InvalidImageName 0 2m57s kubectl -n cattle-system edit deploy rancher image: reg.czl.com/library/rancher/rancher:v2.2.2:v2.2.2 可以看出 镜像的tag 被追加了 如果执行错了，可以用helm delete --purge rancher 删除后 重新执行 然后再到证书生成的目录 执行 kubectl -n cattle-system create \\ secret tls tls-rancher-ingress \\ --cert=./tls.crt \\ --key=./tls.key kubectl -n cattle-system \\ create secret generic tls-ca \\ --from-file=cacerts.pem 如下操作和在线安装文档一样 "},"k8s/deploy_app_in_rancher.html":{"url":"k8s/deploy_app_in_rancher.html","title":"rancher中快速部署应用","keywords":"","body":"rancher中快速部署应用 阅读前提：有docker知识储备，并对kubernetes有一定的了解 通过UI 部署应用 rancher在命名空间又抽象出一层，项目的概念（这个只是便于管理rancher抽象出来的，kubernetes中并没有这层）；我们可以先创建一个项目，然后在里面创建命名空间；这里为方便演示直接使用默认default的项目和default命名空间。 工作负载（deployment daemonset statefulset job等）部署 工作负载主要有deployment、statefulset、daemonset、job等类型，其实它们就是pod控制器，直接部署pod，很不容易控制；用不同类型控制器来生成和控制pod，就来的很方便了。如deployment是无状态的pod控制器，statefulset是有状态的控制器，一般用来部署数据库；daemonset是每个worker节点上都部署一个，一般用来采集worker节点的数据； job一次性的，完成后状态是completed，不再工作。这些控制器都有缩写的别名，具体可以参考博主的kubectl 常见命令使用。 1，在workloads tab中点击deploy 2，填写配置信息 填写镜像名称，副本数量、选择负载类型等关键信息 只有deployment和statefulset可以设置多个副本集； 3，端口映射 如图除\"Layer-4 Load Balancer\"外，还有三种选项；nodeport相当于四层负载一样，选择此选项后，在服务发现中会生成一个deployment同名-nodeport，这种格式的service；这个比较方便；访问集群任意节点ip+此nodeport端口 都能调度此deployment控制的pod上。hostport稍显鸡肋，pod调度到那个节点上，就可以通过节点ip+端口进行访问，如果没有调度的节点ip+端口是访问不了的。Cluster IP最好理解，就是基于它所在namespace组成的局域网，其他namespace或者外部是无法访问的；同一个namespace是可以直接访问的。 4，环境变量 除了可以使用自定义环境变量外，还可以使用内部的资源环境变量，如resource和field 5，调度规则 可以指定调度到（或不到）某个节点 也可以自定义匹配规则 如图中设置 调度只匹配amd64位的节点 在节点调度模块点击show advanced options后，还可以设置容忍度、亲和度匹配规则 6，健康检查 可以设置存活和就绪检查，默认是readliness 检查，设置完成后 存活检查也使用此规则；也可以点击右侧Define a separate liveness check增加一个存活检查。设置http和htpps方式检查 相对友好一下；tcp方式呢，可能应用占有端口还在，但已经不工作了；命令退出状态检查：需要提前把脚本放到镜像或挂载到容器中，执行此脚本后，返回码是否为0 作为判断的依据。 7，卷 这个不做特殊说明了，基本都是docker的知识；这是强调一下，可以把证书、configmap、证书挂载到容器中；如果选择挂载新的持久卷，会让创建一个持久卷；如果挂载一个已存在的持久卷，需要先创建一个持久卷。挂载空数据卷（ephemeral volume）这个比较简单。 8，更新策略 这个选择合适自己就行，如果这三种策略满足不了自己日常使用，可以自定义策略；默认选择\" Rolling: start new pods, then stop old\" 这个一般就能保证应用平滑更新；而且rancher UI上还支持回滚操作，很好保证了，升级错误能快速退回。 点击底部show advanced options后 ，能显示出剩下 使用频率相对较低配置项 9，command 关于entrypoint和command用法 有单独一章博客讲解docker run image -args对应yaml语法/rancher UI操作方式，这里不再赘述。 10，网络（networking） 网络这块，可以修改使用主机网络空间，一些host别名，以及一些DNS解析等配置； 11，标签和注解 主要是给此deploy下的pod打标签和注解的 12，安全 这一块配置也很重要，由于容器为了安全，是和外部环境是隔离的，想使用某些资源，就需要关闭或开启一些安全策略； 此模块还集成了资源限制和提权（最后的双侧复选框）；如果pod采用了HPA（水平扩展），一定要做好存活和资源限制配置。这样一旦触发了资源配置上线，就会自动扩展。 13，菜单功能 右上角菜单，也有很多功能；主要是编辑：修改deploy用的；clone：可以复制此deploy，然后做修改，保存。redeploy：重新部署，这个一般在configmap等更新后，可以重新部署一下。增加边车：可以增加一个辅助容器或者init容器；剩下的根据字面意思 就能知道它作用。最重要的是自己动手操作一遍，加上有一定k8s基础，很容易就找到对应关系并掌握UI上操作的方法。 负载均衡配置（ingress） ingress其实也是一种service，相当于增加一个支持7层负载的nginx，并带有域名；nginx（也可以是其他代理软件甚至是硬件）把接收到请求反向代理到匹配的pod上。新增ingress时，可以选自定义生成的域名，也可以使用自己购买的域名； 选择目标，可以是service或者工作负载（workload）；证书、标签注解都能在此UI配置。 服务发现配置 service可以这么理解：通过标签的键值对，选择匹配的pod，然后用户或其他应用访问入口是service，service调度满足匹配条件的pod上。 \"One or more external IP addresses\"和\"An external hostname\" 类似,把外部ip或者域名定义为内部dns，前者可以一次性定义多个ip，后者只能定义一个。如选择\"One or more external IP addresses\"类型，名称填写my-data;Target IP Addresses填写 172.17.1.4 ；此集群内部pod可以直接访问my-data，dns自动解析到172.17.1.4 \"One or more workloads\"和\"The set of pods which match a selector\"类似，前者相当于工作负载选择器，后者相当于标签选择器；前者通过标签选择工作负载（deployment、daemon-set、statefulset)，后者通过标签选择pod。 在rancherUI部署应用如果采用nodeport，会自动产生两个service，一同名service，一个是加上“-nodeport”的service；如果不想使用rancherUI自动生成的service，可以自己定义一个“The set of pods which match a selector” 标签选择器,这里面再来定义cluster_ip 或nodeport； \"Alias of another DNS record's value\" 这个暂时没未深入研究。 数据卷 数据卷就是持久化存储的 添加数据卷，填写名称，选择提前定义好持久卷，就可以使用了。 定义持久卷可以在storage中定义，有持久卷和存储类两种类型 存储类是可以动态扩展的；持久卷一般定义多少，最大就能使用这么多。存储技术现在很多样，kubernetes（rancher）支持 这么多，可以提前搭建好后，直接使用（如何搭建不同存储平台，这里不做描述）。 其他配置 资源 证书 这个一般是存放CA或其他机构颁发证书、甚至是自定义的证书文件，如tls.key tls.pem这类文件的；最后可以通过volume的形式挂载到容器 configmap 这个configmap 最常用的作用有两个，一个是编写环境变量，然后在工作负载部署时，环境变量可以直接引用此configmap，另一个就是存储应用配置文件，如上图；key就是配置文件名称，value就是配置文件内容；然后通过volume挂载到容器程序存放配置文件的地方。 如部署nginx时有三个配置文件，都写到configmap后，再把三个配置文件（键值对） 都挂载容器同一个目录，并且覆盖掉容器里面配置文件，可以在Mount Point中直接填写路径（只保留配置文件路径，不接配置文件名），Sub Path in Volume中不填写任何内容；如果想把configmap的文件挂载到容器，同时容器中默认的文件不被删除（有同名文件除外），Sub Path in Volume 填写挂载到容器中文件要显示的名称即可； Optional和items是匹配的，Optional为true，items 才可以设置，这个一般是用来只把configmap中 部分文件挂载到容器中使用的； 密文 这个一般用来存储敏感信息，比如密码；然后和configmap一样，可以通过环境变量引用，也可以通过volume挂载也可以；相对简单，操作一两遍 即可掌握。 命名空间和工具成员比较简单，略过 rancherUI 能配置的就这么多，配置不了，可以通过编辑yaml，来手动填写 通过导入yaml部署应用 这个是最简单的，把写好的yaml文件，导入进来就可以了；导入后，如果有问题，根据日志或者event提示，修改调整一下即可。需要说明的时，如果导入时，没有配置文件里面命名空间，需要提前创建命名空间。如果导入yaml文件，有创建命名空间操作，不能和已有命名空间重名；并且导入后，需要给移动到某个项目中。 如何验证部署应用是否运行正常 验证方法有很多，能达到目的就行。一定要灵活，不能把自己限定死。 除了常规的查看日志，通过浏览器直接访问，或进入shell ，进行验证外；我这里重点讲的是借助其他容器进行验证。 推荐如下两款镜像 hwchiu/netutils busybox 在待验证的应用同一个命名空间中，直接rancherUI中直接部署其中一款镜像，无需暴露端口；只需要在部署成功后，进入shell界面；busybox一般支持 nslookup 、telnet等命名，可以直接nslookup service-name/telnet service:ip 而netutils镜像集成工具相对更丰富一些，比如mongo-client redis-client有了这些数据库的客户端，就可以用它连接数据库，测试数据库是否能正常连接，加密是否生效；如redis-cli -h service_name 看看能连接部署redis，连接成功后可以再执行一些命令，如ping ，看看能不能执行成功；如果redis加密了，输入密码后再次执行ping，是否能成功等。 以上就是提供验证的思路，具体操作，需要自己根据情况来实践。 "},"k8s/use_appstore_deploy_es_in_rancher.html":{"url":"k8s/use_appstore_deploy_es_in_rancher.html","title":"利用rancher商店搭建es和beat","keywords":"","body":"rancher应用商店的使用 ​ 应用商店可以理解为helm源或charts仓库；启用后，找到自己要部署的应用，直接通过helm模板进行部署。在部署的时候 有些问答就是重置默认key-value配置的。 配置应用商店 全局启用、配置应用商店 默认只开启了基于Library源的应用商店，这个源由rancher官方维护，稳定性较强。\"Helm Stable\"由helm官方维护，稳定性也行，但没有针对rancher进行优化；\"Helm Incubator\"这个呢是helm社区维护的，稳定性一般。我们可以都设为enable后，进行使用 项目中启用应用商店 直接点击Launch，进行启用，启用后，会发现很多应用的helm源 添加其他charts仓库 可以添加自己私有chart仓库、以及其他企业的（如阿里、elastic、bitnami）到应用商店； 可以选择为全局、集群、项目三个范围内的资源，根据情况灵活添加，如果添加为项目范围的，只能在此项目中使用此helm源（商店）；使用私有helm源，需要提供用户名和密码； 下面罗列了几个比较重要的charts仓库地址。 阿里charts仓库 https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts bitnami https://charts.bitnami.com elastic 如添加elastic helm源 应用商店搜索elastic 发现有很多Elastic-Helm提供的。 说了这么多，接下来真正的实战。 实战 利用rancher 应用商店搭建 elasticsearch+kibana+apm-server 应用商店搜索所需应用 rancher应用商店 搜索efk；我们选择来自Library，这个已经集成了elasticsearch+kibana+fluent；点击View Details,进入配置自定义页面。 命名空间，这些可以指定如果不知道就会部署到默认的efk中。 选项配置： jvm也可以根据情况来调整 ，默认512 m，就可以；调整1g的话，如果运行多个实例在一台机器 较吃力，如果机器性能好，可以适当调整大一些。使用默认镜像选择false后，就可以指定镜像，这里面镜像版本相对落后，从elastic官网镜像https://www.docker.elastic.co/#获取到当前最新稳定版，然后填写进去（这里镜像名称和tag输入框不在一起）。 这里不需要f(luent),就不启用，镜像名称也无需修改了。 调整与完善 elasticsearch参数调整 自定义配置完成后点击launch，待镜像都拉取完成后；根据提示继续完善和调整。这里面elasticsearch组件默认使用的是空数据，我们如果修改为持久卷的话 ，不管是主机是映射，还是持久申明的卷，都要给777权限 我这里没有搭建存储平台，就采用了本地Local Node Path做的持久卷（和主机目录映射差别不大）， 主机映射那个目录只能被1个elasticsearch使用，所以configmap中，变量discovery.zen.minimum_master_nodes 此处修改为1；（这样把elasticsearch主机调度到一台机器上就行了）；discovery.type = single-node 如果上个参数修改为1的话，discovery.zen.ping.unicast.hosts 这个配置在7.0版本后已经不建议使用了 ，也可以去掉 若继续使用默认的空数据卷，elasticsearch单pod也可以按照这种方式修改，具体环境变量参数看截图（在Resources-config maps中可以找到这些变量） 注：node.data=true 这个参数要添加上，避免多次安装出现的问题 kibana参数调整 首先查看对应镜像版本，说明文档https://www.elastic.co/guide/en/kibana/7.1/docker.html#environment-variable-config Docker defaultsedit The following settings have different default values when using the Docker images: server.name kibana server.host \"0\" elasticsearch.hosts http://elasticsearch:9200 xpack.monitoring.ui.container.elasticsearch.enabled true 然后和kibana的configmaps中定义环境变量参数对比一下 若还想调整es和kibana日志 也可以在环境变量中设置 kibana的设置： LOGGING_QUIET true LOGGING_VERBOSE false es设置： logger.org.elasticsearch.transport warn 部署apm-server 应用商店继续搜索：apm-server，直接进入查看详情页面，然后把此应用 指定到和efk 应用同一个命名空间里 apm-server 配置文件和环境变量的调整 到secrets（有的是config map）进行配置， 有个名称为apm-server.yml 的key，其value按照如下修改，把数据写入文件的配置改为写入到elasticsearch中。 另外，需要在环境变量中 加入两个参数： setup.dashboards.enabled=true config.output.file.enabled=false 因为这个在应用商店安装的apm-server，默认是把数据存到file的，前面修改了输出为elasticsearch，这里再把输入到文件设为false；（自己手动部署的 并不需要，应用商店的需要检查一下） 查看apm-server，进行验证 然后点击Launch APM，就可以使用了。如果根据页面提示把apm-agent 嵌入被测应用后，再查看apm-server采集到的数据 若探针采集的数据已经发送过来了 点击进去可以查看详细数据； 手动部署filebeat+metricbeat+packetbeat向es发送数据 部署filebeat 这个其镜像详情里面就提供k8s部署的yaml，稍作修改就可以使用 https://www.elastic.co/guide/en/beats/filebeat/版本号/running-on-kubernetes.html 下载所需yaml后，修改里面命名空间到自己制定的空间 ，直接在rancher中部署完成后修改输出elasticsearch地址（config maps中）,如果是在环境变量中定义elasticsearch地址，直接就在环境变量中修改，这个视情况而定。 部署metricbeat 这个有点复杂些，官方也提供了(具体下载根据版本情况）k8s所需的yaml文件： https://www.elastic.co/guide/en/beats/metricbeat/6.6/running-on-kubernetes.html 把官网提供的yaml下载下来后，用rancher导入到kube-system空间中； 他会部署两个metricbeat，一个daemonset 一个deployment，deployment可以暂停；如果没有kube-state-metrics组件，因为设定了依赖关系，他也会自动部署一个；然后修改配置文件config-map ,第一个配置文件metricbeat.yml：设置抓取的信息要输入到正确的elasticsearch和kibana中 或es输入直接设置为 output.elasticsearch.hosts: ['${ELASTICSEARCH_HOST}'] 把多余的变量删除掉，直接把ELASTICSEARCH_HOST 在环境变量中定义为 http://es_ip:port 第二个配置文件kubernetes.yml module: kubernetes metricsets: ​ \\- node ​ \\- system ​ \\- pod ​ \\- container ​ \\- volume period: 10s host: ${NODE_NAME} \\#hosts: [\"localhost:10255\"] \\# If using Red Hat OpenShift remove the previous hosts entry and \\# uncomment these settings: hosts: [\"https://${HOSTNAME}:10250\"] ssl.verification_mode: \"none\" ssl.certificate_authorities: ​ \\- /etc/kubernetes/ssl/certs/serverca ssl.certificate: \"/etc/kubernetes/ssl/kube-node.pem\" ssl.key: \"/etc/kubernetes/ssl/kube-node-key.pem\" 需要注意地方: 第一个hosts: [\"localhost:10255\"] 修改为hosts: [\"https://${HOSTNAME}:10250\"] 参照https://www.elastic.co/guide/en/beats/metricbeat/current/running-on-kubernetes.html红帽的配置方法 然后把/etc/kubernetes/ssl/certs/serverca、/etc/kubernetes/ssl/kube-node.pem和/etc/kubernetes/ssl/kube-node-key.pem 在工作负载界面通过主机映射挂载进去就行了 （如果没有这些证书文件，是无法从相关接口获取数据，这些文件都是用来鉴权的）。 最后在环境变量中把正确的es地址信息填写进去就可以了 部署kube-state-metrics 如上图中用到kube-state-metrics的数据，没有的话，会自动部署一个；但kube-state-metrics没有部署成功，后者提供的镜像有问题怎么解决呢； 我们可以从https://github.com/kubernetes/kube-state-metrics/blob/master/kubernetes/kube-state-metrics-deployment.yaml 找到正确的镜像文件名称，对照修改镜像名称为：quay.io/coreos/kube-state-metrics:v1.5.0 即可完成部署（现在可能是1.6版本了）。这个地址也有部署需要的各种yaml文件，需要注意不同的版本；我们把 kube-state-metrics-service.yaml、kube-state-metrics-service-account.yaml、kube-state-metrics-deployment.yaml 等所有yaml文件下载下来，先导入SA(service-account)类的yaml，剩下的yaml再逐个导入； 由于master版本可能不确定是那个版本，建议选用release中的具体版本；通过rancher把这些yaml导入到系统空间\"kube-system\" yaml文件里面多个镜像仓库是谷歌的，如果你没有科学上网的方法，就像我一样，换成其他公司同名镜像；如image: k8s.gcr.io/addon-resizer:1.8.3，可以在dockerhub上搜到其他公司提供的;image: siriuszg/addon-resizer:1.8.4，我没有搜索到1.8.3 就选更高版本的1.8.4的了，一般原则是相同版本，然后dockerhub排名较高的镜像 部署完成后，把kube-state-metrics的8080端口映射出来，我们通过浏览器访问验证一下： 访问kube-state-metrics页面会显示健康 \"/health\" 相关指标。 最终采集到的数据在kibana呈现验证： 部署：packetbeat 这个相对简单，官方只提供的了docker的部署方式，我们对着参数 部署到rancher即可： https://www.elastic.co/guide/en/beats/packetbeat/current/load-kibana-dashboards.html docker run --net=\"host\" docker.elastic.co/beats/packetbeat:7.2.0 setup --dashboards 入口命令怎么在rancherUI里面配置，单独一章博客有介绍image和container中的entrypoint、cmd关系 另外还要增加 --strict.perms=false -e -E output.elasticsearch.hosts=ip:port使数据发送到之前搭建的es里面，--net=host 在rancherUI对应的操作是：networking模块把\"Use Host's Network Namespace\"设为真; packetbeat是用来采集host（宿主）机器的数据，部署到容器中，有些数据可能抓取不到;可能需要提权,也可以在rancherUI最后配置项\"Add Capabilities\" 里面把下面两项增加上。 --cap-add=\"NET_RAW\" \\ --cap-add=\"NET_ADMIN\" \\ 经过多次验证，把如下文件挂载到容器，基本和在host（宿主）机器上部署packetbeat效果一样。 volumeMounts: - name: cgroup mountPath: /host/sys/fs/cgroup/ readOnly: true - name: proc mountPath: /host/proc/ readOnly: true - name: docker-sock mountPath: /var/run/docker.sock readOnly: true volumes: - name: cgroup hostPath: path: /sys/fs/cgroup/ - name: proc hostPath: path: /proc/ - name: docker-sock hostPath: path: /var/run/docker.sock 最后有一点要注意 把主机时间也映射到所有的容器中，避免容器时间不一致导致的一系列问题。 "},"k8s/how_to_use_kubectl_noserver.html":{"url":"k8s/how_to_use_kubectl_noserver.html","title":"rancher平台不可用下如何使用kubectl命令","keywords":"","body":"如何在rancher平台宕机情况下，继续使用k8s集群 前提 首先保障rancher管理k8s集群的config文件还在，可以查看~/.kube/config 进行验证 kubectl config view kubectl config use（或者use-context ） xxx ；切换到不同配置项（master上的配置），来使用kubectl #xxx是contexts的name值 验证： kubectl get nodes kubectl get ns 验证是否可以看到节点和命名空间 使用kubectl命令来操作集群 ]$ kubectl set image deployment/reactor reactor=myimage:2.7.1 -n myns --dry-run(空跑一遍 进行验证） ]$ kubectl set image deployment/reactor reactor=myimage:2.7.1 -n myns 验证 ]$ kubectl get deployment/reactor -n myns -o yaml 若rancher彻底不可恢复 待kubectl命令可以使用后，在新搭建rancher平台中，把此集群导入进去，导入完成后 要重建项目，把之前的命名空间移过去即可。 具体导入命令，参考rancher官网 "},"jenkins/jenkins-slave-for-docker.html":{"url":"jenkins/jenkins-slave-for-docker.html","title":"Jenkins调用docker编译程序","keywords":"","body":"如果用docker 容器编译程序 有两种方案可供选择 1，激活镜像作为slave编译 采用Jenkins提供的jnlp-slave 或ssh-slave 标准镜像二次封装，或者初始镜像，然后通过label 选择镜像后进行编译； 这种编译的原理是：Jenkins通过标签选择相关docker镜像，并激活成容器，把此容器当做slave（节点机）使用； ​ Jenkins提供的镜像地址：https://hub.docker.com/u/jenkinsci ​ jnlp-slave 和ssh-slave 镜像都能激活作为slave节点使用，区别是采用不同协议连接到容器内部：ssh和jnlp； 使用这种方式需要额外在Jenkins里面配置，插件里面安装docker插件，然后配置Docker Host URL,来找到可以使用的docker，如果Jenkins和docker在同一台服务器可以直接填写为：unix:///var/run/docker.sock ，如果不在同一台机器要填写docker所在的机器ip：tcp://ip:2375，并且要在docker所在机器的 /etc/docker/daemon.json 里面添加2375端口 {\"hosts\": [\"unix:///var/run/docker.sock\", \"tcp://0.0.0.0:2375\"]} 或{\"hosts\": [\"fd://\", \"tcp://0.0.0.0:2375\"]} 如果添加后，docker无法启动，十有八九是和docker.service 冲突了，查看 /lib/systemd/system/docker.service【centos：/usr/lib/systemd/system/docker.service】 如果有 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock 可以修改为 ExecStart=/usr/bin/dockerd --containerd=/run/containerd/containerd.sock，使用daemon.json的配置；还有一种情况，可能某个系统daemon.json中配置不生效，可以修改docker.service，ExecStart参数中添加 -H fd:// -H tcp://127.0.0.1:2375 (也可以专门在docker.service同级目录下创建 docker.service.d/override.conf，内容如下 [Service] ExecStart= ExecStart=/usr/bin/dockerd -H fd:// -H tcp://127.0.0.1:2375 甚至sudo systemctl edit docker.service 时，添加上面代码，自动生成override.conf文件)，一定要灵活 注：fd:// 等价于 unix:///var/run/docker.sock，可以相互替换 具体怎么调用这里不作为重点，这里重点说明的方案的选择，可以参考 https://blog.csdn.net/qq_31977125/article/details/82999872 这位博主有详细的操作步骤 下面是重点说明编译方式 2，直接docker run编译 把源码下载到宿主机，通过-v 挂载到容器中，然后指定入口命令编译此目录，编译完成后 销毁容器 docker run --rm -v `pwd`:/mypro -w /mypro nodeshift/centos7-s2i-nodejs:10.16.0 /bin/bash -c \"npm install\" docker run --rm -v `pwd`:/opt/mypro -w /opt/mypro goenv-centos:test7 /bin/bash -c \"GOPROXY=https://goproxy.io /usr/local/go/bin/go build\" 上面两个是例子，goenv-centos:test7是自制作的镜像 注意：提前制作好镜像，镜像里面把各种编译依赖放进去，制作镜像有两种方式 ​ 激活一个基础镜像，编译下载各种依赖库，能正式编译后，commit容器为镜像，这种方式不推荐 ​ 建议把操作步骤整理起来 编写到dockerfile中 实战案例 基于centos7.6 镜像制作出可以编译go/rust程序的镜像（生产中最好一个镜像只编译一种语言程序，并且是基于此语言的镜像） Dockerfile FROM centos:7.6.1810 WORKDIR /opt ADD https://github.com/facebook/zstd/releases/download/v1.4.0/zstd-1.4.0.tar.gz ./ ADD https://github.com/edenhill/librdkafka/archive/master.zip ./ ADD https://studygolang.com/dl/golang/go1.12.5.linux-amd64.tar.gz ./ COPY expect.sh ./ RUN mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.bak && \\ curl -s http://mirrors.aliyun.com/repo/Centos-7.repo -o /etc/yum.repos.d/CentOS-Base.repo && \\ curl -s http://mirrors.aliyun.com/repo/epel-7.repo -o /etc/yum.repos.d/epel-7.repo && \\ yum repolist && \\ yum -y install wget make expect gcc gcc-c++ unzip openssl-devel clang-devel libpcap-devel perl.x86_64 \\ which.x86_64 git && \\ curl -sSf https://sh.rustup.rs -o rustup.sh && chmod a+x rustup.sh && \\ chmod a+x expect.sh && /bin/bash ./expect.sh && \\ source $HOME/.cargo/env && \\ tar xzf zstd-1.4.0.tar.gz && \\ tar xzf go1.12.5.linux-amd64.tar.gz -C /usr/local && \\ unzip master.zip && \\ cd zstd-1.4.0 && CFLAGS=\"-O3 -fPIC\" make install && \\ cd ../librdkafka-master && ./configure --prefix=/usr && make && make install && \\ cp -rf /usr/lib/librdkafka* /usr/lib/pkgconfig /usr/lib64 && \\ chmod a+x -R /usr/local/go && \\ echo -e 'export GOPROXY=https://goproxy.io' >> /etc/profile && \\ echo -e 'export GOROOT=/usr/local/go' >> /etc/profile && \\ echo -e 'export GOPATH=/opt/mypro' >> /etc/profile && \\ echo -e 'export export PATH=$GOROOT/bin:/root/.cargo/bin:$PATH' >> /etc/profile && \\ echo -e 'export LIBRARY_PATH=$LIBRARY_PATH:/usr/lib64/llvm' >> /etc/profile && \\ echo -e 'export LD_LIBRARY_PATH=/usr/local/lib/:${LD_LIBRARY_PATH}' >> /etc/profile && \\ source /etc/profile && \\ yum clean all && \\ rm -rf /opt/* && \\ mkdir -p /opt/mypro COPY ustc-config /root/.cargo/config ustc-config [source.crates-io] registry = \"https://github.com/rust-lang/crates.io-index\" replace-with = 'ustc' [source.ustc] registry = \"git://mirrors.ustc.edu.cn/crates.io-index\" expect.sh #!/usr/bash expect 把Dockerfile，和expect.sh、ustc-config放在同一目录，然后在此目录执行build命令： docker build -t 172.16.35.31:1180/apm-images/centos7-goenv:1:1 . go程序编译： docker run --rm -v `pwd`:/opt/mypro -w /opt/mypro 172.16.35.31:1180/apm-images/centos7-goenv:1 /bin/bash -c \"GOPROXY=https://goproxy.io /usr/local/go/bin/go build\" rust程序编译 docker run --rm -v `pwd`:/opt/mypro -w /opt/mypro 172.16.35.31:1180/apm-images/centos7-goenv:1 /bin/bash -c \"/root/.cargo/bin/cargo build --release\" 这种方式编译最大优点，不管是开发、运维还是测试都无需搭建编译环境（有时候搭建一套编译环境是费时费力的事情，而且移植性差），直接从docker仓库拉取此镜像后就能编译； "},"jenkins/install-jenkins.html":{"url":"jenkins/install-jenkins.html","title":"利用docker 镜像，快速搭建Jenkins环境","keywords":"","body":"利用docker image快速搭建Jenkins环境 关于安装部署Jenkins，网上一大堆资料，这里不做详细说明了；可以下载Jenkins的war包直接放到tomcat（其他Java容器也行）通过ip:8080/jenkins即可访问，也可能通过 java –jar Jenkins.war来安装或者带上--ajp13Port=-1 --httpPort=8081参数指定端口就行； 这里重点说的是采用docker化部署，这种方案更快捷和灵活： docker run \\ --name myjenkins \\ --cpus=4 \\ --restart=unless-stopped \\ -u root \\ -d \\ -p 88:8080 \\ -p 50002:50000 \\ -v /home/jenkinsci/jenkins:/var/jenkins_home \\ -v /opt/jenkins-bak-file:/opt/jenkins-bak-file \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /etc/localtime:/etc/localtime:ro \\ -v /etc/timezone:/etc/timezone:ro \\ -v $HOME/.ssh:/root/.ssh:ro \\ jenkinsci/blueocean /etc/timezone 如果没有可以创建一个:echo Asia/Shanghai >/etc/timezone 这个 一定要设置，要不然就是GMT时区了； /opt/jenkins-bak-file目录是为以后Jenkins自身备份预留的目录； 把.ssh挂载进去，让容器使用宿主机的私钥； 如果使用jenkins:2.60.3镜像的话，宿主机的/home/Jenkins 目录一定要sudo chown -R 1000 处理一下；但不推荐这种镜像，已经不支持blueocean了； 如果容器中使用默认的jenkins账户，需要把宿主机的/home/jenkinsci/jenkins目录做一下sudo chown 1000 -R处理，另外宿主机操作docker账户的$HOME/.ssh目录挂载到容器/var/jenkins_home/.ssh下，这样容器就可以使用宿主机的公私钥；更保险的方法是直接进入jenkins容器，生成公私钥，因为公私钥所在目录/var/jenkins_home挂载到宿主机，不用担心重启后找不到的问题； "},"jenkins/freestyle_build_in_jenkins.html":{"url":"jenkins/freestyle_build_in_jenkins.html","title":"jenkins自由风格构建job","keywords":"","body":"新建一个自由风格的job 自由风格的job，最容易上手，一般项目初期或者小型团队和不是特别复杂工程，都可以选择这种风格job进行编译构建。 创建 点击左上角New Item(中文语言环境下是“新建”) 填写名称，选择构建什么类型项目，这里选择自由风格的 job配置 然后点击ok，进入job编辑界面，可以简单填写一下描述信息 可以参数化 构建，如选string Parameter ,这个参数设置一个默认值，每次构建时 都可以修改这个值；这个值也可以由上游job传递过来。 选择构建保留天数和次数 节点选择 ，选择合适节点构建job（默认是master节点） 选择节点是通过label表达式，所以新增salve机器时 加上标签，如果没有标签则默认为节点机名称 如果需要进行源码管理，根据实际情况选择源码管理工具，如git(svn 都支持) 理论上一个job最好只编译一个git仓库代码，如果想获取多个仓储代码，可以添加Multiple SCMs插件； 然后可以设置 拉取多个仓库源的代码，并设置检出到一个子路径。 特别注意：如果拉取代码选用ssh协议，宿主机从gitlab或GitHub仓库拉取代码，一定把这个节点机给Jenkins使用的账户家目录下公钥复制到gitlab或github 的\"SSH keys\" 内。http协议 直接输入git仓库账户和密码就行了。 设定正确的触发器； Build after other projects are built： 这种触发方式也非常实用，如果a程序是用来编译和打包的，b程序是用来更新的；b程序里面就可以用这种方式来触发，然后projects填写a就行，如果是pipeline项目填写到具体分支； Build periodically： 选择build periodically，然后按照规则进行设定，设定规则如下： 第一个参数代表的是分钟 minute，取值 0~59； 第二个参数代表的是小时 hour，取值 0~23； 第三个参数代表的是天 day，取值 1~31； 第四个参数代表的是月 month，取值 1~12； 最后一个参数代表的是星期 week，取值 0~7，0 和 7 都是表示星期天。 所以 0 表示的就是每个小时的第 0 分钟执行一次构建。 例子如下： H 17 * 17点 表示17点进行执行构建任务 H 8,12,15,16 0-6 一天的8点，12点，15点，16点， 每个星期的七天都是如此 如果设成间隔时间，写成 */5 这种格式 Poll SCM： 定时检查源码变更，如果有更新就checkout最新code下来，然后执行构建动作。里面为空就行了；也可以设置为定时任务。 Build when a change is pushed to GitLab： 如果想设定gitlab仓库上master分支一有提交或者合并就触发构建，我们就可以勾选Build when a change is pushed to GitLab，使用它来实现这个功能（前提是jenkins已经安装gitlab hook插件）； 如果勾选了Filter branches by name，就针对某些分支才能触发，如图圈中2，填写包含的分支，多个分支用，隔开；图中3填写排除的分支； 点击4下面的Generate，生成Secret token码；然后进入gitlab的webhooks界面；把上面图中圈中1的地方url填写到下面图中圈中1地方；生成Secret token copy到下图中4的地方； 并勾选上需要的触发条件，配置完成后，点击add webhook ,即可；我们可以通过点击右下角的test按钮进行测试； 若点击test时，出现jenkins gitlab webhook 403 anonymous is missing the Job/Build permission提示，可通过“系统管理 -> 系统设置 -> 去掉 Enable authentication for ‘/project’ end-point” 来解决； 执行构建脚本 构建 注：点击“See the list of available environment variables”蓝色链接字体，可以查看到jenkins自带一些参数及用法示例； 也可以把脚本直接写入到此输入框中，这种模式下Jenkins任务执行完成后，会杀掉脚本里面所有新增进程。这是Jenkins自身机制问题，如何避免，见持续集成实战篇博客。 脚本里面有些参数，如ATRACE_HOME=aaa可以直接在节点里面配置，选择对应node——node Properties——Environment variables中 key:ATRACE_HOME value:aaa 像java_home、git、mvn 节点机上没有写到环境变量的参数，这个地方都可以进行配置。 存档和构建后操作 如果构建完成后，会产出压缩包，我们可以把压缩包存放到当前构建目录，进行存档； 如果构建完成后，把压缩包，或者其他东西copy 指定节点机上，都可以在构建后操作中配置。 其他细节 如果嫌构建后自动形成的名称不符合自己要求，可重命名。 也可以安装插件，让构建后名称按照指定格式显示。 "},"jenkins/variety_pipeline_build.html":{"url":"jenkins/variety_pipeline_build.html","title":"jenkins多种pipeline构建job","keywords":"","body":"pipeline风格的构建 pipeline顾名思义，流水线；这种风格job具有良好的阅读性；这里重点要说的是传统pipeline和多分支pipeline两种构建方式； 如上图所示两种pipeline pipeline就是传统的流水线构建，构建过程用（groove）script语言 按照先后步骤、写出来。构建完成后，可以看出各个步骤构建用时，和构建情况。 Multibranch Pipeline 相当于pipeline的加强版，把构建用的脚本直接和源码一起存储在仓库；这样Jenkins和仓库连接后，获取脚本文件jenkinsfile后 就能执行jenkinsfile里面脚本；如果分支里面没有jenkinsfile，Jenkins则无法基于此分支构建。 jenkinsfile好处在于语法不光可以使用传统的script形式的，且可以更好的支持申明式语法；申明式语法以后会成为主流，这种语法可读性、可理解性好。如果jenkins安装了blue ocean可视化插件，可直接进行可视化编写、修改脚本，完成后；自动生成jenkinsfile，然后提交到代码仓库。 pipeline 新建页面，给job命名后直接选择pipeline风格 然后在job里面会有个pipeline脚本输入框；我们需要提前把构建过程、构建步骤罗列出来； 如：1，选择构建节点机=>拉取代码=>编译=>打包=>存档 2，拉取代码=>编译 =>部署 =>发送邮件 这些构建步骤、流程要根据实际生产情况来决定。每一步的构建目的可以作为stage的名称。 在输入框下面有个\"Pipeline Syntax\"语法翻译器,这个特别有用。比如说从某个项目拉取代码的脚本不会写，可以参照下图，把要操作的步骤配置后，点击\"Generate Pipeline Script\" 就自动生成了script，把这个脚本粘贴到pipeline 输入框即可。 申明式指令生成器 这里面生成语法都是基于申明式语法的。 全局变量 这里面提供pipeline 可能会用到的变量。 下面是jenkins官方提供pipeline例子，我们可以借鉴着编写。 https://jenkins.io/doc/pipeline/examples/ 这里提供一个简单的script示例。 node { stage('拉取代码') { git credentialsId: '0e9c3bxxxxxx', url: ' git@git.xxxx.com:platform/xxr.git' } stage('编译') { dir('./') { sh '/data/local/maven-3.5.4/bin/mvn clean install -DskipTests=true' } } stage('镜像构建') { dir('./') { docker.build(\"git@git.xxxx.com:platform /test_pipeline:prod-${BUILD_ID}\").push() } } stage('镜像构建') { dir('./') { rancher confirm: false, credentialId: 'xxxxxxxxxxxxxxxxxxxxx', endpoint: 'http://docker仓库地址:8080/v2', environmentId: '1a5', environments: '', image: 'docker仓库ip/ test_pipeline:prod-${BUILD_ID}', ports: '', service: 'project /test_pipeline_1', timeout: 50 } } } 构建成功后，每一步都有详细构建用时 Pipeline使用语法详解： https://jenkins.io/doc/book/pipeline/syntax/（官方文档，英文） https://blog.csdn.net/diantun00/article/details/81075007（中文翻译版） Multibranch Pipeline 新增多分支pipeline 把jenkins调整到blueocean模式下，然后创建(Multibranch)Pipeline 需要注意的是，如果git仓库是在本地，没有全局域名最好填写ip；jenkins主机的公钥填进去。 1，要确保自己有新创建分支的权限 2，如果没有创建分支的权限，可以让管理员给自己创建一个自己提交合并代码的分支； 然后选择此分支进行编辑，如果使用节点机构建的话，在agent中选择node（节点机），若本机选择master即可，若不依赖环境可以选择any;如果有docker环境也可以选；Lable选择节点机名称（提前在node里面配置好节点，保持jenkins与节点机的连接状态）； 也可以使用ctrl +s组合键进入快捷编辑界面，直接编写脚本 完成后保存，提交后，代码仓库中会多出一个Jenkinsfile（首字母J是大写的）文件，存放我们刚才编辑生成的脚本；可以在仓库中打开一下Jenkinsfile，查看一下是否正确。需要注意的时，这个编译脚本只针对当前分支，如果以后的分支都想使用；可以把Jenkinsfile都带上，如果编译有差异，可以再通过界面进行修改和微调。 特别注意：不能在当前分支修改后，提交到另一个分支；如在dev分支修改jenkinsfile后，选择提交到master分支，这些会把master分支覆盖掉；可以在本地修改jenkinsfile，再单独把Jenkinsfile，cherry-pick 或checkout 到master中 关于触发器，其他配置的语法，可以自行阅读官网文档，非常详细。 https://jenkins.io/doc/book/pipeline/syntax/ 说明文档相关截图和说明 说pipeline采用了申明式语法，语法的要求等。 右侧目录树，展示了申明式和脚本式语法几大模块。 使用blue ocean 模式编译pipeline好处在于能可视化进行操作，自动生成pipeline 代码，如果可视化里面提供的功能不够，可以ctrl+s，切换到源码界面，然后对照pipeline语法，进行完善。 如果有新增分支了，job页面没有发现，可以在配置页面中点击\" Scan Multibranch Pipeline Now\" 发现新增的分支。但它有个缺点，把所有有更新的分支 都执行一遍。 Jenkinsfile 代码示例 pipeline { agent { node { label 'master' } } stages { stage('Build') { steps { echo 'Build' sh 'sudo chmod 755 deploy/scripts/jenkins/build.sh &&./deploy/scripts/jenkins/build.sh' } } stage('Deploy') { steps { echo 'Deploy' sh 'rm -rf app_receiver*.tar.gz' sh 'tar -czvf app_receiver_${BRANCH_NAME}.tar.gz * --exclude=.git* ' } } stage('archive_file') { steps { echo 'tar app_receiver' archiveArtifacts(artifacts: 'app_receiver*.tar.gz', onlyIfSuccessful: true) } } } tools { nodejs 'nodejsv9' } triggers { pollSCM('H */4 * * 1-5') } } 如何把带有Jenkinsfile的源码纳入到现有Jenkins平台 1，先建一个Multibranch Pipeline 风格的job 2，填写有jenkinsfile文件的仓库地址 3，点击保存后，自动发现和运行 4，在可视化环境中或者clone源码后修改Jenkinsfile中已经不适用的地方； jenkins操作手册说明 官方操作手册地址：https://jenkins.io/doc/book/ 这个第一手权威资料；但是英文的，没办法；一手资料一般都是英文的；如果英文不好，可以Chrome自动翻译，若嫌翻译效果差；或找到中文社区的翻译版，毕竟国内大神不是一般的多。 总结以及插件在jenkins中地位 jenkins工作原理：通过向节点机（或本机）注入agent后，然后通过ssh调用节点机，利用安装的插件执行各种操作及命令，如编译，打包存档等。 再加上各种执行限定条件，如构建时间，构建条件；脚本命令等。 可以说插件是jenkins的灵魂了，如果只安装jenkins没有插件，可能什么也干不了；怎么从上千种插件中快速找到真正所需的插件，这就是积累的过程。这里总结两点，1，所有插件（不管是安装的还是未安装的）点击后，都可以跳转到说明和示例页面；2，要掌握搜索技巧，比如搜索一个copy 存档用的插件，就搜copy；在结果页面，快速浏览一遍后，就基本能锁定大概范围，这个时候逐个点击查看示例页面。 常用编译插件：安卓编译工具gradle，xcode可以从Mac机上调用； "},"jenkins/thinBackup_jenkins.html":{"url":"jenkins/thinBackup_jenkins.html","title":"jenkins config and job的备份","keywords":"","body":"Jenkins 配置和job的备份 关于jenkins中的备份可以借助插件：thinBackup来实现； 备份插件一般有 Backup+Plugin PeriodicBackup+Plugin thinBackup 备份 如果只备份配置和job， 选用thinBackup(也是最可靠的)，安装插件thinBackup后 就可以设定备份 我们再通过脚本 把备份同步到其他机器 #!/bin/bash rsync -avz /opt/jenkins-bak-file root@ip:/opt 然后把这个脚本 加到定时任务中 0 3 * /bin/bash /opt/backup-jenkins-config.sh &>/dev/null 还原备份 把备份相关目录填写好 然后选择要还原的备份，选择全量包的哪一个 ，如果第一个备份是全量包，剩下的都是增量包，先还原全量包，还原成功后再进行增量备份还原，也可以在备份时，设置每天都全量备份，这样便于还原，一定要重启Jenkins，这样就可以看到还原的后的效果了.重启Jenkins若没有生效，在插件里面新安装任意插件或者更新任意插件，勾选上\"Restart Jenkins when installation is complete and no jobs are running\" 采用这种方式来重启Jenkins，这样就能解决还原不生效的问题。一般备份包(文件夹）都有严格的格式： FULL-2019-07-12_15-53 DIFF-2019-07-12_15-53 "},"other/devops_practices_gitbook_web.html":{"url":"other/devops_practices_gitbook_web.html","title":"利用Jenkins+gitbook+git+node-ejs搭建一套实时更新多版本文档网站","keywords":"","body":"利用gitbook+git+jenkins+nodejs搭建起一套多版本说明的文档网站 现有这么一场景，产品产出了大量的文档，这些文档要对外展示，且产品每个版本的文档内容不一样，如何实现这么复杂的需求呢。 首先拆解需求，然后再分步实现。 1).文档编写采用统一标准，都用Markdown格式的，最后可以用gitbook自动生成静态网页； 2).分配好各阶段（版本）相关责任人编写的文档，待文档编写完成后统一上传到git； 3).jenkins负责拉取正式版本（release开头）的文档，用gitbook编译后，同步到云端文档服务器 4).云端服务器制作一个入口页面，页面里面可以指引各个版本文档；http服务用nginx，指引到这个页面； 步骤有了，那就动手开始执行了： 前两步具体细节这里不再做描述；先用gitbook init初始化生成readme.md和summary.md后，完善summary目录后上传到仓库，其他同事按照summary归类以及分配文档任务编写即可。任务制定好了，一定按计划执行,这个是关键。 这里重点要说的是第三步，由于第三步比较复杂，这里会拆成多个小步骤来说明；至于第四步，用nginx增加一个静态网页入口，可以参考 NGINX指向静态网页. jenkins自动获取有改动的正式版本，编译后同步到云端服务器 首先确认哪些版本能自动触发构建，若自动触发构建的是新分支(版本)，云端服务器没有此版本，如何在页面中创建此版本并把静态html文档同步过去。 1，把云端服务器目录通过ftp挂载到本地 这一步也可以不做，直接通过rsync同步到云端就行了；但我这个环境是ftp服务站，并不是真正的服务器。 # Create local mount path mkdir -p /mnt/myftp # Mount the destination ftp site using curlftpfs curlftpfs -o allow_other ftp://myusername:mypassword@ftp.mydomain.com /mnt/myftp 2，Jenkins匹配合适的分支自动构建 源码管理中，只拉取release开头分支（避免拉取到临时分支，个人分支） 采用合理的自动触发构建 触发器选择“Build when a change is pushed to GitLab” 具体配置步骤，参考jenkins的自由风格构建 这里触发构建版本，要用正则表达式筛选一下。 构建、同步以及用nodejs在入口页面中增加新版本模块 获取拉取下来的（release开头）分支名称后通过截取字段手段，保留下版本号；编译后同步过去，如果是新增的版本，创建这个版本目录并在页面中增加这个模块； shell 代码处理的功能 execute shell 内容： echo \"获取版本号\" NEW_BRANCH=$( for branch in `git branch -r | grep -v HEAD`;do echo -e `git show --format=\"%ci %cr\" $branch | head -n 1` \\\\t$branch; done | grep \"release\" | sort -r | head -1 | awk '{print $NF}' |awk -F/ '{print $2}' ) echo \"根据版本号 进行同步\" if [[ \"$NEW_BRANCH\" =~ ^release ]];then BRANCH_SHORT=${NEW_BRANCH#release-} echo \"编译文档，生成HTML\" GITBOOK_PID=$(ps -ef | grep gitbook | grep -v 'grep' | awk '{print $2}') if [[ -n $GITBOOK_PID ]];then kill $GITBOOK_PID fi gitbook install install_status=$(ps -ef | grep \"gitbook install\" | grep -v 'grep' | wc -l) while [[ \"$install_status\" -ne 0 ]] do sleep 1s done BUILD_ID=dontKillMe nohup gitbook serve &>/dev/null & sleep 25s; # 如果本地不需要gitbook server 只需编译出静态网页的话，\"编译文档，生成HTML\"这一块代码直接用`gitbook install` 和`gitbook build` 两行命令替换即可。 echo \"同步到外网ftp服务器\" if [[ ! -d /mnt/myftp/htdocs/doc/app/$BRANCH_SHORT ]];then mkdir -pv /mnt/myftp/htdocs/doc/app/$BRANCH_SHORT cp /mnt/myftp/htdocs/doc/app/index.html /home/backup-ftp/index_$(date +%F-%H-%M-%S ).html # 备份引导页面 cd /root/test-auto-html node index.js /bin/cp -f index.html /mnt/myftp/htdocs/doc/app/index.html # 用nodejs 生成新的引导页面，新页面里面增加新版本模块 fi cd $WORKSPACE rsync -avz --temp-dir=/tmp \\_book/* /mnt/myftp/htdocs/doc/app/$BRANCH_SHORT/ # 把静态页面同步过去 else echo \"not release version\" exit 1 fi 代码中有几处要重点说明的 BUILD_ID=dontKillMe nohup gitbook serve &>/dev/null & 之所以这么写是 jenkins任务执行完成后，会杀掉所有开启的进程，添加这个参数 是为了保留gitbook serve这个进程； rsync --temp-dir=/tmp 搞一个临时目录，是因为有的ftp同步对文件大小有限制，这么做，可以起到缓冲作用。 如果不是通过ftp挂载到本地，可以通过ssh user@server_ip 'test -d /your_path' 返回值是否为0，判断分支的存在（ssh前可以设为公钥登录） nodejs 代码在引导页面增加新模块（可以求助前端开发的帮助） html模板(body里面的代码片段)，命名为index.ejs 管理平台新增和优化流程分析，流程统计功能。xxxxxxxxxxxxx提供流程故障的快速排查能力。 \" class=\"btn bg-link\">阅读文档 node-ejs代码（命名为index.js，如果是其他名称.js也可以，但执行命令为 node xx.js)： let fs = require(\"fs\"); let ejs = require(\"ejs\"); const path = require('path'); let p = path.resolve('E:/node/doc'); let tmpl = fs.readFileSync(\"./index.ejs\", \"utf8\"); //console.log('tmpl: ', tmpl); let data = { versions: [] }; fs.readdirSync(p).forEach(ele => { let info = fs.statSync(path.resolve(p, ele)); if (info.isDirectory()) { data.versions.push(ele); } }); let template = ejs.compile(tmpl, {}); let result = template(data); fs.writeFileSync('./index.html', result); //console.log(result); 最终效果就是本文第一幅插图；如果有新增版本，就会在引导页面增加一个版本引导模块。 index.js把index.ejs读取后，作为模板进行编辑和填充然后生成了一个新的html文件，替换之前的html页面 从引导页，点击任意模块中的按钮都可以进入对应版本文档展示界面： 像本人博客一样（都是gitbook 生成的）；但不限制于此，php jsp aspx 等等网页 都可以按照这个思路来实现，具体页面编码可以寻求前端开发的帮助。 "},"other/devops_practices_k8s.html":{"url":"other/devops_practices_k8s.html","title":"采用jenkins+harbor+kubernetes自动部署最新程序","keywords":"","body":"利用jenkins+harbor+k8s搭建起一套实时更新测试环境 场景描述 有这么一场景，Java被测应用和Java探针一起制作成镜像后部署到k8s环境，当有新版探针发布时，要基于新版探针制作一个新的镜像，并在k8s集群中更换镜像。 分析需求后，决定用两个jenkins project来完成这个任务。 第一个project，把原来编译打包探针project中增加触发下游项目和传递探针版本参数功能。 第二个project，由第一个project触发；1，copy第一个脚本编译出来的探针文件，然后利用docker插件把最新探针文件连同被测应用镜像合成出 我们需要的镜像；2，推送到harbor仓库；3，最后在k8s集群更新镜像。 实现步骤 project1的实现 插件准备：\"Environment Injector \" \"Copy Artifact\" \"Parameterized Trigger \" 一些基础的插件这里不再说了； 原有project功能说明 ​ 我们探针源码有多个分支，有开发用的dev、测试用的test，以及正式版本uat-x.x.x(x为数字)；关于正式版呢，在测试通过后，会基于test分支创建出一个uat-x.x.x的正式版。这个时候人为构建project时，输入正式分支版本号，即可下拉这个版本源码，进行编译、和打包存档操作。 project 部分截图 拉取指定分支源码 编译和存档，编译在execute shell模块实现编译，并对版本参数处理，去掉uat-,只保留数值。 增加的两步 1，在execute shell中 把截取后版本号保存到一个文件中 echo \"SER_NO=${SER_NO}\" > server_release_number.txt 然后注入到环境变量 2，触发下游项目并传递参数 增加构建后操作\"trigger parameterized build on other projects\",里面增加\"Predefined parameters\" 构建工程2的实现 额外所需插件 Docker，建议提前把所有用到插件罗列好，然后再进行安装，这样安装完成后重启jenkins一次就行了。 docker配置： 1，首先把用来制作镜像的节点机 增加调用的接口2375； 具体配置可以参考我的另一篇博客jenkins调用docker编译程序 2，jenkins配置cloud中增加docker 3，也增加构建参数，便于再没有接收到来自project1传递过来的参数时，可以手动输入 4，copy上游构建编译出来的探针 这里先不指定copy目标目录 5，增加copy探针和判断这个版本镜像是否已经存在（很可能出现2.3.5版本有重大问题，开发向2.3.5版本合入修改bug代码） execute shell AGENT_DIR=\"/dockerfile_work_dir/tomcat-no-app\" rm -f $AGENT_DIR/*.tar.gz cp -f $WORKSPACE/*.tar.gz $AGENT_DIR rm -f ${AGENT_DIR}/test-toyota.yaml IMAGES_NU=\"$(docker image ls 172.16.35.31:1180/apm-images/tomcat-agent:${SER_NO} | wc -l)\" echo $IMAGES_NU if [[ $IMAGES_NU -ge 2 ]];then kubectl get deployment/test-toyota-demo -n test-shop --export -o yaml > ${AGENT_DIR}/test-toyota.yaml kubectl get deployment/test-toyota-demo -n test-shop --export -o yaml > ${AGENT_DIR}/test-toyota-${date +%F%H%M%S}.bak kubectl delete deployment test-toyota-demo -n test-shop docker rmi 172.16.35.31:1180/apm-images/tomcat-agent:${SER_NO} fi 删除制作镜像路径下的探针压缩包，把最新的copy过去；然后判断是否已有这个tag的镜像，有就删除应用后，再删除镜像（如果不删除deploy 镜像被占用无法删除）；这个场景重点测试的是探针，所以不用关心应用的高可用、滚动更新、灰度发布等问题；如果重点是应用的测试，一定要保证应用的可用性；这一步的execute shell ，就不需要了，直接进入下一步。 6，镜像生成和推送 填写好dockerfile的目录，cloud选择之前配置的就行，image名称，仓库证书（里面有仓库用户名和密码）就可以把生成镜像推送到仓库了； 7，最关键的一步更换k8s集群中应用的镜像版本； execute shell 代码： AGENT_DIR=\"/dockerfile_work_dir/tomcat-no-app\" IMAGE_NAME=\"172.16.35.31:1180/apm-images/tomcat-agent:${SER_NO}\" if [[ ! -e ${AGENT_DIR}/test-toyota.yaml ]];then kubectl get deployment/test-toyota-demo -n test-shop --export -o yaml > ${AGENT_DIR}/test-toyota.yaml kubectl get deployment/test-toyota-demo -n test-shop --export -o yaml > ${AGENT_DIR}/test-toyota-${date +%F%H%M%S}.bak fi kubectl delete deployment test-toyota-demo -n test-shop sed -ri \"s#image: .*#image: ${IMAGE_NAME}#\" $AGENT_DIR/test-toyota.yaml kubectl apply -f $AGENT_DIR/test-toyota.yaml 要测试最新探针，首先判断一下是否有yaml文件，这里强制性把之前被测应用deploy给删除了，如果换成应用的测试，这里脚本可以稍作调整 除了第一个execute shell 不需要外，此地execute shell 可以调整为： AGENT_DIR=\"/dockerfile_work_dir/tomcat-no-app\" IMAGE_NAME=\"172.16.35.31:1180/apm-images/tomcat-agent:${SER_NO}\" kubectl get deployment/test-toyota-demo -n test-shop --export -o yaml > ${AGENT_DIR}/test-toyota-${date +%F%H%M%S}.bak kubectl set image deployment/test-toyota-demo test-toyota-demo=${IMAGE_NAME} -n test-shop 就可以了，在deploy部署应用时设定了滚动更新策略如金丝雀、先更新后停止pod等。这样应用就达到平滑过渡。 强调一点，部署应用时，一定要设置镜像拉取规则为总是拉取。 "},"shell/get_dir_in_shell.html":{"url":"shell/get_dir_in_shell.html","title":"shell 脚本中获取脚本所在路径","keywords":"","body":"shell脚本中获取路径两种方法 第一种 DIR=$(cd $(dirname $0) && pwd ) echo $DIR 第二种 DIR2=$(cd $(dirname \"${BASH_SOURCE[0]}\") && pwd ) echo $DIR2 上面两种方法都可以获取到当前路径，但第二种方法只适用于含有bash命令的系统，若系统只有sh命令，建议采用第一种方式 "},"shell/sed_use_hard.html":{"url":"shell/sed_use_hard.html","title":"sed不常见用法","keywords":"","body":"1 sed -i '/ADMIN URL/s/localhost/172.17.0.1/; /DATA URL/s/localhost/172.17.0.1/; s/localhost:8082/172.17.0.1:8082/; N;/16379/s/localhost/172.17.0.1/;P;D;' $CONFIG_DIR/fusion_config/production.js sed 可以一次性执行多行代码，每行代码用分号隔开；上面代码块，前两行表示定位后进行替换操作，第三行代码匹配后就替换；第四行代码是模式空间指针到16379后（模式空间有两行），把模式空间里面的localhost替换掉。大写PD 表示打印模式空间第一行和删除第一行； 2 sed -i '/^\\s$/d' test.txt 删除空白行 sed -i 's/\\s//' test.txt 表示删除每行开头的空格 sed -i 's/\\s//g' test.txt 表示删除每行所有的空格 sed -i 's/\\s//;s/\\s$//g' test.txt 删除行首和行尾的空格 也可以拆开写 把删除行尾的空格单独为：sed -i 's/\\s$//g' 正则表达式\\s表示任意空白字符 不管是tab 还是空格 sed 模式匹配的g参数： g是起到一个全局的作用，这个范围是每一行，也就是说是一行为单位，作为一个全局。+g :匹配每一行有行首到行尾的所有字符 所以慎用参数g 3 指定行修改，如第4行末尾追加一行，内容为test sed -i 'N;4atest' test.txt 第4行行首追加一行，内容为test sed -i 'N;4itest' test.txt "},"shell/getopt_and_getopts_use.html":{"url":"shell/getopt_and_getopts_use.html","title":"getopt与getopts用法","keywords":"","body":"getopt 与 getopts用法详解 我们经常使用脚本 后面跟参数这种用法，这个时候使用getopt/getopts再合适不过了；下面就来详细说明 getopt （系统外部用法，后来增加的）与 getopts（内部，不支持长选项 只能是单个字符的短选项）的用法 先看一段代码 #!/bin/bash set -e set -o pipefail cmd=$(basename $0) defaultHost=\"127.0.0.1\" defaultPort=\"2222\" defaultFusion=\"http://$defaultHost:3333\" defaultUser=\"myuser\" defaultPassword=\"123456\" host=$defaultHost port=$defaultPort fusion=$defaultHost user=$defaultUser password=$defaultPassword if which getopt &>/dev/null; then optExist=\"true\" fi Usage() { echo \"帮助说明\" echo \"这个脚本如何使用，如参数 a f h 后面跟什么值，或者长参数appid fusion help 后面跟什么值\" } if [[ \"${optExist}\"x = \"true\"x ]]; then ARGS=$(getopt -o \"a:f:h\" -l \"appid:,fusion:,help\" -n \"${cmd}\" -- \"$@\") eval set -- \"${ARGS}\" while true; do case \"${1}\" in -a|--appid) shift ; if [[ -n \"${1}\" ]]; then appid=\"${1}\" shift ; fi ;; -f|--fusion) shift ; if [[ -n \"${1}\" ]]; then fusion=${1} shift ; fi ;; -h|--help) Usage exit 0 ;; --) shift ; break ; ;; *) Usage exit 0 ;; esac done else while getopts a:f:h opt; do case $opt in a) appid=$OPTARG ;; f) fusion=$OPTARG ;; h) Usage exit 0 ;; ?) Usage exit 1 ;; esac done fi if ! [[ $appid =~ [0-9a-fA-F]{24} ]]; then echo -e \"\\033[1;41;37mInvalid appid!\\033[0m\\n\" Usage exit 1 fi if ! [[ $fusion =~ ^http:// ]]; then fusion=\"http://${fusion}:3000\" fi echo -e \"\\033[32mSending request...\\033[0m\" curl -H \"Content-Type:application/json\" -X PUT --data \"{\\\"app_id\\\": \\\"$appid\\\"}\" \"$fusion/api/v1/app/indexes\" echo -e \"\\033[32mFinished!\\033[0m\" exit 0 代码详解： 先看getopt 与getopts 帮助文档 #[root@gitbook ~]# getopt --help #Usage: # getopt optstring parameters # getopt [options] [--] optstring parameters # getopt [options] -o|--options optstring [options] [--] parameters #Options: # -a, --alternative Allow long options starting with single - # -h, --help This small usage guide # -l, --longoptions Long options to be recognized # -n, --name The name under which errors are reported # -o, --options Short options to be recognized # -q, --quiet Disable error reporting by getopt(3) # -Q, --quiet-output No normal output # -s, --shell Set shell quoting conventions # -T, --test Test for getopt(1) version # -u, --unquoted Do not quote the output # -V, --version Output version information …… #[root@gitbook ~]# getopts # getopts: usage: getopts optstring name [arg] getopts a:f:h opt 表示a f h 必须取值，使用选项取值时，必须使用变量OPTARG保存该值 basename $0值显示当前脚本或命令的名字 shift 命令每执行一次，变量的个数($#)减一，而变量值提前一位 比如shift 3表示原来的$4现在变成$1，原来的$5现在变成$2等等，原来的$1、$2、$3丢弃，$0不移动。不带参数的shift命令相当于shift 1 $#: 参数的个数 $*: 参数列表 所有的参数 作为一个整体 如 ./xx.sh 1 2 3, $*值为“1 2 3” $@: 参数列表 所有参数 逐个输出 如 ./xx.sh 1 2 3 $@ 值为\"1\" \"2\" \"3\" set -- \"${ARGS}\" 相当于重置了 执行脚本 后跟参数，把执行脚本后跟参数‘$@’ 变成了 ${ARGS} 代码大意 先判断是否有getopt命令，如果有就用这个命令 没有的话 采用getopts命令，这个命令简单，不再做特殊说明； getopt 判断体中 首先判断 第一个参数如果是 -a|--appid，则shift 后移一位 也是就是-a 参数后面的值（如果有的话）保存起来； shift 执行一次，然后$1(${1}写法也可以)就变成第三个参数了， 如果 -f|--fusion是第一个参数，操作效果和上面一样；如果第一个参数是 -h|--help 的话调用Usage方法并执行exit 0 退出； 如果第一个参数是 -- 则shift 一下，break 跳出整个循环，（continue一般是 跳出当前循环，然后开始新的循环）； 如果第一个参数是为其他(?通配其他内容)调用Usage方法并执行exit 1退出 其他代码较简单 不做讲解说明 "},"shell/ps3_use.html":{"url":"shell/ps3_use.html","title":"选择项用法","keywords":"","body":"界面选择项用法 PS3=\"Enter option: \" select option in \"Install All\" \"Install Elasticsearch\" \"Install Mongo\" \"Install Nginx\" \"Install Kafka\" \"Install Redis\" \\ \"Install Druid\" \"Reboot System\" \"Exit Program\" do case $option in \"Exit Program\") break ;; \"Install All\") install_all ;; \"Install Elasticsearch\") install_es ;; \"Install Mongo\") install_mongodb ;; \"Install Nginx\") install_nginx_only ;; \"Install Redis\") install_redis_only ;; \"Install Kafka\") install_kafka_only ;; \"Install Druid\") install_druid ;; \"Reboot System\") reboot_system ;; *) echo \"Sorry. Wrong selection\" esac done 最终形成界面多选项，选择某一项的序号，就可以执行此选项脚本代码 "},"shell/trim_string.html":{"url":"shell/trim_string.html","title":"字符串截取","keywords":"","body":"如何去掉行首行尾的空格 通过sed替换方法去掉行首或行尾的空格 $ echo -e \"Hello Word \" | sed 's#\\s*##;s#\\s*$##' Hello Word #可以与/互换，避免混淆这里统一用#来表示分隔符 \\s匹配任何空白字符，包括空格、制表符、换页符等等,等价于[ \\f\\n\\r\\t\\v] \\S匹配任何非空白字符。等价于 [^ \\f\\n\\r\\t\\v]。 其他方法 在GitHub浏览代码时，发现https://github.com/dylanaraps/pure-bash-bible一个特别好的bash代码应用实例，上面截取字符串方法如下： 去掉行首和行尾的空格 trim_string() { # Usage: trim_string \" example string \" : \"${1#\"${1%%[![:space:]]*}\"}\" : \"${_%\"${_##*[![:space:]]}\"}\" printf '%s\\n' \"$_\" } 结果： $ trim_string \" Hello, World \" Hello, World 代码写的很简洁，注释太少；这里做一下注解 ，方便自己理解和以后使用，可以给大家提供一个参考 首先看方法体格式 method(){ : 参数1 : 参数2 } 它里面有注解：The : built-in is used in place of a temporary variable. 用内置函数冒号替代临时变量 ；和${parameter:word} 这种用法类似，把处理后得到的值存储到冒号中了； 这种格式方法，值得借鉴。 \"$_\" 是保存之前执行命令 最后一个参数 字符串截取可以参考 https://www.cnblogs.com/xwdreamer/p/3823463.html 这是假设一个字符串为： \"两个空格Hello空格Word三个空格\"，开头的两个空格命名为A；结尾三个空格命名为B；即“AHello WordB” 字符串截取知识掌握后，再来逐步分析${1#\"${1%%[![:space:]]*}\"} 作用 ，这种层层取值变量，都是先计算最内部的值（如果是=赋值都是先算右边），然后再算外层的；${1%%[![:space:]]*}，${1}是变量，%%[![:space:]]* 这个是从右往左进行截取，直到截取值为最后几个空格（把最后一个非空格都截掉了就剩开头两个空格即A了） ；然后${1#A}，再次字符串截取，相当于只截取掉A（开头两个空格），得到“Hello WordB” ${_%\"${_##*[![:space:]]}\"},${}表示存储之前操作后得到的变量；“${\\##*[![:space:]]}” 从左往右截取，所有非空格都截取掉，只保留最后几个空格即B（结尾的三个空格），然后变为“${_%B}”这个就是把结尾处B（三个空格）截取掉。 发散与扩展 [![:space:]]能不能像sed中用到的正则换成\\S，尝试了一次，没有成功，然后又替换成[^ \\f\\n\\r\\t\\v] 这种非简写的格式，最后也可以实现这个效果；printf 格式输出换成echo也可以 。 trim_stringw() { # Usage: trim_string2 \" example string \" : \"${1#\"${1%%[^ \\f\\n\\r\\t\\v]*}\"}\" : \"${_%\"${_##*[^ \\f\\n\\r\\t\\v]}\"}\" echo \"$_\" } 但是如果换成（无法实现效果）： trim_stringb() { # Usage: trim_string \" example string \" : \"${1#[[:space:]]*}\" : \"${_%*[[:space:]]}\" printf '%s\\n' \"$_\" } 就实现不了这个效果； 去掉所有多余空格，中间的空格只保留一个空格 trim_all() { # Usage: trim_all \" example string \" set -f set -- $* printf '%s\\n' \"$*\" set +f } 这个方法比较好理解，通过set -- $* 去掉了多余的空格，每个参数间只保留一个空格，set -f /+f 取消/增加通配符使用后，方便格式化；所以上面方法可以写成： trim_all() { set -- \"$@\" #或set -- \"$*\" echo \"$@\" #或echo \"$*\" } 如果用$@的话，用引号包起来，表示一个字符串整体(不带引号很容易出错，此时它表示一个数组)，$*用不用引号包起来都表示一个字符串。 最后说一句 GitHub非常好用； https://github.com/jlevy/the-art-of-command-line 这个命令行的艺术也非常好 ​ "},"jmeter/use_jmeter_test_app.html":{"url":"jmeter/use_jmeter_test_app.html","title":"利用jmeter模拟手机接口测试","keywords":"","body":"利用jmeter模拟手机接口测试 本文示例是从网上找到的月光茶人APP程序 首先手机操作月光茶人app一个完整的购买支付流程 我们在监听平台中查看其产生的url（接口），下列列表为手机操作支付流程时，监听平台采集到数据 现实测试的APP，我们可以通过开发提供的api文档、抓包工具如fiddler，抓取app的访问请求，都可以获取到接口URL；如何获取具体接口需要灵活应变；通过浏览器访问的程序可以直接通过Chrome调试network就能获取到接口URL. 上面列表是手机操作月光茶人APP：登录、首页列表、产品列表、加入购物车、成功加入到购物车、加入预购订单、预购订单详情、选择支付、订单提交成功产生的URL接口； 这9步构成一个完整的流程；我们把这9步的http请求加入到jmeter里面 通过监控平台采集到URL进行分析，发现其他步骤会用到登录后产生的返回体里面appCartCookieId和appLoginToken动态参数，所以我们要在登录请求后面加入正则表达式提取器 来提取，它返回的参数 .\"appCartCookieId\":\"(.+?)\". 这个正则表达式 要提取appCartCookieId：后面\"\"里面包含的内容 $1$表示 当有多个正则表达式时，只获取第一个，匹配数字1，表示从第一个开始；匹配数字，-1表示取出所有匹配值 0是随机，1 、2 表示匹配第几个 ​ 如果有多个值和appCartCookieId匹配，一定要用$1$这种形式来选择值，若有极端情况，有多个匹配值且位置不定 如：“address\":{\"area\":{\"store_id\":\"1\",\"shippingGroup\":\"\",\"pathNames\":\"中国/广东省/深圳市/宝安区/福永/福围-下沙南\",\"name\":\"福围-下沙南\",\"id\":\"1000000\",\"pathNames4Print\":\"深圳市宝安区福永福围-下沙南\"},\"isDefault\":\"1\",\"telephone\":\"18812341234\",\"id\":\"100347013e14430696ec765ff464429c\" 取\"18812341234后面的id，可以写成\"18812341234\"\\,\"id\":\"(.+?)\".* ​ 如果手机号是变动的，可以写成\"1[3|4|5|8][0-9]\\d{4,8}\"\\,\"id\":\"(.+?)\".* 手机号正则表达式不能写成：^1[3|4|5|8][0-9]\\d{4,8}$，^和$表示行开始和结束，要去掉，这里手机号并不是独占一行。 接下的步骤就可以引用这两个参数，如下图可以写成parameters里面引用参数，也可以直接在body data里面编写多个参数，多个参数用&来连接 如果想在路径里面使用上一个请求产生的参数，body data或者parameters必须带上这个参数，哪怕请求body体用不上这个参数 最后添加用于查看结果的“查看结果树”和“聚合报告”，在“查看结果树”里面可以详细看到响应的数据、请求数据、取样结果等信息 聚合报告汇总了接口访问总量错误信息等关键指标 也可以把抓包获取的header添加到jmeter里面（模拟的更真实一些，表头一般是存储设备等信息的） 这样发送过来的请求,监控平台上设备就显示为iphone了 模拟登录月光茶人APP后选购支付流程大量并发的实现 如果APP对登录有限制，同一账号只能同时登录一次，且手里没有多余的账号如何进行并发测试呢，这个时候只需单独对登录http请求进行控制即可；其他请求操作可以放在一块进行并发测试； 新建一个setUp Thread Group ​ 使用这个进程组的好处时，他可以和tearDown Thread Group一起使用，构成一个 登录+中间各种操作/请求+退出的流程（单独使用setUp、tearDown也可以），登录请求放在setUp Thread Group,退出请求放在tearDown Thread Group里面，剩下的各种操作http请求放在线程组里面，我们此处没有用到退出操作就不需要新建tearDown Thread Group线程组了； 如下图，在setUp Thread Group里面添加登录http请求后，我们需要获取appCartCookieId和 appLoginToken参数并且要全局化，下面其他进程中的http请求能继续使用；首先用正则表达式提取器提取相关参数，具体操作步骤前面有说过，不再赘述 “(.+?)”.* , (.+?)表示惰性匹配，表示从“开始，然后匹配到” 然后存起来；用\\1 \\2 或者$1 $2 取出第一个 第二个字符； 使用全局变量 添加后置处理器BeanShell PostProcessor，把上一步正则表达式提取器提取参数全局化；如下图 parameters参数填写正则表达式提取器提取的参数，然后在script模块进行全局化申明： String appCartCookieId = bsh.args[0]; ​ print (appCartCookieId); ​ ${__setProperty(newappCartCookieId,${appCartCookieId},)} 引用全局化参数 在其他进程组里面，进行引用全局化参数，引用格式：${__P(newappCartCookieId,)} 上图除了全局变量外，还引用了其他参数： _terminal-type=ios&appCartCookieId=${P(newappCartCookieId,)}&appLoginToken=${P(newappLoginToken,)}&userId=e19fd14f3ebf48bcbc79d09d6775ff04；也可以写成parameters的形式，详细讲解可以参考：http://www.cnblogs.com/allen-zml/p/6552535.html 可以在登录线程组里面添加http信息头管理，填写设备信息tid、uid等这样模拟出来的请求更接近iOS移动设备发出的请求； 控制吞吐量 确定要添加控制吞吐量的位置后，添加-定时器-Constant Throughput Timer，然后填写如图相关信息 如果想控制每秒2个并发，红色区域1填写120即可，如果Constant Throughput Timer添加到所有线程组的前面，都要用到此控制器，下拉选择all active threads选项；如果放到某一进程组，只供此进程组使用，可以选择this thread only； "},"ca/make_key.html":{"url":"ca/make_key.html","title":"私有证书制作","keywords":"","body":"制作自签名证书 1，手动制作自签名证书（NGINX用） openssl req -newkey rsa:2048 -nodes -keyout tls.key -x509 -days 3650 -out tls.pem -subj /C=CN/ST=BJ/L=CY/O=DCLINGCLOUD/OU=APM/CN=apptrace/emailAddress=ca@dclingcloud.cc 注：-keyout 和 -out 可以修改为输出路径+文件名称，名称可以自定义 字段说明： C=CN // 国家代号，中国输入CN ST=BJ // 州（省）名 L=CY // 所在地市的名称 O=DCLINGCLOUD // 组织或者公司名称 OU=APM // 部门名称 CN=apptrace // 通用名，可以是服务器域控名称，或者个人的名字 emailAddress=ca@dclingcloud.cc // 管理邮箱名 2，利用脚本制作所需证书（可以用在harbor和kubernetes、rancher部署上） 脚本下载地址（rancher中国提供） https://github.com/xiaoluhong/server-chart/blob/v2.2.4/create_self-signed-cert.sh 脚本使用示例 cat /etc/hosts 192.16.1.100 Centos76 reg.czl.com 具体名称和设置的自签名域名一致 ./create_self-signed-cert.sh --ssl-domain=reg.czl.com --ssl-trusted-ip=192.16.1.100 详细的用法可以参考脚本下载链接 "},"nginx/direct_static_web.html":{"url":"nginx/direct_static_web.html","title":"如何利用nginx指向本地静态网页","keywords":"","body":"NGINX指向静态网页 server { listen 8081; #也可以指派其他端口 server_name localhost; root /home/apptrace/tracing/dashboard; #如果NGINX非root用户运行，不要放在root目录下 autoindex on; #开启索引功能 autoindex_exact_size off; # 关闭计算文件确切大小（单位bytes），只显示大概大小（单位kb、mb、gb） autoindex_localtime on; # 显示本机时间而非 GMT 时间 location / { try_files $uri $uri/ /index.html; } "},"nginx/load_balance.html":{"url":"nginx/load_balance.html","title":"如何利用nginx实现负载均衡和反向代理","keywords":"","body":"nginx实现负载均衡 upstream ygcr { server 192.16.35.30:38080 weight=1; server 192.16.33.20:38080 weight=1; ip_hash; } 利用upstream（轮询） 可以进行负载均衡，通过weight值的大小决定权重。 ip_hash 来进行会话保持，保证同一个客户访问时 被调度到同一台服务器。 反向代理 server { listen 80; server_name localhost; location ^~ /admin { proxy_pass http://ygcr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; } location ^~ /static/ { proxy_pass http://ygcr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; } location = /product { proxy_pass http://ygcr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; } } 通过url请求访问此服务器，当端口后面是/admin、/static开头以及是/product的都转交给http://ygcr 来处理，ygcr这个在上面的负载均衡中已经定义。如访问http://ip:port/admin ，NGINX监听到这个端口后，匹配admin，然后交给http://ygcr 处理，最终访问的是 http://ygcr/admin 形成反向代理的效果。 "},"nginx/nginx_other.html":{"url":"nginx/nginx_other.html","title":"nginx其他知识","keywords":"","body":"一些常用nginx小知识汇总 nginx自动调整进程数（调整到和核数同样大小） worker_processes auto; 调整客户端最大body大小 client_max_body_size 20m; 包含其他配置（这样就更便于模块化配置） include /etc/nginx/conf.d/*.conf; 某一个访问（80或其他）端口，强制跳转到https(443)协议端口上 rewrite (.*) https://$server_name$request_uri redirect; 如果是非http协议的 负载均衡，可以直接使用stream，然后里面包含upstream（轮询）模块 stream { upstream dns { server 127.0.0.1:3002; #server otherip:3002 weight=5; 此处用来负载均衡指派 } 保留客户端真实ip 客户访问服务器经过nginx多层代理后，如何保留客户的真实ip呢 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; 第一行代码在层层代理时，保留住上级和本级代理的ip，如果只有一层代理，则保留客户端的ip和代理的ip，第二行可以保留客户端的ip。 重新加载配置项 nginx -s reload "},"git/git-use.html":{"url":"git/git-use.html","title":"git日常使用命令","keywords":"","body":"git常见用法 正常提交流程 1，首先从GitHub/gitlab服务器拉下来 git clone 仓库地址 如果使用ssh协议，还需要生成公私钥对，把公钥存储到仓库中 2，然后编辑和添加文件后 git add . （所有文件） git add filename(具体某个文件） 3，然后提交 git commit -m \"message更新信息\" 若没有新增文件，只修改文件，上面两步可以合并为 git commit -am \"message\" 4，最后上传 git push [-u origin your_branch] 把某个分支的文件copy到其他分支，如A分支文件copy到B分支 1,首先切换到B分支 git checkout branchB 2,在B分支下执行 git checkout branchA（A分支） file1 file2 … 3，剩下的步骤是提交和推送，和上面一样； 合并分支 把A分支所有的提交合并到B 首先同步A分支，使A分支本地和远程仓库保持一致 切换到B分支后 git checkout B git merge A 然后A分支的内容就merge到B上了 如果有冲突，通过 git status 查看一下冲突文件，解决冲突，git add 冲突文件后， 剩下的提交和推送操作参照上面步骤 把A 分支某（几）次提交也提交到 B分支 在 A分支下执行 git log 查找到相关提交记录 切换到B分支，git checkout B 把A的某次提交，也提交到B： git cherry-pick 7f00fe9ebb（提交号） 把A的某几次提交，也提交到B： git cherry-pick 7f00fe9ebb..7f00fe9ebb（提交范围） 如果有冲突，通过 git status 查看一下冲突文件，解决冲突，git add 冲突文件后，剩下的提交和推送操作参照上面步骤 没有冲突直接执行 git push [-u origin your_branch] 针对某次提交 打tag 1，使用git log查看提交日志，找出你需要的那个commit。假设提交的commit id git checkout 使用git tag进行打标签，例如：git tag -a v1.4 -m ‘xxxx’ git push origin --tags或者git push origin [tagname] git checkout -b newbranch git push origin newbranch git fetch 与 git pull git fetch origin master:tmp 在本地新建一个temp分支，并将远程origin仓库的master分支代码下载到本地temp分支 git push origin tmp 把tmp分支推送到远程仓库 git pull 相当于 git fetch + git merge 如 git pull tmp相当于 git fetch origin/tmp + git merge origin/tmp 把远程tmp merge到本地tmp分支中 本地仓库回退到某个版本　　 git　　reset　　--hard　　bae168 这个只是本地回滚，回滚到某个提交版本，如有两次提交，分别为A B，最后一次为B；如果回滚编码为A，相当于本地删除了后面的提交B； 如果想把本地回滚后的代码 推送到远程仓库，一定会失败；push 后面需要带上 -f这个表示强制推送的参数。 创建空白新分支 git branch git checkout git rm --cached -r . git clean -f -d 创建空的commit git commit --allow-empty -m \"[empty] initial commit\" 推送新分支 git push origin 把修改存到缓存中 git stash git stash pop(这个会把缓存拿出来，删除缓存） 或 git stash git stash list(查看stash列表） git stash apply [列表名称] .gitignore 的设置： 很多时候，有些文件不需要提交，如开发人员本地环境配置，就用到了.gitignore 只忽略dbg目录，不忽略dbg文件 dbg/ 只忽略dbg文件，不忽略dbg目录 dbg !dbg/ 若把某些目录或文件加入忽略规则，按照上述方法定义后发现并未生效，原因是.gitignore只能忽略那些原来没有被追踪的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。那么解决方法就是先把本地缓存删除（改变成未被追踪状态），然后再提交，这样就不会出现忽略的文件了。git清除本地缓存命令如下： git rm -r --cached . git add . git commit -m 'message' 最后说一个 强制推送（操作前要慎之又慎） git push -f [-u origin your_branch] 这些功能 基本能覆盖到git日常90%使用，剩下的，可以网上搜索资料，或者在git bash 中使用帮助，如 git commit --help; "},"git/install_and_bak_gitlab.html":{"url":"git/install_and_bak_gitlab.html","title":"gitlab搭建和备份","keywords":"","body":"gitlab 在线安装与备份 gitlab 安装方法说明 在线安装文档：https://mirror.tuna.tsinghua.edu.cn/help/gitlab-ce/ 编辑yum gitlab源 vim /etc/yum.repos.d/gitlab-ce.repo [gitlab-ce] name=Gitlab CE Repository baseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/ gpgcheck=0 enabled=1 注：镜像地址：https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/ 这个地址根据具体情况来，centos7 就填写el7 repo源生效 sudo yum makecache sudo yum install gitlab-ce 安装指定版本 sudo yum install gitlab-ce-8.11.5 -y 完成后， 放开 80 8080端口，或者指定的端口。 修改默认配置 安装完成后 要先执行gitlab-ctl reconfigure 再来修改配置文件 vim /etc/gitlab/gitlab.rb 修改备份文件路径 gitlab_rails['backup_path'] = '/mnt/backups' 设置只保存最近7天的备份 gitlab_rails['backup_keep_time'] = 604800 修改gitlab 存储路径 git_data_dirs({\"default\" => \"/home/git-data\"}) 修改访问url external_url 'http://172.18.35.66' 或者使用域名 再次执行gitlab-ctl reconfigure，使配置生效 恢复与备份 恢复gitlab gitlab-rake gitlab:backup:restore BACKUP=备份文件编号 备份gitlab gitlab-rake gitlab:backup:create CRON=1 定时任务脚本 #!/bin/bash /usr/bin/gitlab-rake gitlab:backup:create CRON=1 sleep 10s rsync -e \"ssh -p 22\" -avz /home/gitlab-backups 远端服务器IP:/gitlab-backups --delete 注：不使用 -e \"ssh -p 22\"这个 参数也可以同步，但本人远程服务器只有22端口是放开的 定时任务 （crontab -e后编辑) 0 2 * * 2-6 /path/xxx.sh &>/dev/null "},"autotech/monkey_android.html":{"url":"autotech/monkey_android.html","title":"monkey测试","keywords":"","body":"安卓设备monkey测试 1 adb shell 2 logcat -v threadtime -n 1000 -r 512000 -f /mnt/sdcard/logcat & ​ 这个命令限制每个logcat文件大小为500MB以内，便于查看；保存位置是手机存储中 3 monkey -p com.xxx.auto -s 1000 --ignore-crashes --ignore-timeouts --ignore-security-exceptions --pct-trackball 0 --pct-nav 0 --pct-majornav 0 --pct-anyevent 0 -v -v -v --throttle 500 1200000000 > /mnt/sdcard/monkey.log 2>&1 & ​ com.xxx.auto为APP的包名，这个可以通过开启手机log，然后操作此app，从日志中获取到这个包名。 "},"autotech/adb_cmd.html":{"url":"autotech/adb_cmd.html","title":"adb常用命令","keywords":"","body":"adb常用命令 adb 直译是安卓调试桥，就电脑操作安卓设备用的；使用adb命令要注意如下两点： 安卓手机助手可能和自己adb占用同样端口，有冲突，这个需要手动解决； 安卓设备要调到开发模式。 Log抓取 adb logcat –v time > D:\\xxx.txt（-v为参数规定log的输出格式，-v time为按时间顺序输出log。箭头后面D:\\为输出路径，可以修改路径，xxx.txt为log名称，可按自己需求确定名称）。 如果想输出到文件的同时终端也显示相关信息，上面命令可以改进为： adb logcat -v time | tee D:\\xxx.txt 进一步改进，如果想在logcat中过滤某个关键字 可以用(命名默认在shell终端执行) adb logcat -v time | grep vehice 输出到一个指定文件中 adb logcat -v time | grep vehice | tee D:\\xxx.txt 但上面命令可能在cmd环境执行有问题，cmd对grep、tee命令支持不友好，可以用gitbash或其他bash终端来执行上面命令，或者改进命令为： adb shell \"logcat -v time | grep vehice\" 这里不太好指定输出某一个文件了。如果想在cmd里面进行字段过滤，可以把grep替换为Windows支持的find 、 findstr 、Select-String 这几个关键字进行过滤；如： adb logcat -v time | Select-String \"vehice\" Bugreport： adb bugreport >D:\\xxx.txt （箭头后解释同logcat） Trace： adb pull data/anr D:/XXX(直接把log pull出来，adb pull 目标路径 本地路径) Kernel log： adb shell cat /proc/kmsg > name.txt（此时默认存放在个人文件夹下，可以按个人需要修改存放路径，如>D:\\name.txt,抓之前系统需root ---adb root—adb remount） 查看某个app的版本号： adb shell pm dump com.neolix_self_checking | findstr “versionName” adb shell 后 ，pm dump com.neolix_self_checking | grep “versionName” 如果支持shell命令 可以合并成一个命令 如何通过命令启动APP 1，先查到active的名称 手动打开app ，通过日志查到MainActivity logcat -v time | grep intent 2,通过 am start 启动APP adb shell \"am start -n 包名/.activity.MainActivity\" MainActivity名称不是固定的，还有package_name.MainActivity 等多种命名方式。 如果电脑和安卓设备在同一局域网 ，可以无线连接 adb connect ip:port port一般默认是5555 但是用完后 一定要断开，要不然其它电脑无法无线连接了； adb disconnect 进入到 Fastboot 模式： adb reboot bootloader 录屏和截图（真正应用时 ，可以直接用自己手机拍照和录制视频，灵活应用） adb shell screenrecord /sdcard/filename.mp4 adb shell screencap -p /sdcard/sc.png 其他相关命令 adb devices-----查看设备是否连接成功，显示设备信息。 adb install ------安装apk文件 adb pull 目标路径 本地路径----获取系统中文件 adb push本地路径 目标路径---向系统中发送文件 adb shell -------进入系统的shell 模式 adb root --------获取管理员权限 adb remount---重新挂载系统分区，使系统分区重新可写，一般在adb root后使用 adb shell dumpsys package com.android.settings |grep version 查看应用版本，根据需要改变应用名称 adb shell getprop [ro.build.display.id](http://ro.build.display.id) 查看系统版本 adb wait-for-device logcat -v time 待设备连接成功后，输出log，wait-for-device一般放在adb命令后面，表示先等待设备连接，连接成功后，再执行后续命令；如adb wait-for-device root 也可以写成adb wait-for-device adb root shell终端可以输入的命令 adb shell logcat -v threadtime -n 1000 -r 512000 -f /mnt/sdcard/logcat & //每个logcat存为512M logcat -v threadtime -b events > /mnt/sdcard/events.txt >&1 & logcat -v threadtime -b radio > /mnt/sdcard/radio.txt >&1 & pm命令 pm install xxx.apk安装apk应用 pm uninstall app包名 卸载APP应用 pm list packages | grep nlix 查看包含某个关键字的包名 shell终端里面命令也可以用重定向 进行输入： adb wait-for-device shell /data/local/tmp/bootcomplete }; done echo \"Booted.\" exit EOF 用adb 命令结合bat脚本，更方便的实现一些批量化操作。 "},"autotech/uiautomator_introduce.html":{"url":"autotech/uiautomator_introduce.html","title":"uiautomator 自动化测试","keywords":"","body":"uiautomator自动化测试原理和实现过程 原理阐述 ​ 本人从事过很长一段时间的自动化测试，其中安卓的自动化主要选用的uiautomator框架，我这里阐述一下uiautomator自动化测试的原理，从整体入手、化繁为简，可以轻松理解这款框架精妙之处。就相当于有了屋子的图纸，再分步来添砖加瓦就容易的多。 ​ UIAutomator是Google开发的自动化测试工具，无需源代码，可在不同App间调度；并不需要知道程序内部的结构，通过界面来点击、返回、退出等来对程序进行测试，所以它也是黑盒测试。 ​ uiautomator，把这个单词拆开ui+auto+mator,就很容易理解了，是基于UI界面然后对某个控件操作的自动化；利用uiautomator框架获取到APP 的UI（界面）上各种元素，然后实现对此app各种操作。从自带的控件获取工具uiautomatorview就可以看出来APP控件名称和方法等。 可以看出来，每一个APP图标就是一个控件（如果这里面出现了不能识别中文，把安卓设备自带uiautomator.jar替换成最新的就行了） ​ uiautomator框架是用Java语言编写的，也是三段体的，一个完整自动化过程必须包含setup 、自动化方法、 teardown 三个部分；这就像火车一样，有车头(setup)、车身(方法）、车尾(teardown)；车身（自动化方法体）具体有几节、是载货还是拉人这个每次都由自己编写代码来定义；如果方法所在的类中没有setup、teardown，自动化执行此方法时，它会直接使用（继承的）父类的setup和teardown,如果父类都没有setup和teardown，它会执行uiautomator框架顶级类中setup和teardown；setup一般用来初始化，teardown用来复位；比如你上次自动化停留在某款APP的设置界面；再次执行新的自动化方法时，先初始化到home；这样就能保证左右滑动后能找到其他APP；teardown复位呢，就相当于用例执行完成后停留APP设置界面，执行teardown复位到此APP的主页；同样车头（setup）和车尾（teardown）也可以被改造（重构） ，定制化出来的火车能更好的适应实际生产情况。就像同一个UI（界面）就可以定义为一个特殊的火车，如设置界面中，这个界面所有控件的测试方法可以写到一个类中，共用setup和teardown；初始化（车头）都是打开应用进入我的-设置界面，并把所有开关按钮置为关闭状态；复位（复位）都是点击两次返回按钮进入到APP的初始主页中。然后再来编写某个控件测试的方法。这种三段体架构的精髓在于执行方法即可，初始化和复位无需重复编写，且自动执行，如果初始化和复位写的比较规范，相当于测试下一条用例时前置条件有了双保险； 代码展示 setup重构代码： @Override protected void setUp() throws Exception { super.setUp(); initDeviceParams(); callShell(\"rm -rf /data/anr\"); callShell(\"rm -rf /data/tombstones\"); initCaseFolder(); registerCommonWatcher(); if (!device.isScreenOn()) { device.wakeUp(); sleepSec(1); } launchHomeApp(); } 方法体代码： @CaseName(\"点击快捷菜单中的天气区域\") public void clickWeatherArea() throws UiObjectNotFoundException, RemoteException { addStep(\"进入快捷菜单页面\"); clickExpandButtonOnStatusBar(); sleepSec(2); addStep(\"发现时间区域，进入快捷菜单\"); UiObject clockBTonstatusbar= new UiObject (new UiSelector().className(\"android.widget.TextView\") .resourceId(\"com.android.systemui:id/clock\")); verify(\"没有发现账号登录按钮，进入快捷菜单失败\",clockBTonstatusbar.exists());//验证功能 addStep(\"点击天气区域\"); device.click(384, 173); sleepSec(2); addStep(\"发现天气片段，进入天气应用\"); UiObject check_weatherpage= new UiObject (new UiSelector().className(\"android.widget.LinearLayout\") .resourceId(\"com.letv.weather:id/fragment_weather\")); verify(\"没有进入天气应用\",check_weatherpage.exists()); } 一定要编写好验证的方法(verify，也可以叫其他方法名)，这点非常重要；脚本执行完成后，用例是否达到预期要用verify比对一下。 teardown代码： @Override protected void tearDown() throws Exception { boolean ANRFlag = false; boolean TombstoneFlag = false; int errorTypeCount = 0; callShell(\"/system/bin/sh /data/local/tmp/PullLog.sh \" + caseFolder); callShell(\"mv /data/tombstones \" + caseFolder); File tombstones = new File(caseFolder + File.separator + \"tombstones/tombstone_00\"); File anr = new File(caseFolder + File.separator + \"anr\"); callShell(\"mv -r /data/anr \" + caseFolder); if (anr.exists()) { ANRFlag = true; errorTypeCount = errorTypeCount + 1; UiObject anrWindows = new UiObject(new UiSelector().className(\"android.widget.TextView\").textContains(\"isn't responding\")); if (anrWindows.exists()) { send_status(STATUS_ANR, \"ANR\", \"Detected ANR when running case\"); takeScreenshot(); try { new UiObject(new UiSelector().className(\"android.widget.Button\").text(\"OK\")).click(); } catch (UiObjectNotFoundException e) { e.printStackTrace(); Log.e(logTag,\"OK button in anr window is gone. Is it recoverd?\"); } finally { } Log.i(logTag,\"ANR has been catched and Device recovery normally!!!\"); } else { takeScreenshot(); Log.i(logTag, \"Can not find ANR note!!!\"); } } if (tombstones.exists()) { TombstoneFlag = true; errorTypeCount = errorTypeCount + 1; send_status(STATUS_TOMBSTONES, \"TOMBSTONES\", \"Detected TOMBSTONES when running case\"); takeScreenshot(); } unregisterCommonWatcher(); switch (errorTypeCount) { case 1: if (ANRFlag == true) { Log.i(logTag, \"AutoZM TestCase ended: ANR Occurred\"); fail(\"ANR occurred\"); } if (TombstoneFlag == true) { Log.i(logTag, \"AutoZM TestCase ended: Tombstones Occurred\"); fail(\"TOMBSTONES occurred\"); } break; case 2: if (ANRFlag == true && TombstoneFlag == true) { Log.i(logTag, \"AutoZM TestCase ended: Tombstones and ANR Occurred\"); fail(\"TOMBSTONES and ANR occurred\"); } break; default: Log.i(logTag, \"AutoZM TestCase ended successfully\"); break; } callShell(\"logcat -v time -d -f \" + caseFolder + File.separator + \"logcat.txt\"); callShell(\"logcat -c\"); super.tearDown(); } uiautomator基础对象：UiScrollable UiObject UiSelector UiDevice等这里不做描述，网上有很多相关资料，主要是博主这几年精力基本在运维方面，不知道是否有变化。找到某个控件一般通过UISelector().[text,textContains,resourceId,classname]都行，根据实际情况来选择，比如text名称都一样，就换一个方法来选择控件；这些属性，uiautomatorview里面都可以获取到。 还有一点 就是长按，这个自带的方法，如果不好使；就改用滑动来实现，在按钮上长时间滑动达到长按的效果。 脚本执行 单个方法执行比较简单： 1）把编译出来的jar包，copy到安卓设备 adb push /home/workspace/tvCar/bin/tvCar.jar /data/local/tmp/ 2）执行方法 adb shell uiautomator runtest tvCar.jar -c com.tvCar.cases.smoke.tvTel#testHdTel（执行某一条用例方法）。 如果用例需要传参数，执行时把参数输进去 adb shell uiautomator runtest tvCar.jar -c com.tvCar.cases.smoke.tvCaruser#testLoginuser -e account 152******** -e PWD zhenglin ； 批量执行和报告 如果只能上面单条用例，手动执行，那自动化也就是个弱鸡，啥也干不了； 所以更近一步：我们可以吧所有方法list写到一个表格或者txt中，然后用shell读取后，批量执行自动化用例，把执行结果写成html，这个是重点难点；要分步实现： 批量执行自动化用例 用例列表caselist.txt，把编写好的自动化方法存储到一个txt列表中 com.tv.cases.motor.login#testOpenApps com.tv.cases.motor.login#login com.tv.cases.motor.login#changename com.tv.cases.motor.login#changename 编写批量执行脚本start.sh ，由于博客篇幅限制，这里节选最重要的一段代码 for ((i=1;i> sdcard/case.log\" …… done done caselistloop 为用例执行遍数 生成报告 这个比较复杂，本人采用的是其他团队基于（Python）Django框架编写的模板，然后来生成出html格式的报告，而且是几年前的框架了，这里就不做源码展示了；一般大公司都会有现成自动化报告框架，我们负责把自动化代码编写完成，提交到仓库，cicd会自动把这一套运行起来。 对小型公司来说，可以到GitHub上寻找开源的模板，然后稍作修改后使用； uiautomator自动化的局限性 uiautomator框架需要安卓sdk的支持，sdk包非常大，下载还需要科学上网；编写还要有java语言功底，捕获UI控件来进行各种操作。最大的一个短板：如果界面是动态的，无法正常捕获到控件，采用这个框架，什么也操作不了。生成报告要借助第三方工具或者模板。 uiautomator自动化最适用场景是冒烟测试，那些重要的测试用例，那些改动不大的界面采用此框架进行自动化测试，再合适不过了。如果界面和控件名称改动频繁，即便用uiautomator实现了自动化测试，也得不偿失（可能会累死自动化测试工程师）。 附：uiautomator学习网站https://testerhome.com/topics/node53/popular "},"data/install_mongodb.html":{"url":"data/install_mongodb.html","title":"mongodb安装","keywords":"","body":"mongodb安装 官网下载地址：https://repo.mongodb.org/yum/redhat/7/mongodb-org/3.6/x86_64/RPMS/ 可以根据情况选择具体版本，下载五个安装包，分别是org mongos server shell tools 要保持每一个安装包版本一致。 mongodb安装非常简单，网上资料非常丰富，这里简单写了一个脚本来实现安装mongo，并修改端口、加密操作、启用密码验证等； function install_mongo { sudo rpm -qa | grep mongo &>/dev/null mongo_installed=$? if [[ ${mongo_installed} -eq 0 ]];then echo \"mongod already installed\" else echo \"Installing MongoDB 3.6.8...\" set -e cd $DIR/pkg echo \"Installing MongoDB RPM package...\" sudo rpm -ivh mongodb*.rpm echo \"disable transparent hugepages...\" #可要可不要 sudo /bin/cp -rf disable-transparent-hugepages /etc/init.d/disable-transparent-hugepages sudo chmod 755 /etc/init.d/disable-transparent-hugepages sudo chkconfig --add disable-transparent-hugepages ############################################### echo \"Tuning MongoDB system parameter...\" echo \"Config mongo to listen on all interfaces and config data dir\" sudo chmod a+rw /etc/mongod.conf sudo sed -i \"s/bindIp: 127.0.0.1/bindIp: 0.0.0.0/g\" /etc/mongod.conf # 在REHL(redhat centos)系统中 mongodb 数据不能存储到根目录下 root_dir=\"/root\" if [[ $DIR =~ $root_dir* ]]; then echo \"directory in root ,create new directory \" mkdir -p /data/mongodb/{log,db} sudo chown mongod:mongod -R /data/mongodb sudo chmod 777 -R /data sudo sed -i \"s|path: /var/log/mongodb/mongod.log|path: /data/mongodb/log/mongod.log|g\" /etc/mongod.conf sudo sed -i \"s|dbPath: /var/lib/mongo|dbPath: /data/mongodb/db|g\" /etc/mongod.conf else mkdir -p $MY_DIR/mongodb/{log,db} sudo chown mongod:mongod -R $MY_DIR/mongodb sudo chmod 777 -R $MY_DIR/mongodb sudo sed -i \"s|path: /var/log/mongodb/mongod.log|path: $MY_DIR/mongodb/log/mongod.log|g\" /etc/mongod.conf sudo sed -i \"s|dbPath: /var/lib/mongo|dbPath: $MY_DIR/mongodb/db|g\" /etc/mongod.conf fi set +e echo \"Config MongoDB to start on reboot...\" sudo chmod -R 755 /usr/lib/systemd/system/mongod.service sudo systemctl enable mongod #配置mongo加密 sudo setenforce 0 sudo chmod a+x ~/ echo \"start mongodb...\" sudo systemctl start mongod echo \"config mongo security...\" mongo admin --eval \"db.setProfilingLevel(1, { slowms:\\\" 100000\\\" })\" mongo admin --eval \"db.createUser({user:\\\"admin\\\",customData:{description:\\\"superuser\\\"},pwd:\\\"MYPASSWD1\\\",roles:[{role:\\\"userAdminAnyDatabase\\\",db:\\\"admin\\\"}]})\" sleep 3s mongo admin --eval \"db.createUser({user:\\\"app\\\",pwd:\\\"MYPASSWD2\\\",roles:[\\\"root\\\"]})\" echo \"stop mongodb...\" sudo systemctl stop mongod echo \"Enable mongo security...\" sudo sed -i '/#security/asecurity:\\n authorization: enabled' /etc/mongod.conf sudo sed -i \"s/port: 27017$/port: 17017/g\" /etc/mongod.conf sed -ir '/Group=mongod/a\\Restart=always' /usr/lib/systemd/system/mongod.service mkdir -p $MY_DIR/config/mongod ln -s /etc/mongod.conf $MY_DIR/config/mongod/mongod.conf echo \"add port for mongo...\" sudo firewall-cmd --add-port=17017/tcp --permanent fi } "},"data/use_mongo3.6_deploy_shard_cluster.html":{"url":"data/use_mongo3.6_deploy_shard_cluster.html","title":"mongo分片式集群","keywords":"","body":"MongoDB 分片集群 Mongodb集群架构图 从图中可以看到有三个集群组件：mongos、config server、shard。 mongos: 数据库集群请求的入口，所有的请求都通过mongos进行协调，不需要在应用程序添加一个路由选择器，mongos自己就是一个请求分发中心，它负责把对应的数据请求请求转发到对应的shard服务器上。在生产环境通常有多mongos作为请求的入口，防止其中一个挂掉所有的mongodb请求都没有办法操作。 config server，顾名思义为配置服务器，存储所有数据库元信息（路由、分片）的配置。mongos本身没有物理存储分片服务器和数据路由信息，只是缓存在内存里，配置服务器则实际存储这些数据。mongos第一次启动或者关掉重启就会从 config server 加载配置信息，以后如果配置服务器信息变化会通知到所有的 mongos 更新自己的状态，这样 mongos 就能继续准确路由。在生产环境通常有多个 config server 配置服务器，因为它存储了分片路由的元数据，防止数据丢失！ shard，分片（sharding）是指将数据库拆分，将其分散在不同的机器上的过程。将数据分散到不同的机器上，不需要功能强大的服务器就可以存储更多的数据和处理更大的负载。基本思想就是将集合切成小块，这些块分散到若干片里，每个片只负责总数据的一部分，最后通过一个均衡器来对各个分片进行均衡（数据迁移）。 简单了解之后，我们可以这样总结一下，应用请求mongos来操作mongodb的增删改查，配置服务器存储数据库元信息，并且和mongos做同步，数据最终存入在shard（分片）上，为了防止数据丢失同步在副本集中存储了一份，仲裁在数据存储到分片的时候决定存储到哪个节点。 环境准备 系统系统 redhat 7.2 三台服务器： 192.1.167.13 mongodb001 192.1.167.14 mongodb002 192.1.167.16 mongodb003 服务器规划 服务器mongodb001 服务器mongodb002 服务器mongodb003 mongos mongos mongos config server config server config server shard0000 shard0001 shard0002 端口分配 mongos：20000 config：21000 shard：27001 开放端口 firewall-cmd --add-port=21000/tcp --permanent firewall-cmd --add-port=21000/tcp firewall-cmd --add-port=27001/tcp --permanent firewall-cmd --add-port=27001/tcp firewall-cmd --add-port=20000/tcp --permanent firewall-cmd --add-port=20000/tcp 关闭selinux vi /etc/selinux/config SELINUX=disabled 重启系统才会生效 集群搭建 安装mongodb 使用 rpm install 进行安装 在mongo安装包（3.6.8）执行下面语句： sudo rpm -ivh *.rpm featureCompatibilityVersion版本检查 在mongo 做shard集群前，最好要确认一下featureCompatibilityVersion版本，很多mongo是从较低版本升级到较高版本，比如从3.2升级到3.6后，featureCompatibilityVersion版本可能是3.4；所以确定featureCompatibilityVersion版本和mongod版本很有必要： 执行命令 db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } ) 来查看featureCompatibilityVersion当前版本 如果featureCompatibilityVersion版本和当前使用mongo不一致，还需要执行下面代码： db.adminCommand( { setFeatureCompatibilityVersion: \"3.6\" } ) 其中 setFeatureCompatibilityVersion: version参数要根据当前使用mongo版本来定； 具体执行过程如下： [root@apptrace_pak ~]# mongo MongoDB shell version v3.6.8 connecting to: mongodb://127.0.0.1:27017 MongoDB server version: 3.6.8 Server has startup warnings: 2018-10-08T13:51:28.737+0800 I CONTROL [initandlisten] 2018-10-08T13:51:28.737+0800 I CONTROL [initandlisten] ** WARNING: Access control is not enabled for the database. 2018-10-08T13:51:28.737+0800 I CONTROL [initandlisten] ** Read and write access to data and configuration is unrestricted. 2018-10-08T13:51:28.737+0800 I CONTROL [initandlisten] 2018-10-08T13:51:28.737+0800 I CONTROL [initandlisten] ** WARNING: This server is bound to localhost. 2018-10-08T13:51:28.737+0800 I CONTROL [initandlisten] ** Remote systems will be unable to connect to this server. 2018-10-08T13:51:28.737+0800 I CONTROL [initandlisten] ** Start the server with --bind_ip to specify which IP 2018-10-08T13:51:28.737+0800 I CONTROL [initandlisten] ** addresses it should serve responses from, or with --bind_ip_all to 2018-10-08T13:51:28.737+0800 I CONTROL [initandlisten] ** bind to all interfaces. If this behavior is desired, start the 2018-10-08T13:51:28.737+0800 I CONTROL [initandlisten] ** server with --bind_ip 127.0.0.1 to disable this warning. 2018-10-08T13:51:28.737+0800 I CONTROL [initandlisten] > db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } ) { \"featureCompatibilityVersion\" : { \"version\" : \"3.4\" }, \"ok\" : 1 } > db.adminCommand( { setFeatureCompatibilityVersion: \"3.6\" } ) { \"ok\" : 1 } > db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } ) { \"featureCompatibilityVersion\" : { \"version\" : \"3.6\" }, \"ok\" : 1 } > 禁用自带服务： systemctl disable mongod systemctl stop mongod 路径规划并创建 分别在每台机器建立conf、mongos、config、shard、目录，因为mongos不存储数据，只需要建立日志文件目录即可。 配置文件路径 mkdir -p /etc/mongod/conf.d pid文件路径 /var/run/mongodb 数据存储路径 config和shard server数据存储路径 mkdir -p $APPTRACE_HOME/mongodb/db/shard/data mkdir -p $APPTRACE_HOME/mongodb/db/config/data chown -R mongod:mongod $APPTRACE_HOME/mongodb 需要注意的是，如果$APPTRACE_HOME真实路径是/app/apptrace ，一定要保证当前账号对/app目录有读写执行权限；(很重要) 日志文件路径 /var/log/mongodb 2、config server配置服务器 添加配置文件 cp /etc/mongod.conf /etc/mongod/conf.d/config.conf vi /etc/mongod/conf.d/config.conf 配置文件内容: # config.conf # for documentation of all options, see: # http://docs.mongodb.org/manual/reference/configuration-options/ # where to write logging data. systemLog: destination: file logAppend: true path: /var/log/mongodb/configsvr.log # Where and how to store data. storage: dbPath: $APPTRACE_HOME/mongodb/db/config/data journal: enabled: true # engine: # mmapv1: # wiredTiger: # how the process runs processManagement: fork: true # fork and run in background pidFilePath: /var/run/mongodb/configsvr.pid # location of pidfile timeZoneInfo: /usr/share/zoneinfo # network interfaces net: port: 21000 bindIp: 0.0.0.0 # Listen on all interfaces. maxIncomingConnections: 20000 #security: #operationProfiling: #replication: replication: replSetName: csReplSet #sharding: sharding: clusterRole: configsvr ## Enterprise-Only Options #auditLog: #snmp: 创建systemctl unit文件mongod-configsvr.service cp /usr/lib/systemd/system/mongod.service /usr/lib/systemd/system/mongod-configsvr.service vi /usr/lib/systemd/system/mongod-configsvr.service [Unit] Description=Mongodb Config Server After=network.target Documentation=https://docs.mongodb.org/manual [Service] User=mongod Group=mongod Environment=\"OPTIONS=-f /etc/mongod/conf.d/config.conf\" ExecStart=/usr/bin/mongod $OPTIONS ExecStartPre=/usr/bin/mkdir -p /var/run/mongodb ExecStartPre=/usr/bin/chown mongod:mongod /var/run/mongodb ExecStartPre=/usr/bin/chmod 0755 /var/run/mongodb PermissionsStartOnly=true PIDFile=/var/run/mongodb/configsvr.pid Type=forking # file size LimitFSIZE=infinity # cpu time LimitCPU=infinity # virtual memory size LimitAS=infinity # open files LimitNOFILE=64000 # processes/threads LimitNPROC=64000 # locked memory LimitMEMLOCK=infinity # total threads (user+kernel) TasksMax=infinity TasksAccounting=false # Recommended limits for for mongod as specified in # http://docs.mongodb.org/manual/reference/ulimit/#recommended-settings [Install] WantedBy=multi-user.target 在另外两台上执行上面同样的操作，可以把上面两个文件复制到另外两个文件 scp root@ip:/etc/mongod/conf.d/config.conf /etc/mongod/conf.d/config.conf scp root@ip:/usr/lib/systemd/system/mongod-configsvr.service /usr/lib/systemd/system/mongod-configsvr.service 在安徽移动这个方法不好使，直接一台手动复制的； 启动三台服务器的config server systemctl enable mongod-configsvr systemctl start mongod-configsvr 初始化配置 登录任意一台配置服务器 #连接 mongo 127.0.0.1:21000 #config变量 config = { _id : \"csReplSet\", members : [ {_id : 1, host : \"10.243.167.13:21000\" }, {_id : 2, host : \"10.243.167.14:21000\" }, {_id : 3, host : \"10.243.167.16:21000\" } ] } 初始化 rs.initiate(config) 执行结果： > rs.initiate(config); { \"ok\" : 1, \"operationTime\" : Timestamp(1537320975, 1), \"$gleStats\" : { \"lastOpTime\" : Timestamp(1537320975, 1), \"electionId\" : ObjectId(\"000000000000000000000000\") }, \"$clusterTime\" : { \"clusterTime\" : Timestamp(1537320975, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } 其中，\"_id\" : \" csReplSet \"应与配置文件中配置的 replicaction.replSetName 一致，\"members\" 中的 \"host\" 为三个节点的 ip 和 port 3、配置分片(三台机器) 1) 设置分片 添加配置文件 cp /etc/mongod.conf /etc/mongod/conf.d/shard.conf vi /etc/mongod/conf.d/shard.conf 配置文件内容 systemLog: destination: file logAppend: true path: /var/log/mongodb/shard.log storage: dbPath: $APPTRACE_HOME/mongodb/db/shard/data journal: enabled: true processManagement: fork: true pidFilePath: /var/run/mongodb/shard.pid timeZoneInfo: /usr/share/zoneinfo net: port: 27001 bindIp: 0.0.0.0 maxIncomingConnections: 20000 #replication: # replSetName: shard sharding: clusterRole: shardsvr 创建systemctl unit文件mongod-shard.service cp /usr/lib/systemd/system/mongod.service /usr/lib/systemd/system/mongod-shard.service vi /usr/lib/systemd/system/mongod-shard.service mongod-shard.service配置文件内容 [Unit] Description=Mongodb shard Server After=network.target Documentation=https://docs.mongodb.org/manual [Service] User=mongod Group=mongod Environment=\"OPTIONS=-f /etc/mongod/conf.d/shard.conf\" ExecStart=/usr/bin/mongod $OPTIONS ExecStartPre=/usr/bin/mkdir -p /var/run/mongodb ExecStartPre=/usr/bin/chown mongod:mongod /var/run/mongodb ExecStartPre=/usr/bin/chmod 0755 /var/run/mongodb PermissionsStartOnly=true PIDFile=/var/run/mongodb/shard.pid Type=forking # file size LimitFSIZE=infinity # cpu time LimitCPU=infinity # virtual memory size LimitAS=infinity # open files LimitNOFILE=64000 # processes/threads LimitNPROC=64000 # locked memory LimitMEMLOCK=infinity # total threads (user+kernel) TasksMax=infinity TasksAccounting=false # Recommended limits for for mongod as specified in # http://docs.mongodb.org/manual/reference/ulimit/#recommended-settings [Install] WantedBy=multi-user.target 在另外两台上执行同样操作，可以把这两个文件内容粘贴到另外两台机器，如果可以使用scp copy的话，可以参考下面方式 scp root@ip:/etc/mongod/conf.d/shard.conf /etc/mongod/conf.d/shard.conf scp root@ip:/usr/lib/systemd/system/mongod-shard.service /usr/lib/systemd/system/mongod-shard.service 启动三台服务器的shard server systemctl enable mongod-shard systemctl start mongod-shard 4、配置路由服务器 mongos 先启动配置服务器和分片服务器,后启动路由实例启动路由实例:（三台机器） 添加配置文件 cp /etc/mongod.conf /etc/mongod/conf.d/mongos.conf vi /etc/mongod/conf.d/mongos.conf 修改配置文件内容 systemLog: destination: file logAppend: true path: /var/log/mongodb/mongos.log processManagement: fork: true pidFilePath: /var/run/mongodb/mongos.pid timeZoneInfo: /usr/share/zoneinfo net: port: 20000 bindIp: 0.0.0.0 maxIncomingConnections: 20000 sharding: configDB: csReplSet/10.243.167.13:21000, 10.243.167.14:21000, 10.243.167.16:21000 #注意监听的配置服务器,只能有1个或者3个 csReplSet为配置服务器的名字 创建systemctl unit文件mongod-mongos.service cp /usr/lib/systemd/system/mongod.service /usr/lib/systemd/system/mongod-mongos.service vi /usr/lib/systemd/system/mongod-mongos.service [Unit] Description=Mongodb Mongos Server After=network.target mongod-shard.service Documentation=https://docs.mongodb.org/manual [Service] User=mongod Group=mongod Environment=\"OPTIONS=-f /etc/mongod/conf.d/mongos.conf\" ExecStart=/usr/bin/mongos $OPTIONS ##注意是mongos ExecStartPre=/usr/bin/mkdir -p /var/run/mongodb ExecStartPre=/usr/bin/chown mongod:mongod /var/run/mongodb ExecStartPre=/usr/bin/chmod 0755 /var/run/mongodb PermissionsStartOnly=true PIDFile=/var/run/mongodb/mongos.pid Type=forking # file size LimitFSIZE=infinity # cpu time LimitCPU=infinity # virtual memory size LimitAS=infinity # open files LimitNOFILE=64000 # processes/threads LimitNPROC=64000 # locked memory LimitMEMLOCK=infinity # total threads (user+kernel) TasksMax=infinity TasksAccounting=false # Recommended limits for for mongod as specified in # http://docs.mongodb.org/manual/reference/ulimit/#recommended-settings [Install] WantedBy=multi-user.target 在另外两台上执行，同样操作； 启动三台服务器的mongos server systemctl enable mongod-mongos systemctl start mongod-mongos 5、启用分片 目前搭建了mongodb配置服务器、路由服务器，各个分片服务器，不过应用程序连接到mongos路由服务器并不能使用分片机制，还需要在程序里设置分片配置，让分片生效。 把集群机器中所有shard、configsvr、mongos都停掉；删除他们目录下面的数据，然后按照configsvr，shard顺序在集群中都启动起来，然后只启动任意一台mongos，登录后执行如下命令 mongo 127.0.0.1:20000 #使用admin数据库 use admin #串联路由服务器与分配 sh.addShard(\"10.243.167.13:27001\") 每添加一台shard，在其shard服务器上验证一下 登录shard mongo 127.0.0.1:27001 再执行 db.adminCommand( { shardingState: 1 }) 来验证shard添加是否成功 sh.addShard(\"10.243.167.14:27001\") 后也按照上面的验证 sh.addShard(\"10.243.167.16:27001\") 同样按照上面的验证 执行结果： mongos> use admin; switched to db admin mongos> sh.addShard(\"10.243.167.13:27001\") { \"shardAdded\" : \"shard0000\", \"ok\" : 1, \"operationTime\" : Timestamp(1538214138, 6), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1538214138, 6), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } 验证shard mongo 127.0.0.1:27001 MongoDB shell version v3.6.7 connecting to: mongodb://127.0.0.1:27001/test MongoDB server version: 3.6.7 Server has startup warnings: 2018-10-08T09:53:42.486+0800 I CONTROL [initandlisten] 2018-10-08T09:53:42.486+0800 I CONTROL [initandlisten] ** WARNING: Access control is not enabled for the database. 2018-10-08T09:53:42.486+0800 I CONTROL [initandlisten] ** Read and write access to data and configuration is unrestricted. 2018-10-08T09:53:42.486+0800 I CONTROL [initandlisten] 2018-10-08T09:53:42.486+0800 I CONTROL [initandlisten] 2018-10-08T09:53:42.486+0800 I CONTROL [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/enabled is 'always'. 2018-10-08T09:53:42.486+0800 I CONTROL [initandlisten] ** We suggest setting it to 'never' 2018-10-08T09:53:42.486+0800 I CONTROL [initandlisten] 2018-10-08T09:53:42.486+0800 I CONTROL [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/defrag is 'always'. 2018-10-08T09:53:42.486+0800 I CONTROL [initandlisten] ** We suggest setting it to 'never' 2018-10-08T09:53:42.486+0800 I CONTROL [initandlisten] shard1:PRIMARY> db.adminCommand( { shardingState: 1 }) { \"enabled\" : true, \"configServer\" : \"csReplSet/172.16.33.51:21000,172.16.33.52:21000,172.16.33.53:21000\", \"shardName\" : \"shard1\", \"clusterId\" : ObjectId(\"5ba1a81dd8bb342cc9d2f391\"), \"versions\" : { …… }, \"ok\" : 1, \"operationTime\" : Timestamp(1538994543, 1), \"$gleStats\" : { \"lastOpTime\" : Timestamp(0, 0), \"electionId\" : ObjectId(\"7fffffff0000000000000005\") }, \"$configServerState\" : { \"opTime\" : { \"ts\" : Timestamp(1538994539, 2), \"t\" : NumberLong(3) } }, \"$clusterTime\" : { \"clusterTime\" : Timestamp(1538994543, 1), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } 其他几台shard服务器执行过程类型，不再粘贴执行过程； 查看集群状态 sh.status() mongos> sh.status()sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"minCompatibleVersion\" : 5, \"currentVersion\" : 6, \"clusterId\" : ObjectId(\"5baf35bdf7644518088915af\") } shards: { \"_id\" : \"shard0000\", \"host\" : \"10.243.167.13:27001\", \"state\" : 1 } { \"_id\" : \"shard0001\", \"host\" : \"10.243.167.14:27001\", \"state\" : 1 } { \"_id\" : \"shard0002\", \"host\" : \"10.243.167.16:27001\", \"state\" : 1 } active mongoses: \"3.6.8\" : 3 autosplit: Currently enabled: yes balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations databases: { \"_id\" : \"config\", \"primary\" : \"config\", \"partitioned\" : true } 然后再把剩下的mongos启动； 6、测试 目前配置服务、路由服务、分片服务、服务都已经串联起来了，但我们的目的是希望插入数据，数据能够自动分片。连接在mongos上，准备让指定的数据库、指定的集合分片生效。 mongo 127.0.0.1:20000 show dbs use config mongos> db.settings.find() 先查看默认chunk大小 #设置分片chunk大小 use config db.settings.save({ \"_id\" : \"chunksize\", \"value\" : 1 }) 设置1M是为了测试，否则要插入大量数据才能分片。最后再改回默认大小（64） db.settings.save({ \"_id\" : \"chunksize\", \"value\" : 64 }) #指定test分片生效 sh.enableSharding(\"test\") 执行结果： { \"ok\" : 1, \"operationTime\" : Timestamp(1538214554, 9), \"$clusterTime\" : { \"clusterTime\" : Timestamp(1538214554, 9), \"signature\" : { \"hash\" : BinData(0,\"AAAAAAAAAAAAAAAAAAAAAAAAAAA=\"), \"keyId\" : NumberLong(0) } } } #指定数据库里需要分片的集合和片键 use test db.users.createIndex({user_id : 1}) use admin sh.shardCollection(\"test.users\", {user_id: 1}) 我们设置testdb的 table1 表需要分片，根据 user_id 自动分片到 shard0000 shard0001 shard0002。要这样设置是因为不是所有mongodb 的数据库和表 都需要分片！ 测试分片配置结果 mongo 127.0.0.1:20000 use test; for (var i = 1; i sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"minCompatibleVersion\" : 5, \"currentVersion\" : 6, \"clusterId\" : ObjectId(\"5ba1a81dd8bb342cc9d2f391\") } 可以看到相关数据分片 集群起停(注意顺序) 启动 mongodb的启动顺序是，先启动配置服务器，在启动分片，最后启动mongos. systemctl start mongod-configsvr systemctl start mongod-shard systemctl start mongod-mongos 关闭: systemctl stop mongod-mongos systemctl stop mongod-shard systemctl stop mongod-configsvr "},"data/install_mysql.html":{"url":"data/install_mysql.html","title":"linux中安装MySQL5.6/5.7","keywords":"","body":"linux中安装mysql5.6和5.7 安装检查 检查是否已有mariadb 数据库 rpm -qa | grep mariadb 如果检查出有mariadb安装文件，一定要卸载掉，否则无法安装 rpm -e --nodeps $mariadbname （$mariadbname代表检查出来mariadb的数据库安装包名称） 安装mysql5.6 官网下载下来的mysql5.6可能含有多个安装文件只需安装如图圈中的名称含有MySQL-server、MySQL-client的即可 1,安装 rpm -ivh MySQL-server*.rpm --nodeps （去掉依赖） rpm -ivh MySQL-client*.rpm 2，重置密码 Mysql初始化密码不知道，查找太麻烦，我们可以强制在安全模式下快速重置密码即可； 重置MySQL5.6 的密码 mysqld_safe --user=mysql --skip-grant-tables --skip-networking & 进入mysql进程后ctrl+c停止 如果有提示： ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2) 我们可以通过启停一下MySQL让它生成mysql.sock文件 /etc/init.d/mysql start /etc/init.d/mysql stop 再次进入安全模式后，输入mysql -u root mysql（一定要确保是在安全模式下，如果提示Unknown database 'mysql'，去掉后面的mysql） 然后修改密码： update user set password=password('123456') where user='root'; flush privileges; 然后杀掉MySQL所有进程 正常启动：/etc/init.d/mysql start 在正常模式下登录后mysql -u root -p12345 若登录成功后，无法对数据进行查看等操作，再输入下列命令，重置一下密码； SET PASSWORD = PASSWORD('123456'); 安装mysql5.7 1,安装mysql5.7 rpm -ivh mysql-community-common-5.7.15-1.el6.x86_64.rpm rpm -ivh mysql-community-libs-5.7.15-1.el6.x86_64.rpm rpm -ivh mysql-community-client-5.7.15-1.el6.x86_64.rpm rpm -ivh mysql-community-server-5.7.15-1.el6.x86_64.rpm MySQL5.7安装文件有4个，common、libs、client、server,根据linux系统版本不同选择不同的RPM包，如centos6 选择mysql-community-server-5.7.15-1.el6.x86_64.rpm，el6的安装包，7选择el7的安装包；且安装顺序必须按照文档排列 MySQL5.7安装成功后，会生成一个临时密码，通过如下命令，即可获得 grep \"A temporary password\" /var/log/mysqld.log 2,Mysql5.7密码重置 若无法获取临时密码，或者临时密码有特殊字符，无法正确输入(加上转义字符\\也不好使），也可以通过安全模式重置密码 mysqld_safe --user=mysql --skip-grant-tables --skip-networking & 操作步骤和上面一样，唯一区别时，5.7的密码列是authentication_string 所以要输入： update user set authentication_string=password('123abc') where user='root'; flush privileges; 来修改密码，然后重启MySQL "},"vm/use_tem_deploy_centos7.html":{"url":"vm/use_tem_deploy_centos7.html","title":"利用模板快速部署centos7服务器","keywords":"","body":"Vcenter中利用模板快速部署centos虚拟机： 1，登录vcenter https://ip:port 输入账号，密码；进入后选择HTML5格式的 2，找到centos虚机模板 找到之前已经制作好的centos7.5模板 3，创建虚拟机 选择从此模板新建虚拟机 4，命名和选择合适的位置 给新的虚机命名 这里面vcenter管理了4台esxi 系统服务器；选择要部署到那台服务器上。 5，找到新建虚机修改配置: 查看部署 进度 待完成后 要修正此虚机默认配置 6，开机固定ip hostname等 如果电脑装有VMware Workstation 直接选择 remote console 如果没有装 可以用登录这台虚机所属的esxi（vmmare服务器）如：https://esxi 打开浏览器控制台 不管是通过浏览器还是VMware Workstation 能进入到虚机就行 输入密码和账号（制作模板时设定的密码） 先ip a 查看一下 自动获取的ip 或者之前的ip 输入nmtui 进行修改ip 修改完成输入 systemctl restart network 重启网卡 检查一下 修改成功 Hostname 也可以修改一下 Hostnamectl set-hostname app002 Bash 然后就可以看到新的hostname了 7，跳板机登录服务器 在跳板机中（或ssh） 输入服务器账号密码信息后 可以正常登录，然后进行剩下的操作。 "},"middleware/install_websphere8.5.html":{"url":"middleware/install_websphere8.5.html","title":"websphere8.5安装和使用","keywords":"","body":"was8.5在线安装和示例部署 WebSphere8.5及以上版本，都是采用控制台安装 安装 解压从官网下载下来的文件后,要在有可视化界面的Linux进行操作； 执行./install 进行安装 只选择我们要安装的WebSphere 服务器，因为需要到IBM服务器下载，异常缓慢；如果我们安装时，异常缓慢或者服务器总掉线时，我们可以先停止安装，编辑install.xml文件；修改之前备份一份，如下图红色部分都可以删除，删除完成后；再执行install，这个时候只是安装了install manager并未安装WebSphere；我们再把原来的install.xml替换回来，执行./install 命令安装，勾选上我们需要安装的WebSphere服务和演示程序即可； 创建服务 安装完成后，在安装目录如：/opt/IBM/WebSphere/AppServer/bin/ProfileManagement/ 下，执行 ./wct.sh后出现下图面板，我们创建 Application server 类型服务器即可； 创建完成后，弹出first steps 窗口，我们再窗口里面点击 Installation verification，即可验证和启动程序，同时弹出日志小窗口，如果提示browser找不到，我们可以到环境变量文件/etc/profiles里面把火狐浏览器位置修正一下； 启动成功后，日志小窗口会提示访问服务器的URL结果，我们可以把里面连接粘贴到浏览器里面进行访问； 如果我们下次开启firststeps 可以到安装目录：WebSphere/AppServer/profiles/安装服务器的名称/ firststeps 里面执行./ firststeps.sh 来开启服务器 部署演示程序 安装数据库 进入演示程序目录：WebSphere/AppServer/samples/ 后解压PlantsByWebSphere文件后会出现数据库文件，pbw-db.jar；我们通过命令 ：jar -xvf pbw-db.jar后把数据copy指定目录：cp -r PLANTSDB /opt/IBM/WebSphere/AppServer/derby/databases/ 部署程序： 访问控制台： https://ip:9043/ibm/console/logon.jsp 账号：admin，密码：123456； 在控制台中新建应用，路径选择远程，如下图，不断往上回溯，找到根目录，然后再进入搭配sample存放的目录，选择 pbw-ear.ear 文件，导入即可； 导入成功后，我们启动相应的应用即可，如果上下文根过长，可以修改； 测试应用地址： http://192.168.1.34:9080/plants "},"middleware/update_jvm_value.html":{"url":"middleware/update_jvm_value.html","title":"tomcat、weblogic、was等组件jvm调整","keywords":"","body":"tomcat was weblogic jvm参数调整 tomcat linux 可以在bin目录下，新增一个setenv.sh文件，添加如下内容 JAVA_OPTS=\"$JAVA_OPTS -Xms1g -Xmx1g\" 或者 export CATALINA_OPTS=\"$CATALINA_OPTS -Xms1g -Xmx1g\" 也可以直接在catalina.sh中修改，使用JAVA_OPTS=\"$JAVA_OPTS -Xms1g -Xmx1g\" windonws set \"JAVA_OPTS=%JAVA_OPTS% -Xms1g -Xmx1g\" Windows 变量用%%包围起来，要带上set（相当于export） websphere（was） 在WebSphere服务器管理界面，依次点击应用程序服务器 > server1 > 进程定义 > Java 虚拟机目录，通用JVM参数区域内填写如下参数； weblogic 在启动脚本 里面增加 export JAVA_OPTIONS=\"$JAVA_OPTIONS -Xms1g -Xmx1g\" 或者新增一个启动脚本，里面增加上面代码，并调用启动脚本。 Windows 系统： 把export换成set 变量用%包起来就行了； "},"middleware/java-code.html":{"url":"middleware/java-code.html","title":"关于数组位置调整和元素出现次数统计代码","keywords":"","body":"调整数组，正数放在左边 负数放在右边，0在中间相对位置不变 public static int[] tzsx(int[] myarr) throws ArrayIndexOutOfBoundsException { int[] newarr = new int[myarr.length]; int zhengshu = 0; int zerocount = 0; int count = 0; for(int a:myarr){ if(a>0){ zhengshu++; //计算出正数的总个数 } if(a == 0){ zerocount++;//计算出0的总个数 } } int nofushu=zhengshu+zerocount; for(int i=0;i 0){ newarr[count] = myarr[i]; //把正数放到新数组中，索引从0开始 count++; } else if(myarr[i] == 0){ newarr[zhengshu] = myarr[i];//把0放到新数组中，索引从正数总个数开始 zhengshu++; } else{ newarr[nofushu]=myarr[i];//把负数放到新数组中，索引从(正数+0)总个数开始 nofushu++; } } return newarr; } 打印出数组中元素出现次数大于2的元素和次数 public static void countStr(String[] arr) { Map myMap=new HashMap<>(); for(String str:arr) { Integer num=myMap.get(str); myMap.put(str, num==null?1:num+1); } Iterator it=myMap.keySet().iterator(); while (it.hasNext()) { Object key =it.next(); if(myMap.get(key)>1) { System.out.println(key+\"\\t total view \"+myMap.get(key)+\" times \"); } } } 实现方式：把数组元素插入到hashmap中，元素作为key，出现次数作为value，最后遍历出来，判断一下，大于2次，就打印。 "},"middleware/prime_number_product.html":{"url":"middleware/prime_number_product.html","title":"for循环实践：生成一万内的质数","keywords":"","body":"for循环实践：生成10000以内的质数 分析：质数是只能被1和本身整除的正整数，最小质数是2，所以要对质数求余，除本身外，除以其他数都会有余数； int zs = 0; x1: for (int i = 2; i 这里为了方便调试，先以100以内的质数，用i当做被除数，j当做除数；两层循环都来搞定；j不断++，当i能被j整除就跳出外层循环，继续往下循环；但i等于2，或者i和j相等时 ；他们求余肯定也是0，要排除在外，因为质数也满足这个条件；让满足条件的数走另一个分支；把i存到zs变量这个分支，最后打印出来； 这里分别实践了for循环和循环标签； 继续提升难度，每4个换行一次，代码如何实现呢？我们增加一个计数器，当zs有4个时，就换行： int zs = 0; int count = 0; x1: for (int i = 2; i 一定要注意，这里打印是print，不是println，另外打印内容中zs在前，换行和tab符在后； 继续延伸，把这些质数存储到一个数组中怎么实现呢？ 分析：我们事先不知道数组长度，可以存储到一个能自动增长的集合中，然后再转存到数组中，就行了； private static int[] zhishu(int k) { int zs = 0; int count = 0; // int[] myarr = new int[0]; List list = new ArrayList<>(); x1: for (int i = 2; i 这样又能运用到集合的知识； 这个例子能让我们熟悉for循环的同时并学习到集合、计数器、打印等知识。 "},"middleware/compara-arry.html":{"url":"middleware/compara-arry.html","title":"比较两个数组是否相同","keywords":"","body":"比较两个数组是否相同（忽略元素位置） java语言实现 public static boolean comparaarry(int[] arr1,int[] arr2) { if(arr1.length != arr2.length) { return false; }else { Arrays.sort(arr1); Arrays.sort(arr2); int wcount = 0; for (int i=0;i方法很简单，就是先判断长度，如果长度相同，进行排序，然后逐个比较，每一个元素相同，计数器就自增一次，最后比较计数器和数组长度是否一致，如果一致，则两个数组相同。 "},"middleware/for-while.html":{"url":"middleware/for-while.html","title":"for和while异同比较","keywords":"","body":"for和while相互转换 先看段代码： for (int i = 1; i \"); //转换成while循环 int k = 1; while (k 上面代码效果一样，都是输出（乘法口诀外层）： 1 1 2 1 2 3 1 2 3 4 1 2 3 4 5 1 2 3 4 5 6 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 9 for循环的格式是：for([初始化];[判断条件];[条件改变方式]){循环体} 其中()里面除了两个引号，其他不是非必须的 while更简单就括号里面判断条件；互相转换规律时，把for循环里面初始化写到外面，while括号里面只写判断语句，条件改变方式写到while循环体中，这样就可以达到for循环的效果； int k = 1; while (k 但while循环语句更容易控制，只需验证括号里面布尔表达式是否成立就行，至于判断条件的变化，可以放到循环体里面来实现，并能灵活的加上限制条件（if),如实现 打印如下形状的数据，用while循环更合适； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 这种漏斗形式的数据，先总结规律，再来代码实现；上面形式数据特点是：每一行的元素个数和行号一样多； 那就说明当前行和当前列是一样多的，换句话说：当前行 最后一列位置和下一行 行号是一样的，让当前行 最后一列位置用换行符占着。 思路有了，代码就好实现；需要三个变量来实现上面功能，一个供输出的变量index，一个行变量line，一个列变量coumn；当前行最后一列 和下一行行号相等的话，此列输出换号符；并让列重1开始，行号+1；继续打印i，然后列号+1；循环判断行号列的关系，一旦相等 就在此列位置输出换行符； 但要排除一种情况，就是行和列 都是1时，如果此时也输出换号的话，相等于第一行 第一列是个换行符 ；而1打印到第二行了。 代码实现： int line = 1; int coumn = 1; int index = 1; while (index while 布尔表达式的灵活性还有用变量来表示，如 boolean flag = true； while(flag){循环体}，在循环体中，出现某种情况让flag值变为false，即可实现中断循环的功能；或者更简单，出现符合条件后用break跳出即可。另外不管是何种循环控制器 都可以用break或continue。 "},"middleware/java-recursion.html":{"url":"middleware/java-recursion.html","title":"利用单项链表学习递归","keywords":"","body":""},"middleware/java-decortar.html":{"url":"middleware/java-decortar.html","title":"装饰者模式扩充方法","keywords":"","body":"如何利用装饰者模式进行方法扩充 适用场景说明 在编码过程中经常会遇见到原有方法需要扩充的情况，要么在原有类上进行扩充、要么写一个继承类进行扩充； 但这样做都有一定的弊端；原有类进行扩充会返工 导致其他问题，继承类进行扩充耦合度太高；这时候装饰者模式就派上用场了。 如有一个类DecoratorA，里面有一个m1方法： public class DecoratorA { public void m1() { System.out.println(\"我是A\"); } } DecoratorB对其扩充： public class DecoratorB{ DecoratorA da; public DecoratorB(DecoratorA da){ this.da=da; } public void m1() { System.out.println(\"我是B,我对A类的方法进行扩展1\"); da.m1(); System.out.println(\"我是B,我对A类的方法进行扩展2\"); } } 这样也能完成对DecoratorA类的m1方法扩充，new一个DecoratorB 类型的对象，构造参数是DecoratorA 类型就行； 用DecoratorB 类型对象调用m1，就实现调用扩充后的m1方法；但限制条件DecoratorB和DecoratorA的m1方法名称和写法、参数要一致； 为了规范，避免方法参数不一致导致的异常，可以再编写一个抽象类（接口也可以），里面只有一个抽象方法m1： package com.zhengling.work; public abstract class DecoratorFather { public abstract void m1(); } 然后让DecoratorB和DecoratorA去继承这个抽象类即可；DecoratorB 构造器中参数类型可以写成DecoratorFather的，利用java多态特性，到时候new 一个DecoratorB ，构造参数可以填写子类型DecoratorA； 改进后的DecoratorA： package com.zhengling.work; import org.junit.Test; public class DecoratorA extends DecoratorFather { @Override @Test public void m1() { System.out.println(\"我是A\"); } } DecoratorB : package com.zhengling.work; import org.junit.Test; public class DecoratorB extends DecoratorFather { DecoratorFather df; public DecoratorB(DecoratorFather df){ this.df=df; } public void m1() { System.out.println(\"我是B,我对A类的方法进行扩展1\"); df.m1(); System.out.println(\"我是B,我对A类的方法进行扩展2\"); } } 再编写一个测试类： package com.zhengling.work; public class DecortarTest { public static void main(String[] args) { DecoratorB db = new DecoratorB(new DecoratorA()); db.m1(); } } 输出结果中可以看出DecoratorB 对DecoratorA的m1方法进行了扩展；这种装饰者模式在IO流控制类中多有实践，如某一个输出流中关闭了，所有的流都会同步被关闭。 "},"middleware/java-enmu-switch.html":{"url":"middleware/java-enmu-switch.html","title":"枚举和switch结合使用","keywords":"","body":"枚举与switch结合实践 枚举，可以罗列出有限序列集合，比如最常见的周一至周日，switch分支判断语句，这里可以做一下结合，这样能掌握两个知识点。 package com.zhengling.work; // 枚举中也可以使用构造器，默认是使用private修饰的； 枚举中每个元素 都是按照构造器（格式）生成的对象； public enum WeekEnum { MON(\"1001\",\"番茄鸡蛋\",12.0), TUE(\"1002\",\"鱼香肉丝\",24.0), WED(\"1003\",\"地三鲜\",18.0), THU(\"1004\",\"红烧肉\",35.0), FRI(\"1005\",\"宫保鸡丁\",30.0), SAT(\"1006\",\"麻辣香锅\",32.0), SUN(\"1007\",\"家常豆腐\",15.0); private String no; private String name; private double price; WeekEnum(String no, String name, double price) { this.no = no; this.name = name; this.price = price; } public void setNo(String no) { this.no = no; } public void setName(String name) { this.name = name; } public void setPrice(double price) { this.price = price; } public String getNo() { return no; } public String getName() { return name; } public double getPrice() { return price; } } 上面代码创建了一个常见的周一至周日的枚举，在每天推出不同的特价菜；然后定义一个构造方法（默认是用private修饰的构造方法，只能用private修饰）、getter、sett等；枚举中的每个元素都要按照这个构造方法来实现。 然后我们在用switch 来一一判断： package com.zhengling.work; //import static org.junit.jupiter.api.Assertions.*; //枚举和switch case的学习 class WeekEnumTest { public static void main(String[] args) { discountMenu(WeekEnum.SAT); } public static void discountMenu(WeekEnum we) { switch (we){ case MON: System.out.println(\"今日特价菜是：\"+we.getName()); break; case TUE: System.out.println(\"今日特价菜是：\"+we.getName()); break; case WED: System.out.println(\"今日特价菜是：\"+we.getName()); break; case THU: System.out.println(\"今日特价菜是：\"+we.getName()); break; case FRI: System.out.println(\"今日特价菜是：\"+we.getName()); break; case SAT: System.out.println(\"今日特价菜是：\"+we.getName()); break; case SUN: System.out.println(\"今日特价菜是：\"+we.getName()); break; default: throw new IllegalStateException(\"Unexpected value: \" + we); } } } 从这个案例，可以迅速掌握enmu和switch语法。随着jdk最新版本推出，会逐渐丰富switch写法的；最新的jdk13已经支持 switch里面支持匹配多条件、携带返回值、最新的lamda格式的写法，以后会越来越好用。 "},"middleware/java-interface-lamda.html":{"url":"middleware/java-interface-lamda.html","title":"接口与lamda表达式","keywords":"","body":"利用lamda表达式实现接口方法 若想直接使用 接口中方法，在jdk8之前的做法有：1，编写一个类实现这个接口的（抽象）方法；2，或者直接在创建对象时 采用匿名类。 接口： package com.zhengling.work; public interface DecoratorF { void m1(); } 里面有个方法m1;如果想调用m1方法，必须先实现这个接口的m1方法。用匿名内部类来实现： DecoratorF df = new DecoratorF() { @Override public void m1() { System.out.println(\"内部匿名类实现m1方法\"); } }; df.m1(); 这样就可以直接调用m1方法；或者搞一个内部类来实现这个接口： static class B implements DecoratorF{ @Override public void m1() { System.out.println(\"静态内部类实现接口的m1方法\"); } //定义为静态内部类 方便在main方法中调用 } 然后用多态的特点 调用m1方法： DecoratorF def = new B(); def.m1(); 上面不管哪种写法都显得比较繁琐，而且效率不高。jdk8推出了lamda表达式，采用这种形式能快速简化代码；说句题外话Python中lamda已经推出好久了，以后编程语言会大一同，很多优秀规范、表达式会互相借鉴。像jdk13中也引入了三个双引号 +内容+三个双引号，可以原样保持内容的规范、switch表达式借鉴swift语言写法。总之就是代码上越来越简单。 lamda表达式写法 public static void doM1(DecoratorF df){ df.m1(); } 之所以定义为static类型方法，是方便在main中调用和测试。 上面方法是直接调用了m1方法，在main方法中，调用时 用lamda表达式实现接口中的方法。 doM1(()-> System.out.println(\"lamda表达式实现的抽象方法\")); doM1(这个里面其实是DecoratorF 类型参数，这个参数并且实现了m1方法)，由于m1是没有参数的方法，可以直接写成()->{} ，由于方法体就是一句话，{}可以省略。如果有参数可以写成 (parm1,parm2)->{}这种形式。 不适用地方，接口中有多个方法的。 "},"middleware/java-shopping-map.html":{"url":"middleware/java-shopping-map.html","title":"利用购物车系统学习HashMap与equals方法重写","keywords":"","body":"运用简单的超市购物车系统，理解重写equals、hashcode的意义，以及map的学习 商品类： 先编写一个商品类，要有编号、名称、价格三个成员变量，然后把getter、setter都写好（可以用ide生成） package com.zhengling.work; import java.util.HashMap; import java.util.Map; public class Shopping { String no; String name; double price; public Shopping() { } public Shopping(String no,String name,double price) { this.name = name; this.no = no; this.price = price; } public void setName(String name) { this.name = name; } public void setNo(String no){ this.no=no; } public void setPrice(double price) { this.price = price; } public String getName() { return name; } public String getNo(){ return no; } public double getPrice() { return price; } @Override public String toString() { return \"Shopping{\" + \"编号：'\" + no + '\\'' + \", 名称是：'\" + name + '\\'' + \", 单价是：\" + price + '}'; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Shopping shopping = (Shopping) o; return no.equals(shopping.no); } @Override public int hashCode() { return no.hashCode(); } } 这个商品类，里面重写了toString方法，方便打印，构造器必须有编号、名称、价格三个参数。为什么要重写equals方法呢，所有类默认都是继承Object类的，Object的equals方法是比较两个对象的内存地址，如果相同则是同一个对象，我们这里商品编号相同 就是同一个东西； Object里面判断方法： if (this == o) return true 重写equals只需在上面代码基础上继续丰富就行。 if (o == null || getClass() != o.getClass()) return false; Shopping shopping = (Shopping) o; return no.equals(shopping.no); 还有一种常见的写法： if (o instanceof Shopping) { Shopping shopping = (Shopping) o; return no.equals(shopping.no); } 推荐第一种写法，第一种写法限制对象必须是当前类型及Shopping类型。第二种方法风险在于如果有两个类都继承了Shopping，都有成员变量弄，且都没有重写equals，就会判断失误。 为什么要同时重写hashCode方法呢，因为这些类 new出来的对象，存放到HashSet、HashMap等集合、map中；首先比较的是hash值（散列值），如果没有重写hashCode，就很容易出现hash值相同的数据被覆盖掉的情况。hashCode 最简单的一个写法就是：返回用于比较两个对象是否相等的成员变量的hash值 public int hashCode() { return no.hashCode(); } 也可以参照String Integer源码 里面hashCode的写法。 购物车 购物车用HashMap表示，key表示存放商品，value表示数量。 import java.util.HashMap; import java.util.Iterator; import java.util.Map; import java.util.Set; public class ShoppingCart { Map map = new HashMap<>(); public void add(Shopping s){ add(s,1); } public void add(Shopping s,Integer num){ map.put(s,num); } public void deleteShopping(Shopping s){ deleteShopping(s,1); } public void deleteAllShopping(Shopping s){ map.remove(s); } public void deleteShopping(Shopping s,Integer num){ if(num set = map.keySet(); if(set.size() == 0) System.out.println(\"购物车为空！！！！！\"); Iterator it = set.iterator(); Shopping sp; double total=0.0; while (it.hasNext()){ sp=it.next(); System.out.println(sp.no+\"\\t\"+sp.name+\" 单价：\"+sp.price+\"\\t数量：\"+map.get(sp)+\"\\t 小计：\" +(double)(sp.price*map.get(sp).intValue())); total += (double)(sp.price*map.get(sp).intValue()); } System.out.println(\"\\t\\t\\t\\t\\t\\t\\t\\t合计：\"+total+\"元\"); } public void printShopping(){ Set> myset = map.entrySet(); if(myset.size() == 0) System.out.println(\"购物车为空\"); Shopping p; int number; double total=0.0; for (Map.Entry ent: myset) { p= ent.getKey(); number=ent.getValue().intValue(); System.out.println(p.name+\"的单价是：\"+p.price+\"\\t 数量是 \"+number+\"\\t 小计：\"+(double)(p.price*number)); total += (double)(p.price*number); } System.out.println(\"\\t\\t\\t\\t\\t\\t 商品总价是：\"+total); } } 里面有增加商品、清除商品、清空购物车、打印清单等功能。 最后写一个简单的测试方法 package com.zhengling.work; public class ShoppingCartTest { public static void main(String[] args) { Shopping sp1 = new Shopping(\"1001\",\"西瓜\",1.0); Shopping sp2 = new Shopping(\"1002\",\"黄瓜\",12.0); Shopping sp3 = new Shopping(\"2001\",\"苹果\",2.0); Shopping sp4 = new Shopping(\"3001\",\"菠萝\",12.0); System.out.println(sp1); System.out.println(\"-----\"); ShoppingCart spc = new ShoppingCart(); spc.add(sp1); spc.add(sp2,2); spc.add(sp3,4); spc.add(sp4); //spc.printShopping(); spc.printTotalItem(); System.out.println(); spc.printShopping(); } } 然后运行，并验证通过。 "},"data/install_druid.html":{"url":"data/install_druid.html","title":"druid安装与优化","keywords":"","body":"Druid 安装与优化 以下是druid安装脚本，利用druid和tranquility压缩包进行安装，安装完成后，制作成系统服务。安装过程中有druid 初始化、jvm参数调整、与tranquility连接等操作。 function config_druid { if [[ -e /usr/lib/systemd/system/druid.service ]];then echo \"druid already installed\" else # 安装配置druid ############################################### cd $DIR mkdir -p ../{druid,tranquility} ############################################### echo \"unzip druid package...\" tar -xzf pkg/druid*.tar.gz -C ../druid --strip-components 1 tar -xzf pkg/tranquility*.tgz -C ../tranquility --strip-components 1 echo \"config druid use right zookeeper...\" sed -i 's/druid.zk.service.host=.*/druid.zk.service.host=localhost:12181/g' ../druid/conf-quickstart/druid/_common/common.runtime.properties echo \"copy kafka.json...\" /bin/cp -rf ../kafka.json ../tranquility # 这个json文件是配置与tranquility连接用的，根据生产情况来配置 echo \"add start druid scripts\" cat > ../druid/switch-druid ../tranquility/start-tran > \\$dir/start-tran.log 2>&1 & EOF echo \"add chmod 777...\" sudo chmod -R 777 ../druid sudo chmod -R 777 ../tranquility cd ../druid/ ./bin/init echo \"configure druid broker jvm size\" sed -i \"s#druid.processing.buffer.sizeBytes=.*#druid.processing.buffer.sizeBytes=104857600#g\" \\ conf-quickstart/druid/broker/runtime.properties sed -i \"s#MaxDirectMemorySize=.*#MaxDirectMemorySize=600m#g\" \\ conf-quickstart/druid/broker/jvm.config echo \"configure druid historical jvm size\" sed -i \"s#druid.processing.buffer.sizeBytes=.*#druid.processing.buffer.sizeBytes=104857600#g\" \\ conf-quickstart/druid/historical/runtime.properties sed -i \"s#-Xms.*#-Xms500m#g\" conf-quickstart/druid/historical/jvm.config sed -i \"s#-Xmx.*#-Xmx700m#g\" conf-quickstart/druid/historical/jvm.config sed -i \"s#MaxDirectMemorySize=.*#MaxDirectMemorySize=600m#g\" conf-quickstart/druid/historical/jvm.config echo \"configure druid middleManager buffer size\" sed -i '$a\\druid.indexer.task.restoreTasksOnRestart=true' conf-quickstart/druid/middleManager/runtime.properties sed -i \"s#druid.indexer.fork.property.druid.processing.buffer.sizeBytes=.*#druid.indexer.fork.property.druid.processing.buffer.sizeBytes=128000000#g\" \\ conf-quickstart/druid/middleManager/runtime.properties sed -ir 's#druid.worker.capacity=.*#druid.worker.capacity=6#g' \\ conf-quickstart/druid/middleManager/runtime.properties echo \"configure druid log level\" sed -i \"s#Root level=\\\"info\\\"#Root level=\\\"error\\\"#g\" conf-quickstart/druid/_common/log4j2.xml echo \"set druid to service\" cat > /usr/lib/systemd/system/druid.service /usr/lib/systemd/system/tran.service "},"data/kafka_cmd.html":{"url":"data/kafka_cmd.html","title":"kafka常用命令","keywords":"","body":"kafka常用命令 我这里zookeeper端口修改为12181 kafka端口为19092了 cd kafka/bin/ 列出所有topic ./kafka-topics.sh --list --zookeeper localhost:12181 查看某个topic详情 ./kafka-topics.sh --describe --zookeeper localhost:12181 --topic metric 修改topic分区大小 ./kafka-topics.sh --alter --zookeeper localhost:2181 --topic TransactionTopic --partitions 16 查看所有groups ./kafka-consumer-groups.sh --bootstrap-server localhost:19092 --list 查看groups详情，这个是动态的，一般可以作为是否采集到数据判断依据 ./kafka-consumer-groups.sh --bootstrap-server localhost:19092 --describe --group trx "},"data/update_jvm_zk.html":{"url":"data/update_jvm_zk.html","title":"zookeeper和kafka参数调整","keywords":"","body":"Jvm 参数优化 Zookeeper 修改方式： vi zookeeper/conf/java.env 输入export JVMFLAGS=\"$JVMFLAGS -Xms16g -Xmx16g\" 重启 Zookeeper 服务生效 Kafka 修改方式： vi kafka/bin/kafka-server-start.sh 将export KAFKA_HEAP_OPTS=\"-Xmx1G -Xms1G\"中的 1G 修改为 32G 重启 Kafka 服务生效 这里具体调整多少，根据实际生产情况，如果是一个每天千万级别业务量，服务器内存大于64G，可以按照上面示例来调整。 "},"data/cluster_redis.html":{"url":"data/cluster_redis.html","title":"redis多实例","keywords":"","body":"单台机器redis多实例 redis作为缓冲、验证码数据库非常好用，但redsi是单线程的，如果通过rpm工具安装的话，一台机器只能安装一个实例，如果有多个实例，需要部署多台机器上；下面脚本就实现了，一台机器安装5个redis，每个redis端口都不一样；端口从16379开始。 #!/bin/bash # 告诉bash如果任何语句的执行结果不是true则应该退出 set -e echo \"\" echo \"Installing redis_cluster....\" echo \"\" # 获取系统环境变量 source ~/.bash_profile # 获取脚本所在的目录名称，并进入该目录 DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\" echo \"$DIR\" cd $DIR/pkg # 把redis制作成服务 echo \"Installing redis...\" redis_version=`rpm -qa|grep redis` if [[ ! -n \"$redis_version\" ]]; then if [[ ! -n $(uname -r | grep el7) ]]; then rpm -ivh jemalloc-3.6.0-1.el6.x86_64.rpm rpm -ivh redis-4.0.1-2.el6.remi.x86_64.rpm else rpm -ivh jemalloc-3.6.0-1.el7.x86_64.rpm rpm -ivh redis-4.0.1-2.el7.remi.x86_64.rpm fi sudo sed -i \"s/^bind *.*.*.*$/bind 0.0.0.0/g\" /etc/redis.conf sudo sed -i \"s/^save 900 1$/#save 900 1/g\" /etc/redis.conf sudo sed -i \"s/^save 300 10$/#save 300 10/g\" /etc/redis.conf sudo sed -i \"s/^save 60 10000$/#save 60 10000/g\" /etc/redis.conf sudo sed -i \"s/^#requirepass foobared$/requirepass RedisP@ssw0rd/g\" /etc/redis.conf sudo sed -i 's/port 6379$/port 16379/g' /etc/redis.conf fi # 因为我们环境中 redis 存储不需落盘，直接把\"save 900 1\" 都注释掉了 echo \"复制redis配置文件,并且修改redis端口等配置\" sudo mkdir -p /etc/redis/ sudo mv /etc/redis.conf /etc/redis/16379.conf sudo chmod 777 /etc/redis/16379.conf sudo cp -r /etc/redis/16379.conf /etc/redis/16380.conf sudo sed -i 's/^pidfile \\/var\\/run\\/redis\\/.*$/pidfile \\/var\\/run\\/redis\\/redis16380.pid/g' /etc/redis/16380.conf sudo sed -i 's/^port 163.*$/port 16380/g' /etc/redis/16380.conf sudo sed -i 's/^logfile \\/var\\/log\\/redis\\/redis.*$/logfile \\/var\\/log\\/redis\\/redis16380.log/g' /etc/redis/16380.conf sudo sed -i 's/^dbfilename dump.*$/dbfilename dump16380.rdb/g' /etc/redis/16380.conf sudo cp -r /etc/redis/16379.conf /etc/redis/16381.conf sudo sed -i 's/^pidfile \\/var\\/run\\/redis\\/.*$/pidfile \\/var\\/run\\/redis\\/redis16381.pid/g' /etc/redis/16381.conf sudo sed -i 's/^port 163.*$/port 16381/g' /etc/redis/16381.conf sudo sed -i 's/^logfile \\/var\\/log\\/redis\\/redis.*$/logfile \\/var\\/log\\/redis\\/redis16381.log/g' /etc/redis/16381.conf sudo sed -i 's/^dbfilename dump.*$/dbfilename dump16381.rdb/g' /etc/redis/16381.conf sudo cp -r /etc/redis/16379.conf /etc/redis/16382.conf sudo sed -i 's/^pidfile \\/var\\/run\\/redis\\/.*$/pidfile \\/var\\/run\\/redis\\/redis16382.pid/g' /etc/redis/16382.conf sudo sed -i 's/^port 163.*$/port 16382/g' /etc/redis/16382.conf sudo sed -i 's/^logfile \\/var\\/log\\/redis\\/redis.*$/logfile \\/var\\/log\\/redis\\/redis16382.log/g' /etc/redis/16382.conf sudo sed -i 's/^dbfilename dump.*$/dbfilename dump16382.rdb/g' /etc/redis/16382.conf sudo cp -r /etc/redis/16379.conf /etc/redis/16383.conf sudo sed -i 's/^pidfile \\/var\\/run\\/redis\\/.*$/pidfile \\/var\\/run\\/redis\\/redis16383.pid/g' /etc/redis/16383.conf sudo sed -i 's/^port 163.*$/port 16383/g' /etc/redis/16383.conf sudo sed -i 's/^logfile \\/var\\/log\\/redis\\/redis.*$/logfile \\/var\\/log\\/redis\\/redis16383.log/g' /etc/redis/16383.conf sudo sed -i 's/^dbfilename dump.*$/dbfilename dump16383.rdb/g' /etc/redis/16383.conf if [[ ! -n $(uname -r | grep el7) ]]; then sudo chmod 777 /etc/rc.d/init.d/redis sudo sed -i 's/^REDIS_CONFIG=\"\\/etc\\/redis.*\"$/REDIS_CONFIG=\"\\/etc\\/redis\\/16379.conf\"/g' /etc/rc.d/init.d/redis sudo chkconfig redis on sudo cp -r /etc/rc.d/init.d/redis /etc/rc.d/init.d/redis16380 sudo sed -i 's/^pidfile=\"\\/var\\/run\\/redis\\/.*\"$/pidfile=\"\\/var\\/run\\/redis\\/redis16380.pid\"/g' /etc/rc.d/init.d/redis16380 sudo sed -i 's/^REDIS_CONFIG=\"\\/etc\\/redis.*\"$/REDIS_CONFIG=\"\\/etc\\/redis\\/16380.conf\"/g' /etc/rc.d/init.d/redis16380 sudo chkconfig redis16380 on sudo cp -r /etc/rc.d/init.d/redis /etc/rc.d/init.d/redis16381 sudo sed -i 's/^pidfile=\"\\/var\\/run\\/redis\\/.*\"$/pidfile=\"\\/var\\/run\\/redis\\/redis16381.pid\"/g' /etc/rc.d/init.d/redis16381 sudo sed -i 's/^REDIS_CONFIG=\"\\/etc\\/redis.*\"$/REDIS_CONFIG=\"\\/etc\\/redis\\/16381.conf\"/g' /etc/rc.d/init.d/redis16381 sudo chkconfig redis16381 on sudo cp -r /etc/rc.d/init.d/redis /etc/rc.d/init.d/redis16382 sudo sed -i 's/^pidfile=\"\\/var\\/run\\/redis\\/.*\"$/pidfile=\"\\/var\\/run\\/redis\\/redis16382.pid\"/g' /etc/rc.d/init.d/redis16382 sudo sed -i 's/^REDIS_CONFIG=\"\\/etc\\/redis.*\"$/REDIS_CONFIG=\"\\/etc\\/redis\\/16382.conf\"/g' /etc/rc.d/init.d/redis16382 sudo chkconfig redis16382 on sudo cp -r /etc/rc.d/init.d/redis /etc/rc.d/init.d/redis16383 sudo sed -i 's/^pidfile=\"\\/var\\/run\\/redis\\/.*\"$/pidfile=\"\\/var\\/run\\/redis\\/redis16383.pid\"/g' /etc/rc.d/init.d/redis16383 sudo sed -i 's/^REDIS_CONFIG=\"\\/etc\\/redis.*\"$/REDIS_CONFIG=\"\\/etc\\/redis\\/16383.conf\"/g' /etc/rc.d/init.d/redis16383 sudo chkconfig redis16383 on sudo /sbin/iptables -I INPUT -p tcp --dport 16379 -j ACCEPT sudo /sbin/iptables -I INPUT -p tcp --dport 16380 -j ACCEPT sudo /sbin/iptables -I INPUT -p tcp --dport 16381 -j ACCEPT sudo /sbin/iptables -I INPUT -p tcp --dport 16382 -j ACCEPT sudo /sbin/iptables -I INPUT -p tcp --dport 16383 -j ACCEPT sudo /etc/rc.d/init.d/iptables save sudo service iptables restart else sudo /bin/cp -r redis\\@.service /usr/lib/systemd/system/ cd /usr/lib/systemd/system/ sudo systemctl daemon-reload sudo systemctl enable redis@16379.service sudo systemctl enable redis@16380.service sudo systemctl enable redis@16381.service sudo systemctl enable redis@16382.service sudo systemctl enable redis@16383.service # 判断防火墙状态 如果结果为0表示防火墙为运行状态 set +e sudo firewall-cmd --state 1>/dev/null 2>&1 firewall_status=$? if [ $firewall_status -eq 0 ]; then sudo firewall-cmd --add-port=16379/tcp --permanent sudo firewall-cmd --add-port=16380/tcp --permanent sudo firewall-cmd --add-port=16381/tcp --permanent sudo firewall-cmd --add-port=16382/tcp --permanent sudo firewall-cmd --add-port=16383/tcp --permanent # 重启防火墙使其生效 sudo firewall-cmd --reload fi fi echo \"启动多redis实例\" sleep 5s sudo ./start-redis_cluster.sh 系统服务\"redis@.service\" 脚本，存放到/usr/lib/systemd/system/目录下 [Unit] Description=redis for %i After=network.target [Service] ExecStart=/usr/bin/redis-server /etc/redis/%i.conf --daemonize no ExecStop=/usr/bin/redis-shutdown /etc/redis/%i.conf Restart=on-failure RestartSec=1s User=redis Group=redis [Install] WantedBy=multi-user.target 增加一个启动脚本，因为单独一个个启动太麻烦。 对照着，也可以写一个停止脚本。 #!/bin/bash if [[ ! -n $(uname -r | grep el7) ]]; then service redis start service redis16380 start service redis16381 start service redis16382 start service redis16383 start else sudo systemctl start redis@16379 sudo systemctl start redis@16380 sudo systemctl start redis@16381 sudo systemctl start redis@16382 sudo systemctl start redis@16383 fi "}}